#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹å¯¼å‡ºè„šæœ¬ - æ‰¹é‡å¯¼å‡ºæ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ä¸ºä¸åŒæ ¼å¼
æ”¯æŒå¯¼å‡ºä¸º TorchScript å’Œ ONNX æ ¼å¼
"""
import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List
import torch
import logging

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from inference_runner import InferenceRunner
from curriculum_trainer import CurriculumTrainer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_all_model_paths() -> Dict[str, Dict[str, str]]:
    """è·å–æ‰€æœ‰æ¨¡å‹è·¯å¾„"""
    models_dir = Path("../../models/curriculum_v2")
    model_paths = {}
    
    for port_dir in models_dir.iterdir():
        if port_dir.is_dir():
            port_name = port_dir.name
            model_paths[port_name] = {}
            
            for model_file in port_dir.glob("stage_*_best.pt"):
                stage_name = model_file.stem.replace("stage_", "").replace("_best", "")
                model_paths[port_name][stage_name] = str(model_file)
    
    return model_paths

def create_sample_input(port_name: str, stage_name: str) -> tuple:
    """ä¸ºæŒ‡å®šæ¸¯å£å’Œé˜¶æ®µåˆ›å»ºç¤ºä¾‹è¾“å…¥"""
    try:
        trainer = CurriculumTrainer(port_name)
        stages = trainer.curriculum_stages
        
        # æ‰¾åˆ°å¯¹åº”çš„é˜¶æ®µ
        target_stage = None
        for stage in stages:
            if stage.name == stage_name:
                target_stage = stage
                break
        
        if target_stage is None:
            logger.warning(f"æœªæ‰¾åˆ°é˜¶æ®µ {stage_name}ï¼Œä½¿ç”¨é»˜è®¤è¾“å…¥")
            # ä½¿ç”¨é»˜è®¤ç»´åº¦
            batch_size = 1
            state_dim = 56
            num_nodes = 10
            node_feature_dim = 8
        else:
            # æ ¹æ®é˜¶æ®µé…ç½®åˆ›å»ºè¾“å…¥
            batch_size = 1
            state_dim = 56  # ç»Ÿä¸€ä¸º56ç»´çŠ¶æ€
            num_nodes = min(target_stage.max_vessels + target_stage.max_berths, 20)
            node_feature_dim = 8
        
        # åˆ›å»ºç¤ºä¾‹å¼ é‡
        device = torch.device("cpu")
        sample_input = (
            torch.randn(batch_size, state_dim, device=device),
            torch.randn(batch_size, num_nodes, node_feature_dim, device=device),
            torch.randn(batch_size, num_nodes, num_nodes, device=device)
        )
        
        return sample_input
        
    except Exception as e:
        logger.error(f"åˆ›å»ºç¤ºä¾‹è¾“å…¥å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤è¾“å…¥
        device = torch.device("cpu")
        return (
            torch.randn(1, 56, device=device),
            torch.randn(1, 10, 8, device=device),
            torch.randn(1, 10, 10, device=device)
        )

def export_single_model(port_name: str, stage_name: str, model_path: str,
                       output_dir: Path, formats: List[str]) -> Dict:
    """å¯¼å‡ºå•ä¸ªæ¨¡å‹"""
    results = {
        'port': port_name,
        'stage': stage_name,
        'source_model': model_path,
        'exports': {}
    }
    
    try:
        # åˆå§‹åŒ–æ¨ç†è¿è¡Œå™¨
        runner = InferenceRunner(port_name, device="cpu")
        
        # åŠ è½½PyTorchæ¨¡å‹
        success = runner.load_pytorch_model(stage_name, model_path)
        if not success:
            results['error'] = "PyTorchæ¨¡å‹åŠ è½½å¤±è´¥"
            return results
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        sample_input = create_sample_input(port_name, stage_name)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        port_output_dir = output_dir / port_name
        port_output_dir.mkdir(parents=True, exist_ok=True)
        
        # å¯¼å‡ºä¸åŒæ ¼å¼
        if 'torchscript' in formats:
            torchscript_path = port_output_dir / f"stage_{stage_name}_best.pt"
            success = runner.export_torchscript(stage_name, str(torchscript_path), sample_input)
            results['exports']['torchscript'] = {
                'success': success,
                'path': str(torchscript_path) if success else None
            }
        
        if 'onnx' in formats:
            onnx_path = port_output_dir / f"stage_{stage_name}_best.onnx"
            success = runner.export_onnx(stage_name, str(onnx_path), sample_input)
            results['exports']['onnx'] = {
                'success': success,
                'path': str(onnx_path) if success else None
            }
        
        logger.info(f"âœ… å®Œæˆå¯¼å‡º: {port_name}/{stage_name}")
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å‡ºå¤±è´¥ {port_name}/{stage_name}: {e}")
        results['error'] = str(e)
    
    return results

def export_all_models(output_dir: str, formats: List[str]) -> Dict:
    """æ‰¹é‡å¯¼å‡ºæ‰€æœ‰æ¨¡å‹"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è·å–æ‰€æœ‰æ¨¡å‹è·¯å¾„
    model_paths = get_all_model_paths()
    
    export_results = {
        'total_models': 0,
        'successful_exports': 0,
        'failed_exports': 0,
        'formats': formats,
        'output_directory': str(output_path),
        'results': []
    }
    
    # é€ä¸ªå¯¼å‡ºæ¨¡å‹
    for port_name, stages in model_paths.items():
        for stage_name, model_path in stages.items():
            export_results['total_models'] += 1
            
            logger.info(f"å¯¼å‡ºæ¨¡å‹: {port_name}/{stage_name}")
            result = export_single_model(port_name, stage_name, model_path, 
                                       output_path, formats)
            
            export_results['results'].append(result)
            
            # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥
            if 'error' in result:
                export_results['failed_exports'] += 1
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„å¯¼å‡º
                has_success = any(
                    export_info.get('success', False) 
                    for export_info in result['exports'].values()
                )
                if has_success:
                    export_results['successful_exports'] += 1
                else:
                    export_results['failed_exports'] += 1
    
    return export_results

def create_deployment_manifest(export_results: Dict, output_dir: str):
    """åˆ›å»ºéƒ¨ç½²æ¸…å•"""
    manifest = {
        'version': '1.0',
        'export_date': torch.utils.data.get_worker_info(),  # è·å–æ—¶é—´æˆ³çš„æ›¿ä»£æ–¹æ³•
        'summary': {
            'total_models': export_results['total_models'],
            'successful_exports': export_results['successful_exports'],
            'failed_exports': export_results['failed_exports'],
            'formats': export_results['formats']
        },
        'models': {}
    }
    
    # ç»„ç»‡æ¨¡å‹ä¿¡æ¯
    for result in export_results['results']:
        if 'error' in result:
            continue
            
        port = result['port']
        stage = result['stage']
        
        if port not in manifest['models']:
            manifest['models'][port] = {}
        
        manifest['models'][port][stage] = {
            'source_model': result['source_model'],
            'exports': {}
        }
        
        for format_name, export_info in result['exports'].items():
            if export_info.get('success', False):
                manifest['models'][port][stage]['exports'][format_name] = export_info['path']
    
    # ä¿å­˜æ¸…å•
    manifest_path = Path(output_dir) / "deployment_manifest.json"
    with open(manifest_path, 'w', encoding='utf-8') as f:
        json.dump(manifest, f, indent=2, ensure_ascii=False)
    
    logger.info(f"éƒ¨ç½²æ¸…å•å·²ä¿å­˜: {manifest_path}")
    return manifest_path

def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡å¯¼å‡ºæ¨¡å‹")
    parser.add_argument("--output-dir", default="../../exports", 
                       help="å¯¼å‡ºç›®å½•")
    parser.add_argument("--formats", nargs="+", 
                       choices=["torchscript", "onnx"],
                       default=["torchscript", "onnx"],
                       help="å¯¼å‡ºæ ¼å¼")
    parser.add_argument("--port", help="æŒ‡å®šæ¸¯å£ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--stage", help="æŒ‡å®šé˜¶æ®µï¼ˆå¯é€‰ï¼Œéœ€è¦åŒæ—¶æŒ‡å®šæ¸¯å£ï¼‰")
    
    args = parser.parse_args()
    
    logger.info("å¼€å§‹æ‰¹é‡æ¨¡å‹å¯¼å‡º...")
    logger.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    logger.info(f"å¯¼å‡ºæ ¼å¼: {args.formats}")
    
    if args.port and args.stage:
        # å¯¼å‡ºå•ä¸ªæ¨¡å‹
        model_paths = get_all_model_paths()
        if args.port not in model_paths or args.stage not in model_paths[args.port]:
            logger.error(f"æœªæ‰¾åˆ°æ¨¡å‹: {args.port}/{args.stage}")
            sys.exit(1)
        
        model_path = model_paths[args.port][args.stage]
        output_path = Path(args.output_dir)
        
        result = export_single_model(args.port, args.stage, model_path,
                                   output_path, args.formats)
        
        print("å¯¼å‡ºç»“æœ:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
        
    else:
        # æ‰¹é‡å¯¼å‡ºæ‰€æœ‰æ¨¡å‹
        export_results = export_all_models(args.output_dir, args.formats)
        
        # åˆ›å»ºéƒ¨ç½²æ¸…å•
        manifest_path = create_deployment_manifest(export_results, args.output_dir)
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        print("\n" + "="*50)
        print("ğŸ“¦ æ¨¡å‹å¯¼å‡ºå®Œæˆ")
        print("="*50)
        print(f"æ€»æ¨¡å‹æ•°: {export_results['total_models']}")
        print(f"æˆåŠŸå¯¼å‡º: {export_results['successful_exports']}")
        print(f"å¤±è´¥å¯¼å‡º: {export_results['failed_exports']}")
        print(f"å¯¼å‡ºæ ¼å¼: {', '.join(args.formats)}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"éƒ¨ç½²æ¸…å•: {manifest_path}")
        
        # æ˜¾ç¤ºå¤±è´¥çš„å¯¼å‡º
        failed_exports = [r for r in export_results['results'] if 'error' in r]
        if failed_exports:
            print(f"\nâŒ å¤±è´¥çš„å¯¼å‡º ({len(failed_exports)}):")
            for result in failed_exports:
                print(f"  - {result['port']}/{result['stage']}: {result['error']}")
        
        print("="*50)

if __name__ == "__main__":
    main()