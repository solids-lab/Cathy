#!/usr/bin/env python3
"""
æµ·äº‹GAT-PPOè”é‚¦è®­ç»ƒå™¨
é›†æˆFedMLæ¡†æ¶ï¼Œå®ç°å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ 
"""

import logging
import torch
import numpy as np
from typing import Dict, Any, Optional, Tuple
from fedml.core import ClientTrainer

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from models.maritime_gat_ppo import MaritimeGATPPOAgent, PPOConfig
from models.fairness_reward import ComprehensiveFairnessRewardCalculator


class MaritimeFedTrainer(ClientTrainer):
    """
    æµ·äº‹GAT-PPOè”é‚¦è®­ç»ƒå™¨
    
    å°è£…MaritimeGATPPOAgentï¼Œä½¿å…¶èƒ½å¤Ÿå‚ä¸FedMLè”é‚¦å­¦ä¹ 
    """
    
    def __init__(self, model, args, node_id: int = 0, device='cpu'):
        """
        åˆå§‹åŒ–è”é‚¦è®­ç»ƒå™¨
        
        Args:
            model: åŸºç¡€PyTorchæ¨¡å‹ï¼ˆå®é™…ä½¿ç”¨å†…éƒ¨çš„GAT-PPOæ™ºèƒ½ä½“ï¼‰
            args: FedMLå‚æ•°
            node_id: èŠ‚ç‚¹IDï¼ˆå¯¹åº”æ¸¯å£/èˆªé“èŠ‚ç‚¹ï¼‰
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__(model, args)
        
        self.node_id = node_id
        self.device = device
        self.args = args
        
        # åˆ›å»ºPPOé…ç½®
        self.ppo_config = PPOConfig(
            learning_rate=getattr(args, 'learning_rate', 3e-4),
            gamma=getattr(args, 'gamma', 0.99),
            gae_lambda=getattr(args, 'gae_lambda', 0.95),
            clip_ratio=getattr(args, 'clip_ratio', 0.2),
            ppo_epochs=getattr(args, 'ppo_epochs', 4),
            batch_size=getattr(args, 'ppo_batch_size', 20),
            mini_batch_size=getattr(args, 'ppo_mini_batch_size', 10)
        )
        
        # åˆ›å»ºæµ·äº‹GAT-PPOæ™ºèƒ½ä½“
        self.maritime_agent = MaritimeGATPPOAgent(
            node_id=node_id, 
            config=self.ppo_config
        )
        
        # åˆ›å»ºå…¬å¹³æ€§å¥–åŠ±è®¡ç®—å™¨
        self.fairness_calculator = ComprehensiveFairnessRewardCalculator()
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'local_episodes': 0,
            'total_reward': 0.0,
            'policy_losses': [],
            'value_losses': [],
            'fairness_scores': []
        }
        
        logging.info(f"âœ… æµ·äº‹è”é‚¦è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ - èŠ‚ç‚¹ID: {node_id}")

    def get_model_params(self) -> Dict[str, torch.Tensor]:
        """
        è·å–æ¨¡å‹å‚æ•°ï¼ˆç”¨äºè”é‚¦èšåˆï¼‰
        
        Returns:
            æ¨¡å‹çŠ¶æ€å­—å…¸
        """
        # è¿”å›GAT-PPOæ™ºèƒ½ä½“çš„å®Œæ•´çŠ¶æ€
        agent_state = self.maritime_agent.state_dict()
        
        # åŒ…å«é¢å¤–çš„è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        agent_state['training_stats'] = {
            'node_id': self.node_id,
            'local_episodes': self.training_stats['local_episodes'],
            'total_reward': self.training_stats['total_reward'],
            'avg_policy_loss': np.mean(self.training_stats['policy_losses']) if self.training_stats['policy_losses'] else 0.0,
            'avg_value_loss': np.mean(self.training_stats['value_losses']) if self.training_stats['value_losses'] else 0.0
        }
        
        return agent_state

    def set_model_params(self, model_parameters: Dict[str, torch.Tensor]):
        """
        è®¾ç½®æ¨¡å‹å‚æ•°ï¼ˆæ¥è‡ªè”é‚¦èšåˆï¼‰
        
        Args:
            model_parameters: èšåˆåçš„æ¨¡å‹å‚æ•°
        """
        # æå–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        if 'training_stats' in model_parameters:
            stats = model_parameters.pop('training_stats')
            logging.info(f"ğŸ“Š æ¥æ”¶å…¨å±€æ¨¡å‹ - æ¥æºèŠ‚ç‚¹: {stats.get('node_id', 'unknown')}")
        
        # åŠ è½½æ¨¡å‹å‚æ•°åˆ°GAT-PPOæ™ºèƒ½ä½“
        self.maritime_agent.load_state_dict(model_parameters, strict=False)
        
        logging.info("âœ… å…¨å±€æ¨¡å‹å‚æ•°å·²æ›´æ–°")

    def train(self, train_data, device, args) -> Dict[str, float]:
        """
        æ‰§è¡Œæœ¬åœ°è®­ç»ƒ
        
        Args:
            train_data: è®­ç»ƒæ•°æ®ï¼ˆæµ·äº‹ç¯å¢ƒæ•°æ®ï¼‰
            device: è®¡ç®—è®¾å¤‡
            args: è®­ç»ƒå‚æ•°
            
        Returns:
            è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        logging.info(f"ğŸš€ å¼€å§‹æœ¬åœ°è®­ç»ƒ - èŠ‚ç‚¹ID: {self.node_id}")
        
        self.maritime_agent.train()
        
        # å¦‚æœä¼ å…¥çš„æ˜¯ç¯å¢ƒæ•°æ®ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        if hasattr(train_data, '__iter__'):
            training_episodes = list(train_data)
        else:
            # ç”Ÿæˆæ¨¡æ‹Ÿçš„æµ·äº‹ç¯å¢ƒäº¤äº’æ•°æ®
            training_episodes = self._generate_maritime_episodes(args.epochs)
        
        episode_rewards = []
        training_losses = []
        
        for episode_idx, episode_data in enumerate(training_episodes):
            # æ‰§è¡Œä¸€ä¸ªepisodeçš„è®­ç»ƒ
            episode_reward, training_stats = self._run_episode(episode_data)
            
            episode_rewards.append(episode_reward)
            if training_stats:
                training_losses.append(training_stats)
            
            # æ›´æ–°æœ¬åœ°ç»Ÿè®¡
            self.training_stats['local_episodes'] += 1
            self.training_stats['total_reward'] += episode_reward
            
            if (episode_idx + 1) % 5 == 0:
                logging.info(f"  Episode {episode_idx + 1}: å¥–åŠ±={episode_reward:.2f}")
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_reward = np.mean(episode_rewards)
        avg_policy_loss = np.mean([stats.get('policy_loss', 0) for stats in training_losses])
        avg_value_loss = np.mean([stats.get('value_loss', 0) for stats in training_losses])
        
        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        self.training_stats['policy_losses'].extend([stats.get('policy_loss', 0) for stats in training_losses])
        self.training_stats['value_losses'].extend([stats.get('value_loss', 0) for stats in training_losses])
        
        train_results = {
            'avg_reward': avg_reward,
            'avg_policy_loss': avg_policy_loss,
            'avg_value_loss': avg_value_loss,
            'total_episodes': len(training_episodes),
            'node_id': self.node_id
        }
        
        logging.info(f"âœ… æœ¬åœ°è®­ç»ƒå®Œæˆ - å¹³å‡å¥–åŠ±: {avg_reward:.2f}, ç­–ç•¥æŸå¤±: {avg_policy_loss:.6f}")
        
        return train_results

    def _generate_maritime_episodes(self, num_episodes: int = 10) -> list:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿçš„æµ·äº‹ç¯å¢ƒepisodeæ•°æ®
        
        Args:
            num_episodes: episodeæ•°é‡
            
        Returns:
            episodeæ•°æ®åˆ—è¡¨
        """
        episodes = []
        
        for _ in range(num_episodes):
            # æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„æµ·äº‹äº¤é€šåœºæ™¯
            episode = {
                'initial_state': self._generate_maritime_state(),
                'steps': 10,  # æ¯ä¸ªepisode 10æ­¥
                'target_fairness': np.random.uniform(0.7, 0.9)  # ç›®æ ‡å…¬å¹³æ€§åˆ†æ•°
            }
            episodes.append(episode)
        
        return episodes

    def _generate_maritime_state(self) -> Dict[str, Dict]:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿçš„æµ·äº‹çŠ¶æ€è§‚æµ‹ - åŸºäºä¸åŒæ¸¯å£ç‰¹å¾çš„å·®å¼‚åŒ–æ•°æ®
        
        Returns:
            åŒ…å«æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€çš„å­—å…¸
        """
        # å®šä¹‰ä¸åŒæ¸¯å£/èŠ‚ç‚¹çš„ç‰¹å¾å·®å¼‚
        port_characteristics = {
            'NodeA': {  # æ–°å¥¥å°”è‰¯æ¸¯ä¸»å…¥å£ - é«˜æµé‡é›†è£…ç®±æ¸¯
                'base_traffic': (8, 20),     # ç­‰å¾…èˆ¹èˆ¶æ•°é‡èŒƒå›´
                'throughput_range': (4, 10), # ååé‡èŒƒå›´
                'wait_time_range': (10, 30), # ç­‰å¾…æ—¶é—´èŒƒå›´
                'traffic_pattern': 'peak_hours',  # æµé‡æ¨¡å¼
                'ship_mix': 'container_heavy'     # èˆ¹èˆ¶ç±»å‹ç»„åˆ
            },
            'NodeB': {  # å¯†è¥¿è¥¿æ¯”æ²³å£ - ä¸­ç­‰æµé‡æ•£è´§æ¸¯
                'base_traffic': (5, 15),
                'throughput_range': (2, 7),
                'wait_time_range': (8, 20),
                'traffic_pattern': 'steady',
                'ship_mix': 'bulk_carrier'
            },
            'NodeC': {  # æ²³é“ä¸­æ®µ - ä½æµé‡å†…æ²³æ¸¯
                'base_traffic': (2, 10),
                'throughput_range': (1, 5),
                'wait_time_range': (5, 15),
                'traffic_pattern': 'seasonal',
                'ship_mix': 'inland_vessels'
            },
            'NodeD': {  # è¿‘æµ·é”šåœ° - å˜åŠ¨æµé‡ä¸´æ—¶åœæ³Š
                'base_traffic': (3, 18),
                'throughput_range': (2, 8),
                'wait_time_range': (15, 40),
                'traffic_pattern': 'irregular',
                'ship_mix': 'mixed_fleet'
            }
        }
        
        state = {}
        current_hour = np.random.randint(0, 24)  # æ¨¡æ‹Ÿæ—¶é—´å½±å“
        
        for node_name, characteristics in port_characteristics.items():
            # åŸºäºæ—¶é—´å’Œæ¸¯å£ç‰¹å¾è°ƒæ•´å‚æ•°
            traffic_multiplier = self._get_traffic_multiplier(
                characteristics['traffic_pattern'], current_hour, self.node_id
            )
            
            # ç”Ÿæˆè¯¥èŠ‚ç‚¹çš„çŠ¶æ€
            base_range = characteristics['base_traffic']
            waiting_ships = max(1, int(np.random.randint(*base_range) * traffic_multiplier))
            
            throughput_range = characteristics['throughput_range']
            throughput = max(1, int(np.random.randint(*throughput_range) * traffic_multiplier))
            
            wait_range = characteristics['wait_time_range']
            waiting_time = max(1, int(np.random.randint(*wait_range) / traffic_multiplier))
            
            state[node_name] = {
                'waiting_ships': waiting_ships,
                'throughput': throughput,
                'waiting_time': waiting_time,
                'signal_phase': np.random.randint(0, 2),
                'weather_condition': np.random.uniform(0.6, 1.0),
                # æ–°å¢æ¸¯å£ç‰¹å¾å­—æ®µ
                'port_type': characteristics['ship_mix'],
                'traffic_pattern': characteristics['traffic_pattern'],
                'node_id': self.node_id
            }
        
        return state
    
    def _get_traffic_multiplier(self, pattern: str, hour: int, node_id: int) -> float:
        """
        æ ¹æ®æµé‡æ¨¡å¼å’Œæ—¶é—´è·å–æµé‡ä¹˜æ•°
        
        Args:
            pattern: æµé‡æ¨¡å¼
            hour: å½“å‰å°æ—¶
            node_id: èŠ‚ç‚¹IDï¼ˆå¼•å…¥èŠ‚ç‚¹é—´å·®å¼‚ï¼‰
            
        Returns:
            æµé‡ä¹˜æ•°
        """
        base_multiplier = 1.0
        
        if pattern == 'peak_hours':
            # æ¨¡æ‹Ÿå·¥ä½œæ—¶é—´é«˜å³°æœŸ
            if 8 <= hour <= 17:
                base_multiplier = 1.5 + 0.3 * np.sin((hour - 8) * np.pi / 9)
            else:
                base_multiplier = 0.7
        elif pattern == 'steady':
            # ç›¸å¯¹ç¨³å®šçš„æµé‡
            base_multiplier = 1.0 + 0.2 * np.sin(hour * np.pi / 12)
        elif pattern == 'seasonal':
            # å­£èŠ‚æ€§å˜åŒ–ï¼ˆç®€åŒ–ä¸ºæ—¥å†…å˜åŒ–ï¼‰
            base_multiplier = 0.8 + 0.4 * np.sin(hour * np.pi / 24)
        elif pattern == 'irregular':
            # ä¸è§„åˆ™å˜åŒ–
            base_multiplier = 0.6 + 0.8 * np.random.random()
        
        # åŠ å…¥èŠ‚ç‚¹é—´çš„å·®å¼‚æ€§
        node_variation = 0.8 + 0.4 * (node_id / 4.0)
        
        return base_multiplier * node_variation

    def _run_episode(self, episode_data: Dict) -> Tuple[float, Optional[Dict]]:
        """
        è¿è¡Œä¸€ä¸ªå®Œæ•´çš„episode
        
        Args:
            episode_data: episodeé…ç½®æ•°æ®
            
        Returns:
            (episodeæ€»å¥–åŠ±, è®­ç»ƒç»Ÿè®¡)
        """
        observations = episode_data['initial_state']
        episode_reward = 0.0
        training_stats = None
        
        # æ‰§è¡Œepisodeæ­¥éª¤
        for step in range(episode_data['steps']):
            # è·å–æ™ºèƒ½ä½“åŠ¨ä½œ
            action, log_prob, value, entropy = self.maritime_agent.get_action_and_value(observations)
            
            # æ¨¡æ‹Ÿç¯å¢ƒååº”
            next_observations = self._simulate_environment_step(observations, action)
            
            # è®¡ç®—å¥–åŠ±ï¼ˆåŒ…å«å…¬å¹³æ€§ï¼‰
            reward = self.maritime_agent.calculate_comprehensive_reward(
                observations, action, next_observations
            )
            episode_reward += reward
            
            # å­˜å‚¨ç»éªŒ
            done = (step == episode_data['steps'] - 1)
            self.maritime_agent.store_transition(
                observations, action, reward, next_observations, done, log_prob, value.item()
            )
            
            observations = next_observations
        
        # å°è¯•PPOæ›´æ–°
        if len(self.maritime_agent.memory) >= self.ppo_config.mini_batch_size:
            training_stats = self.maritime_agent.update_policy()
        
        return episode_reward, training_stats

    def _simulate_environment_step(self, current_state: Dict, action: int) -> Dict:
        """
        æ¨¡æ‹Ÿç¯å¢ƒæ­¥éª¤
        
        Args:
            current_state: å½“å‰çŠ¶æ€
            action: æ™ºèƒ½ä½“åŠ¨ä½œ
            
        Returns:
            ä¸‹ä¸€ä¸ªçŠ¶æ€
        """
        next_state = {node: state.copy() for node, state in current_state.items()}
        
        # æ¨¡æ‹ŸåŠ¨ä½œå¯¹ä¸»èŠ‚ç‚¹çš„å½±å“
        main_node = f'Node{chr(65 + self.node_id)}'  # NodeA, NodeB, etc.
        if main_node in next_state:
            # åŠ¨ä½œå½±å“ç­‰å¾…æ—¶é—´
            time_reduction = [1, 2, 3, 4][action]  # å¯¹åº”ä¸åŒä¿¡å·é•¿åº¦
            next_state[main_node]['waiting_time'] = max(
                0, current_state[main_node]['waiting_time'] - time_reduction
            )
            
            # ååé‡å¾®è°ƒ
            next_state[main_node]['throughput'] = min(
                10, current_state[main_node]['throughput'] + time_reduction // 2
            )
        
        return next_state

    def test(self, test_data, device, args) -> Dict[str, float]:
        """
        æ‰§è¡Œæµ‹è¯•è¯„ä¼°
        
        Args:
            test_data: æµ‹è¯•æ•°æ®
            device: è®¡ç®—è®¾å¤‡
            args: æµ‹è¯•å‚æ•°
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        self.maritime_agent.eval()
        
        test_episodes = 5  # æµ‹è¯•episodeæ•°é‡
        test_rewards = []
        fairness_scores = []
        
        with torch.no_grad():
            for _ in range(test_episodes):
                observations = self._generate_maritime_state()
                episode_reward = 0.0
                
                for step in range(10):
                    action, _, _, _ = self.maritime_agent.get_action_and_value(observations)
                    next_observations = self._simulate_environment_step(observations, action)
                    
                    reward = self.maritime_agent.calculate_comprehensive_reward(
                        observations, action, next_observations
                    )
                    episode_reward += reward
                    
                    # è®¡ç®—å…¬å¹³æ€§åˆ†æ•°
                    fairness_result = self.fairness_calculator.calculate_comprehensive_reward(
                        observations, {f'Node{chr(65 + self.node_id)}': reward}, next_observations
                    )
                    fairness_scores.append(fairness_result.get('fairness_reward', 0.0))
                    
                    observations = next_observations
                
                test_rewards.append(episode_reward)
        
        test_results = {
            'test_avg_reward': np.mean(test_rewards),
            'test_fairness_score': np.mean(fairness_scores),
            'test_reward_std': np.std(test_rewards),
            'node_id': self.node_id
        }
        
        logging.info(f"ğŸ§ª æµ‹è¯•å®Œæˆ - å¹³å‡å¥–åŠ±: {test_results['test_avg_reward']:.2f}, "
                    f"å…¬å¹³æ€§åˆ†æ•°: {test_results['test_fairness_score']:.2f}")
        
        return test_results

    def get_training_stats(self) -> Dict[str, Any]:
        """
        è·å–è¯¦ç»†çš„è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            è®­ç»ƒç»Ÿè®¡å­—å…¸
        """
        return {
            **self.training_stats,
            'agent_stats': self.maritime_agent.get_training_stats(),
            'node_id': self.node_id,
            'device': str(self.device)
        }


# åˆ›å»ºå·¥å‚å‡½æ•°ï¼Œä¾¿äºFedMLè°ƒç”¨
def create_maritime_trainer(model, args, node_id=0, device='cpu'):
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæµ·äº‹è”é‚¦è®­ç»ƒå™¨
    
    Args:
        model: åŸºç¡€æ¨¡å‹
        args: FedMLå‚æ•°
        node_id: èŠ‚ç‚¹ID
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        MaritimeFedTrainerå®ä¾‹
    """
    return MaritimeFedTrainer(model, args, node_id, device)


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    class MockArgs:
        learning_rate = 3e-4
        epochs = 3
    
    import torch.nn as nn
    mock_model = nn.Linear(10, 4)  # å ä½ç¬¦æ¨¡å‹
    mock_args = MockArgs()
    
    trainer = MaritimeFedTrainer(mock_model, mock_args, node_id=0)
    print("âœ… æµ·äº‹è”é‚¦è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
    
    # æµ‹è¯•è®­ç»ƒ
    train_results = trainer.train(None, 'cpu', mock_args)
    print(f"ğŸ“Š è®­ç»ƒç»“æœ: {train_results}") 