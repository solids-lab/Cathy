#!/usr/bin/env python3
"""
å¤šç«¯å£CityFlowè”é‚¦å­¦ä¹ ç³»ç»Ÿ
åŸºäºçœŸå®CityFlowä»¿çœŸçš„å¤šç«¯å£æµ·äº‹äº¤é€šæ§åˆ¶è”é‚¦å­¦ä¹ æ¡†æ¶
æ¯ä¸ªç«¯å£è¿è¡Œç‹¬ç«‹çš„CityFlowä»¿çœŸç¯å¢ƒ
"""

import os
import sys
import json
import numpy as np
import torch
import time
import threading
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import logging
import subprocess
import multiprocessing as mp

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "FedML" / "python"))

# CityFlowå¯¼å…¥
try:
    import cityflow
    CITYFLOW_AVAILABLE = True
    print("âœ… CityFlow å¯ç”¨")
except ImportError:
    CITYFLOW_AVAILABLE = False
    print("âš ï¸ CityFlow ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿç¯å¢ƒ")

# è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥
from src.models.maritime_gat_ppo import MaritimeGATPPOAgent
from src.models.fairness_reward import AlphaFairRewardCalculator
from src.federated.real_data_collector import RealDataCollector, initialize_data_collector

class PortEnvironment:
    """å•ä¸ªæ¸¯å£çš„CityFlowç¯å¢ƒ"""
    
    def __init__(self, port_id: int, port_name: str, topology_size: str = "3x3"):
        self.port_id = port_id
        self.port_name = port_name
        self.topology_size = topology_size
        
        # é…ç½®æ–‡ä»¶è·¯å¾„
        self.topology_dir = project_root / "topologies"
        self.config_file = self.topology_dir / f"maritime_{topology_size}_config.json"
        
        # CityFlowå¼•æ“
        self.cityflow_engine = None
        self.current_step = 0
        self.max_steps = 3600  # 1å°æ—¶ä»¿çœŸ
        
        # çŠ¶æ€å’Œè§‚æµ‹
        self.last_state = None
        self.action_space_size = 4  # æ¸¯å£æ§åˆ¶åŠ¨ä½œ
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = {
            'total_waiting_time': 0.0,
            'average_speed': 0.0,
            'throughput': 0.0,
            'queue_length': 0.0,
            'safety_score': 1.0
        }
        
        self._initialize_environment()
        
    def _initialize_environment(self):
        """åˆå§‹åŒ–æ¸¯å£ç¯å¢ƒ"""
        try:
            if CITYFLOW_AVAILABLE and self.config_file.exists():
                # ä¸ºæ¯ä¸ªç«¯å£åˆ›å»ºç‹¬ç«‹çš„é…ç½®
                port_config = self._create_port_specific_config()
                self.cityflow_engine = cityflow.Engine(port_config, thread_num=1)
                print(f"âœ… ç«¯å£ {self.port_name} CityFlowç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
            else:
                print(f"âš ï¸ ç«¯å£ {self.port_name} ä½¿ç”¨æ¨¡æ‹Ÿç¯å¢ƒ")
                self._init_mock_environment()
                
        except Exception as e:
            print(f"âŒ ç«¯å£ {self.port_name} ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            self._init_mock_environment()
    
    def _create_port_specific_config(self) -> str:
        """ä¸ºæ¯ä¸ªç«¯å£åˆ›å»ºç‰¹å®šçš„é…ç½®æ–‡ä»¶"""
        # è¯»å–åŸºç¡€é…ç½®
        with open(self.config_file, 'r') as f:
            config = json.load(f)
        
        # ä¿®æ”¹é…ç½®ä»¥é€‚åº”ç‰¹å®šç«¯å£
        port_config_file = self.topology_dir / f"maritime_{self.topology_size}_{self.port_name}_config.json"
        
        # ä¿®æ”¹ç§å­ä»¥ç¡®ä¿ä¸åŒç«¯å£æœ‰ä¸åŒçš„éšæœºæ€§
        config["seed"] = 42 + self.port_id * 100
        
        # ä¿®æ”¹è¾“å‡ºæ–‡ä»¶è·¯å¾„
        config["roadnetLogFile"] = f"maritime_{self.topology_size}_{self.port_name}_replay_roadnet.json"
        config["replayLogFile"] = f"maritime_{self.topology_size}_{self.port_name}_replay.txt"
        
        # ä¿å­˜ç«¯å£ç‰¹å®šé…ç½®
        with open(port_config_file, 'w') as f:
            json.dump(config, f, indent=2)
        
        return str(port_config_file)
    
    def _init_mock_environment(self):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿç¯å¢ƒ"""
        self.cityflow_engine = None
        # åˆå§‹åŒ–æ¨¡æ‹ŸçŠ¶æ€
        self.mock_state = {
            'vehicles': np.random.randint(10, 50),
            'waiting_vehicles': np.random.randint(5, 20),
            'average_speed': np.random.uniform(5, 15),
            'queue_lengths': np.random.randint(0, 10, size=4).tolist()
        }
    
    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        
        if self.cityflow_engine:
            self.cityflow_engine.reset()
        else:
            self._init_mock_environment()
        
        return self.get_state()
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ"""
        if self.cityflow_engine:
            return self._cityflow_step(action)
        else:
            return self._mock_step(action)
    
    def _cityflow_step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """CityFlowç¯å¢ƒæ­¥è¿›"""
        # åº”ç”¨åŠ¨ä½œåˆ°ä¿¡å·ç¯æ§åˆ¶
        self._apply_action_to_signals(action)
        
        # æ‰§è¡Œä¸€æ­¥ä»¿çœŸ
        self.cityflow_engine.next_step()
        self.current_step += 1
        
        # è·å–æ–°çŠ¶æ€
        state = self._get_cityflow_state()
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(state, action)
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self.current_step >= self.max_steps
        
        # æ›´æ–°æŒ‡æ ‡
        self._update_metrics(state)
        
        info = {
            'step': self.current_step,
            'metrics': self.metrics.copy(),
            'port': self.port_name
        }
        
        return state, reward, done, info
    
    def _mock_step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """æ¨¡æ‹Ÿç¯å¢ƒæ­¥è¿›"""
        self.current_step += 1
        
        # æ¨¡æ‹ŸåŠ¨ä½œæ•ˆæœ
        action_effects = {
            0: 0.95,  # ä¿æŒå½“å‰
            1: 1.1,   # å¢åŠ é€šé‡
            2: 0.9,   # å‡å°‘ç­‰å¾…
            3: 1.05   # å¹³è¡¡ç­–ç•¥
        }
        
        effect = action_effects.get(action, 1.0)
        
        # æ›´æ–°æ¨¡æ‹ŸçŠ¶æ€
        self.mock_state['vehicles'] = max(5, int(self.mock_state['vehicles'] * np.random.uniform(0.9, 1.1)))
        self.mock_state['waiting_vehicles'] = max(0, int(self.mock_state['waiting_vehicles'] * effect))
        self.mock_state['average_speed'] = np.clip(
            self.mock_state['average_speed'] * np.random.uniform(0.95, 1.05) * (2-effect), 
            2, 20
        )
        
        # æ„å»ºçŠ¶æ€å‘é‡
        state = np.array([
            self.mock_state['vehicles'] / 100.0,
            self.mock_state['waiting_vehicles'] / 50.0,
            self.mock_state['average_speed'] / 20.0,
            np.mean(self.mock_state['queue_lengths']) / 10.0
        ], dtype=np.float32)
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_mock_reward(state, action)
        
        done = self.current_step >= self.max_steps
        
        # æ›´æ–°æŒ‡æ ‡
        self.metrics.update({
            'total_waiting_time': self.mock_state['waiting_vehicles'] * 2.0,
            'average_speed': self.mock_state['average_speed'],
            'throughput': max(0, 50 - self.mock_state['waiting_vehicles']),
            'queue_length': np.mean(self.mock_state['queue_lengths']),
            'safety_score': np.random.uniform(0.8, 1.0)
        })
        
        info = {
            'step': self.current_step,
            'metrics': self.metrics.copy(),
            'port': self.port_name
        }
        
        return state, reward, done, info
    
    def _apply_action_to_signals(self, action: int):
        """å°†åŠ¨ä½œåº”ç”¨åˆ°ä¿¡å·ç¯æ§åˆ¶"""
        if not self.cityflow_engine:
            return
            
        # è·å–æ‰€æœ‰ä¿¡å·ç¯
        signals = self.cityflow_engine.get_lane_vehicle_count()
        
        # æ ¹æ®åŠ¨ä½œè°ƒæ•´ä¿¡å·ç¯ç›¸ä½
        action_to_phase = {
            0: 0,  # ä¿æŒå½“å‰ç›¸ä½
            1: 1,  # åˆ‡æ¢åˆ°é«˜é€šé‡ç›¸ä½
            2: 2,  # åˆ‡æ¢åˆ°å‡å°‘ç­‰å¾…ç›¸ä½
            3: 3   # å¹³è¡¡ç›¸ä½
        }
        
        target_phase = action_to_phase.get(action, 0)
        
        # è®¾ç½®ä¿¡å·ç¯ç›¸ä½ï¼ˆå¦‚æœæœ‰å¤šä¸ªä¿¡å·ç¯ï¼Œå¯ä»¥è®¾ç½®ä¸åŒç­–ç•¥ï¼‰
        try:
            intersections = self.cityflow_engine.get_intersection_ids()
            for intersection_id in intersections[:1]:  # åªæ§åˆ¶ç¬¬ä¸€ä¸ªäº¤å‰å£ä½œä¸ºç¤ºä¾‹
                self.cityflow_engine.set_tl_phase(intersection_id, target_phase)
        except:
            pass  # å¿½ç•¥è®¾ç½®é”™è¯¯
    
    def _get_cityflow_state(self) -> np.ndarray:
        """ä»CityFlowè·å–çŠ¶æ€"""
        try:
            # è·å–åŸºç¡€çŠ¶æ€ä¿¡æ¯
            lane_count = self.cityflow_engine.get_lane_vehicle_count()
            lane_waiting = self.cityflow_engine.get_lane_waiting_vehicle_count()
            vehicle_speed = self.cityflow_engine.get_vehicle_speed()
            
            # è®¡ç®—èšåˆæŒ‡æ ‡
            total_vehicles = sum(lane_count.values()) if lane_count else 0
            total_waiting = sum(lane_waiting.values()) if lane_waiting else 0
            avg_speed = np.mean(list(vehicle_speed.values())) if vehicle_speed else 0
            
            # æ„å»ºçŠ¶æ€å‘é‡
            state = np.array([
                total_vehicles / 100.0,      # å½’ä¸€åŒ–æ€»è½¦è¾†æ•°
                total_waiting / 50.0,        # å½’ä¸€åŒ–ç­‰å¾…è½¦è¾†æ•°
                avg_speed / 20.0,            # å½’ä¸€åŒ–å¹³å‡é€Ÿåº¦
                len(lane_count) / 20.0       # å½’ä¸€åŒ–è½¦é“æ•°
            ], dtype=np.float32)
            
            return state
            
        except Exception as e:
            print(f"è·å–CityFlowçŠ¶æ€å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çŠ¶æ€
            return np.array([0.1, 0.1, 0.5, 0.2], dtype=np.float32)
    
    def _calculate_reward(self, state: np.ndarray, action: int) -> float:
        """è®¡ç®—å¥–åŠ±"""
        # åŸºç¡€æ•ˆç‡å¥–åŠ±
        efficiency_reward = (state[2] * 10) - (state[1] * 5)  # é€Ÿåº¦å¥–åŠ± - ç­‰å¾…æƒ©ç½š
        
        # ååé‡å¥–åŠ±
        throughput_reward = (state[0] * 2) if state[1] < 0.3 else 0
        
        # åŠ¨ä½œå¥–åŠ±
        action_rewards = {0: 0, 1: 2, 2: 1, 3: 1.5}
        action_reward = action_rewards.get(action, 0)
        
        # ç¨³å®šæ€§å¥–åŠ±ï¼ˆé¿å…æç«¯çŠ¶æ€ï¼‰
        stability_reward = 0
        if 0.2 < state[0] < 0.8 and state[1] < 0.4:
            stability_reward = 2
        
        total_reward = efficiency_reward + throughput_reward + action_reward + stability_reward
        return float(total_reward)
    
    def _calculate_mock_reward(self, state: np.ndarray, action: int) -> float:
        """è®¡ç®—æ¨¡æ‹Ÿå¥–åŠ±"""
        return self._calculate_reward(state, action)
    
    def _update_metrics(self, state: np.ndarray):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.metrics.update({
            'total_waiting_time': state[1] * 50 * 2,  # ç­‰å¾…è½¦è¾† * å¹³å‡ç­‰å¾…æ—¶é—´
            'average_speed': state[2] * 20,           # åå½’ä¸€åŒ–é€Ÿåº¦
            'throughput': max(0, state[0] * 100 - state[1] * 50),  # é€šè¿‡é‡
            'queue_length': state[1] * 10,            # é˜Ÿåˆ—é•¿åº¦
            'safety_score': min(1.0, 1.2 - state[1]) # å®‰å…¨åˆ†æ•°
        })
    
    def get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€"""
        if self.cityflow_engine:
            return self._get_cityflow_state()
        else:
            return np.array([
                self.mock_state['vehicles'] / 100.0,
                self.mock_state['waiting_vehicles'] / 50.0,
                self.mock_state['average_speed'] / 20.0,
                np.mean(self.mock_state['queue_lengths']) / 10.0
            ], dtype=np.float32)
    
    def get_metrics(self) -> Dict[str, float]:
        """è·å–å½“å‰æŒ‡æ ‡"""
        return self.metrics.copy()
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if self.cityflow_engine:
            try:
                # CityFlowæ²¡æœ‰æ˜¾å¼çš„closeæ–¹æ³•ï¼Œç›´æ¥è®¾ä¸ºNone
                self.cityflow_engine = None
            except:
                pass

class MultiPortFederatedAgent:
    """å¤šç«¯å£è”é‚¦å­¦ä¹ æ™ºèƒ½ä½“"""
    
    def __init__(self, port_id: int, port_name: str, topology_size: str = "3x3"):
        self.port_id = port_id
        self.port_name = port_name
        self.topology_size = topology_size
        
        # ç«¯å£ç¯å¢ƒ
        self.env = PortEnvironment(port_id, port_name, topology_size)
        
        # GAT-PPOæ™ºèƒ½ä½“
        self.state_dim = 4  # çŠ¶æ€ç»´åº¦
        self.action_dim = 4  # åŠ¨ä½œç»´åº¦
        self.node_num = 9 if topology_size == "3x3" else 16  # æ ¹æ®æ‹“æ‰‘ç¡®å®šèŠ‚ç‚¹æ•°
        
        self.gat_ppo_agent = MaritimeGATPPOAgent(
            node_num=self.node_num,
            node_dim=self.state_dim,
            action_dim=self.action_dim
        )
        
        # å…¬å¹³æ€§å¥–åŠ±è®¡ç®—å™¨
        self.fairness_calculator = AlphaFairRewardCalculator(alpha=0.5)
        
        # è®­ç»ƒå†å²
        self.training_history = []
        self.episode_rewards = []
        
        print(f"âœ… ç«¯å£ {port_name} æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def train_episode(self, max_steps: int = 1000) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        state = self.env.reset()
        episode_reward = 0
        episode_steps = 0
        episode_metrics = []
        
        # æ„å»ºèŠ‚ç‚¹ç‰¹å¾ï¼ˆGATéœ€è¦ï¼‰
        node_features = self._build_node_features(state)
        
        for step in range(max_steps):
            # GAT-PPOå†³ç­–
            action, action_prob = self.gat_ppo_agent.select_action(node_features)
            
            # ç¯å¢ƒäº¤äº’
            next_state, reward, done, info = self.env.step(action)
            
            # è®¡ç®—å…¬å¹³æ€§å¢å¼ºå¥–åŠ±
            fairness_reward = self.fairness_calculator.calculate_reward(
                base_reward=reward,
                current_state=state,
                action=action,
                agent_id=self.port_id,
                other_agents_states={}  # åœ¨è”é‚¦è®¾ç½®ä¸­ï¼Œè¿™é‡Œä¸åŒ…å«å…¶ä»–æ™ºèƒ½ä½“çŠ¶æ€
            )
            
            total_reward = reward + fairness_reward
            
            # æ„å»ºä¸‹ä¸€çŠ¶æ€çš„èŠ‚ç‚¹ç‰¹å¾
            next_node_features = self._build_node_features(next_state)
            
            # å­˜å‚¨ç»éªŒ
            self.gat_ppo_agent.store_experience(
                state=node_features,
                action=action,
                reward=total_reward,
                next_state=next_node_features,
                done=done,
                action_prob=action_prob
            )
            
            # æ›´æ–°çŠ¶æ€
            state = next_state
            node_features = next_node_features
            episode_reward += total_reward
            episode_steps += 1
            
            # è®°å½•æŒ‡æ ‡
            episode_metrics.append(info['metrics'])
            
            if done:
                break
        
        # æ‰§è¡ŒPPOæ›´æ–°
        training_stats = self.gat_ppo_agent.update()
        
        # è®¡ç®—episodeç»Ÿè®¡
        avg_metrics = {}
        if episode_metrics:
            for key in episode_metrics[0].keys():
                avg_metrics[f'avg_{key}'] = np.mean([m[key] for m in episode_metrics])
        
        episode_result = {
            'episode_reward': episode_reward,
            'episode_steps': episode_steps,
            'avg_reward_per_step': episode_reward / max(episode_steps, 1),
            **avg_metrics,
            **training_stats,
            'port_name': self.port_name,
            'port_id': self.port_id
        }
        
        self.training_history.append(episode_result)
        self.episode_rewards.append(episode_reward)
        
        return episode_result
    
    def _build_node_features(self, state: np.ndarray) -> torch.Tensor:
        """æ„å»ºGATéœ€è¦çš„èŠ‚ç‚¹ç‰¹å¾"""
        # å°†çŠ¶æ€æ‰©å±•åˆ°æ‰€æœ‰èŠ‚ç‚¹
        # ç®€åŒ–å®ç°ï¼šæ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨ç›¸åŒçš„åŸºç¡€çŠ¶æ€ï¼ŒåŠ ä¸Šä½ç½®ç¼–ç 
        node_features = []
        
        for i in range(self.node_num):
            # åŸºç¡€çŠ¶æ€ç‰¹å¾
            base_features = state.copy()
            
            # æ·»åŠ èŠ‚ç‚¹ä½ç½®ç¼–ç 
            row = i // int(np.sqrt(self.node_num))
            col = i % int(np.sqrt(self.node_num))
            position_encoding = np.array([row / int(np.sqrt(self.node_num)), 
                                        col / int(np.sqrt(self.node_num))])
            
            # ç»„åˆç‰¹å¾
            combined_features = np.concatenate([base_features, position_encoding])
            node_features.append(combined_features)
        
        return torch.FloatTensor(node_features).unsqueeze(0)  # æ·»åŠ batchç»´åº¦
    
    def get_model_parameters(self) -> Dict:
        """è·å–æ¨¡å‹å‚æ•°ï¼ˆç”¨äºè”é‚¦å­¦ä¹ ï¼‰"""
        return self.gat_ppo_agent.get_model_parameters()
    
    def set_model_parameters(self, parameters: Dict):
        """è®¾ç½®æ¨¡å‹å‚æ•°ï¼ˆç”¨äºè”é‚¦å­¦ä¹ ï¼‰"""
        self.gat_ppo_agent.set_model_parameters(parameters)
    
    def get_training_statistics(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not self.episode_rewards:
            return {}
        
        recent_rewards = self.episode_rewards[-10:]  # æœ€è¿‘10ä¸ªepisode
        
        return {
            'total_episodes': len(self.episode_rewards),
            'avg_episode_reward': np.mean(self.episode_rewards),
            'recent_avg_reward': np.mean(recent_rewards),
            'best_reward': max(self.episode_rewards),
            'reward_std': np.std(self.episode_rewards),
            'port_name': self.port_name,
            'port_id': self.port_id
        }
    
    def close(self):
        """å…³é—­æ™ºèƒ½ä½“"""
        self.env.close()

class MultiPortFederatedSystem:
    """å¤šç«¯å£è”é‚¦å­¦ä¹ ç³»ç»Ÿ"""
    
    def __init__(self, num_ports: int = 4, topology_size: str = "3x3"):
        self.num_ports = num_ports
        self.topology_size = topology_size
        
        # ç«¯å£åç§°æ˜ å°„
        self.port_names = {
            0: "new_orleans",
            1: "south_louisiana", 
            2: "baton_rouge",
            3: "gulfport"
        }
        
        # åˆ›å»ºç«¯å£æ™ºèƒ½ä½“
        self.port_agents = {}
        for i in range(num_ports):
            port_name = self.port_names.get(i, f"port_{i}")
            self.port_agents[i] = MultiPortFederatedAgent(i, port_name, topology_size)
        
        # è”é‚¦å­¦ä¹ å‚æ•°
        self.global_model_params = None
        
        # æ•°æ®æ”¶é›†å™¨
        self.data_collector = initialize_data_collector("multi_port_cityflow_experiment")
        
        print(f"âœ… å››æ¸¯å£è”é‚¦å­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ - {num_ports}ä¸ªç«¯å£äº’ç›¸å­¦ä¹ ")
        print(f"   ğŸ“ å‚ä¸æ¸¯å£: {[agent.port_name for agent in self.port_agents.values()]}")
        print(f"   ğŸ¤ è”é‚¦å­¦ä¹ æ¨¡å¼: çŸ¥è¯†å…±äº«ï¼Œæ•°æ®éšç§ä¿æŠ¤")
    
    def federated_training_round(self, episodes_per_agent: int = 5) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€è½®å››æ¸¯å£è”é‚¦å­¦ä¹  - æ‰€æœ‰æ¸¯å£äº’ç›¸å­¦ä¹ """
        print(f"ğŸ”„ å¼€å§‹å››æ¸¯å£è”é‚¦å­¦ä¹ è½®æ¬¡ - æ¯ä¸ªæ¸¯å£è®­ç»ƒ {episodes_per_agent} episodes")
        print(f"   ğŸ¤ è”é‚¦å­¦ä¹ æ¨¡å¼: {len(self.port_agents)}ä¸ªæ¸¯å£å…±äº«çŸ¥è¯†ï¼Œéšç§ä¿æŠ¤")
        
        round_results = {}
        client_models = {}
        
        # 1. å„ç«¯å£æœ¬åœ°è®­ç»ƒ
        for port_id, agent in self.port_agents.items():
            print(f"   ğŸ“ ç«¯å£ {agent.port_name} å¼€å§‹æœ¬åœ°è®­ç»ƒ...")
            
            port_results = []
            for episode in range(episodes_per_agent):
                episode_result = agent.train_episode()
                port_results.append(episode_result)
                
                # æ”¶é›†è®­ç»ƒæ•°æ®
                if self.data_collector:
                    self.data_collector.collect_training_data(
                        str(port_id), 
                        {
                            'avg_reward': episode_result['episode_reward'],
                            'avg_policy_loss': episode_result.get('policy_loss', 0),
                            'avg_value_loss': episode_result.get('value_loss', 0),
                            'total_episodes': 1
                        }
                    )
            
            # è·å–æœ¬åœ°æ¨¡å‹å‚æ•°
            client_models[port_id] = agent.get_model_parameters()
            
            # è®¡ç®—ç«¯å£å¹³å‡ç»“æœ
            avg_reward = np.mean([r['episode_reward'] for r in port_results])
            round_results[port_id] = {
                'port_name': agent.port_name,
                'episodes_trained': len(port_results),
                'avg_episode_reward': avg_reward,
                'training_results': port_results
            }
            
            print(f"   âœ… ç«¯å£ {agent.port_name} è®­ç»ƒå®Œæˆ - å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        
        # 2. è”é‚¦èšåˆ
        print("   ğŸ”„ æ‰§è¡Œè”é‚¦æ¨¡å‹èšåˆ...")
        aggregated_params = self._federated_averaging(client_models)
        
        # 3. åˆ†å‘å…¨å±€æ¨¡å‹
        for agent in self.port_agents.values():
            agent.set_model_parameters(aggregated_params)
        
        self.global_model_params = aggregated_params
        
        # æ”¶é›†èšåˆæ•°æ®
        if self.data_collector:
            avg_client_reward = np.mean([r['avg_episode_reward'] for r in round_results.values()])
            self.data_collector.collect_aggregation_data({
                'participating_clients': len(self.port_agents),
                'total_samples': sum(r['episodes_trained'] for r in round_results.values()),
                'aggregation_weights': {str(i): 1.0/len(self.port_agents) for i in range(len(self.port_agents))},
                'avg_client_reward': avg_client_reward
            })
        
        return round_results
    
    def _federated_averaging(self, client_models: Dict[int, Dict]) -> Dict:
        """å››æ¸¯å£è”é‚¦å¹³å‡èšåˆ - æ‰€æœ‰æ¸¯å£çŸ¥è¯†èåˆ"""
        if not client_models:
            return {}
        
        print(f"   ğŸ”„ èšåˆ{len(client_models)}ä¸ªæ¸¯å£çš„æ¨¡å‹å‚æ•°...")
        
        # è”é‚¦å¹³å‡èšåˆ
        aggregated_params = {}
        num_clients = len(client_models)
        
        # è·å–ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„å‚æ•°ç»“æ„
        first_client_params = next(iter(client_models.values()))
        
        # ä¸ºæ¯ä¸ªæ¸¯å£è®¡ç®—æƒé‡ï¼ˆå¯ä»¥åŸºäºæ€§èƒ½åŠ¨æ€è°ƒæ•´ï¼‰
        port_weights = {}
        for port_id in client_models.keys():
            # ç®€å•å¹³å‡æƒé‡ï¼Œä¹Ÿå¯ä»¥åŸºäºç«¯å£æ€§èƒ½è°ƒæ•´
            port_weights[port_id] = 1.0 / num_clients
        
        print(f"   âš–ï¸ æ¸¯å£æƒé‡: {port_weights}")
        
        for param_name in first_client_params.keys():
            # è®¡ç®—åŠ æƒå¹³å‡
            param_sum = None
            total_weight = 0
            
            for port_id, client_params in client_models.items():
                if param_name in client_params:
                    weight = port_weights[port_id]
                    if param_sum is None:
                        param_sum = client_params[param_name].clone() * weight
                    else:
                        param_sum += client_params[param_name] * weight
                    total_weight += weight
            
            if param_sum is not None and total_weight > 0:
                aggregated_params[param_name] = param_sum / total_weight
        
        print(f"   âœ… è”é‚¦èšåˆå®Œæˆ - èåˆäº†{num_clients}ä¸ªæ¸¯å£çš„çŸ¥è¯†")
        return aggregated_params
    
    def run_federated_experiment(self, num_rounds: int = 10, episodes_per_round: int = 5):
        """è¿è¡Œå®Œæ•´çš„å››æ¸¯å£è”é‚¦å­¦ä¹ å®éªŒ"""
        print(f"ğŸš€ å¼€å§‹å››æ¸¯å£è”é‚¦å­¦ä¹ å®éªŒ - æ¸¯å£é—´çŸ¥è¯†å…±äº«")
        print(f"   ğŸ“Š è”é‚¦è½®æ¬¡: {num_rounds}")
        print(f"   ğŸ”„ æ¯è½®episodeæ•°: {episodes_per_round}")
        print(f"   ğŸ­ å‚ä¸æ¸¯å£: {[agent.port_name for agent in self.port_agents.values()]}")
        print(f"   ğŸ¤ å­¦ä¹ æ¨¡å¼: æ¯è½®æ‰€æœ‰æ¸¯å£äº’ç›¸å­¦ä¹ ï¼Œå…±äº«æœ€ä¼˜ç­–ç•¥")
        
        # å¯åŠ¨æ•°æ®æ”¶é›†
        if self.data_collector:
            port_names = [agent.port_name for agent in self.port_agents.values()]
            self.data_collector.start_experiment(num_rounds, "Multi-Port-CityFlow-GAT-FedPPO")
        
        experiment_results = []
        
        for round_num in range(1, num_rounds + 1):
            print(f"\nğŸ“ è”é‚¦å­¦ä¹ è½®æ¬¡ {round_num}/{num_rounds}")
            
            # å¯åŠ¨è½®æ¬¡
            if self.data_collector:
                self.data_collector.start_round(round_num)
            
            # æ‰§è¡Œè”é‚¦è®­ç»ƒè½®æ¬¡
            round_result = self.federated_training_round(episodes_per_round)
            
            # æ·»åŠ è½®æ¬¡ä¿¡æ¯
            round_result['round'] = round_num
            round_result['timestamp'] = datetime.now().isoformat()
            
            experiment_results.append(round_result)
            
            # æ‰“å°è½®æ¬¡æ€»ç»“
            avg_rewards = [r['avg_episode_reward'] for r in round_result.values() if isinstance(r, dict) and 'avg_episode_reward' in r]
            if avg_rewards:
                print(f"   ğŸ“Š è½®æ¬¡ {round_num} å¹³å‡å¥–åŠ±: {np.mean(avg_rewards):.2f}")
        
        # å®Œæˆå®éªŒ
        if self.data_collector:
            timestamp = self.data_collector.finish_experiment()
            print(f"âœ… å®éªŒæ•°æ®å·²ä¿å­˜: {timestamp}")
        
        # ç”Ÿæˆå®éªŒæ€»ç»“
        self._generate_experiment_summary(experiment_results)
        
        return experiment_results
    
    def _generate_experiment_summary(self, results: List[Dict]):
        """ç”Ÿæˆå®éªŒæ€»ç»“"""
        print(f"\nğŸ‰ å¤šç«¯å£è”é‚¦å­¦ä¹ å®éªŒå®Œæˆ!")
        print("=" * 60)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        all_rewards = []
        for round_result in results:
            for port_result in round_result.values():
                if isinstance(port_result, dict) and 'avg_episode_reward' in port_result:
                    all_rewards.append(port_result['avg_episode_reward'])
        
        if all_rewards:
            print(f"ğŸ“Š å®éªŒç»Ÿè®¡:")
            print(f"   æ€»è½®æ¬¡: {len(results)}")
            print(f"   å¹³å‡å¥–åŠ±: {np.mean(all_rewards):.2f}")
            print(f"   æœ€ä½³å¥–åŠ±: {max(all_rewards):.2f}")
            print(f"   å¥–åŠ±æ ‡å‡†å·®: {np.std(all_rewards):.2f}")
        
        # å„ç«¯å£è¡¨ç°
        print(f"\nğŸ­ å„ç«¯å£è¡¨ç°:")
        for agent in self.port_agents.values():
            stats = agent.get_training_statistics()
            if stats:
                print(f"   {stats['port_name']}: å¹³å‡å¥–åŠ± {stats['avg_episode_reward']:.2f}, "
                      f"æœ€ä½³å¥–åŠ± {stats['best_reward']:.2f}")
    
    def close(self):
        """å…³é—­ç³»ç»Ÿ"""
        for agent in self.port_agents.values():
            agent.close()

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå››æ¸¯å£è”é‚¦å­¦ä¹ """
    print("ğŸš€ å¯åŠ¨å››æ¸¯å£CityFlowè”é‚¦å­¦ä¹ ç³»ç»Ÿ")
    
    # åˆ›å»ºå››æ¸¯å£ç³»ç»Ÿ
    system = MultiPortFederatedSystem(num_ports=4, topology_size="3x3")
    
    try:
        # è¿è¡Œå®éªŒ
        results = system.run_federated_experiment(num_rounds=5, episodes_per_round=3)
        
        print("\nğŸ¯ å®éªŒå®Œæˆï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆå¯è§†åŒ–:")
        print("python src/federated/visualization_generator.py")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        system.close()

if __name__ == "__main__":
    main()