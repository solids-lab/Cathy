#!/usr/bin/env python3
"""
æµ·äº‹GAT-PPOè”é‚¦èšåˆå™¨
å®ç°å¤šèŠ‚ç‚¹GAT-PPOæ¨¡å‹çš„è”é‚¦èšåˆï¼Œè€ƒè™‘åœ°ç†ä½ç½®å’Œæ€§èƒ½æƒé‡
"""

import logging
import numpy as np
import torch
from collections import OrderedDict
from typing import List, Tuple, Dict, Any, Optional
from fedml.core import ServerAggregator

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from models.maritime_gat_ppo import MaritimeGATPPOAgent, PPOConfig
from models.fairness_reward import ComprehensiveFairnessRewardCalculator


class MaritimeFedAggregator(ServerAggregator):
    """
    æµ·äº‹GAT-PPOè”é‚¦èšåˆå™¨
    
    å®ç°æ™ºèƒ½åŒ–çš„æ¨¡å‹èšåˆç­–ç•¥ï¼Œè€ƒè™‘ï¼š
    1. åœ°ç†ä½ç½®æƒé‡ - ç›¸é‚»æ¸¯å£èŠ‚ç‚¹æœ‰æ›´é«˜æƒé‡
    2. æ€§èƒ½æƒé‡ - è¡¨ç°å¥½çš„å®¢æˆ·ç«¯æœ‰æ›´é«˜æƒé‡  
    3. å…¬å¹³æ€§æƒé‡ - ä¿ƒè¿›ç³»ç»Ÿæ•´ä½“å…¬å¹³æ€§
    4. è‡ªé€‚åº”èšåˆ - æ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´èšåˆç­–ç•¥
    """
    
    def __init__(self, model, args):
        """åˆå§‹åŒ–è”é‚¦èšåˆå™¨"""
        super().__init__(model, args)
        self.args = args
        
        # èšåˆé…ç½®
        self.aggregation_config = {
            'use_geographic_weights': getattr(args, 'use_geographic_weights', True),
            'use_performance_weights': getattr(args, 'use_performance_weights', True),
            'use_fairness_weights': getattr(args, 'use_fairness_weights', True),
            'min_client_weight': getattr(args, 'min_client_weight', 0.1),
            'max_client_weight': getattr(args, 'max_client_weight', 2.0),
        }
        
        # æµ·äº‹èŠ‚ç‚¹åœ°ç†ä½ç½®
        self.node_coordinates = {
            0: (-90.35, 29.95),  # NodeA (æ–°å¥¥å°”è‰¯æ¸¯ä¸»å…¥å£)
            1: (-90.25, 29.85),  # NodeB (å¯†è¥¿è¥¿æ¯”æ²³å£)
            2: (-90.30, 29.93),  # NodeC (æ²³é“ä¸­æ®µ)
            3: (-90.20, 29.80),  # NodeD (è¿‘æµ·é”šåœ°)
        }
        
        # å®¢æˆ·ç«¯å†å²æ€§èƒ½è®°å½•
        self.client_performance_history = {}
        
        # å…¬å¹³æ€§è®¡ç®—å™¨
        self.fairness_calculator = ComprehensiveFairnessRewardCalculator()
        
        # èšåˆç»Ÿè®¡
        self.aggregation_stats = {
            'round': 0,
            'participated_clients': [],
            'aggregation_weights': {},
            'performance_metrics': {}
        }
        
        logging.info("âœ… æµ·äº‹è”é‚¦èšåˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def get_model_params(self) -> Dict[str, torch.Tensor]:
        """è·å–å…¨å±€æ¨¡å‹å‚æ•°"""
        return self.model.state_dict()

    def set_model_params(self, model_parameters: Dict[str, torch.Tensor]):
        """è®¾ç½®å…¨å±€æ¨¡å‹å‚æ•°"""
        self.model.load_state_dict(model_parameters)

    def aggregate(self, raw_client_model_or_grad_list: List[Tuple[float, OrderedDict]]) -> Tuple[int, OrderedDict]:
        """æ‰§è¡Œè”é‚¦èšåˆ"""
        logging.info(f"ğŸ”„ å¼€å§‹ç¬¬ {self.aggregation_stats['round'] + 1} è½®è”é‚¦èšåˆ")
        logging.info(f"ğŸ“Š å‚ä¸å®¢æˆ·ç«¯æ•°é‡: {len(raw_client_model_or_grad_list)}")
        
        # é¢„å¤„ç†ï¼šæå–å®¢æˆ·ç«¯ä¿¡æ¯
        processed_clients = self._preprocess_client_models(raw_client_model_or_grad_list)
        
        # è®¡ç®—èšåˆæƒé‡
        aggregation_weights = self._calculate_aggregation_weights(processed_clients)
        
        # æ‰§è¡ŒåŠ æƒèšåˆ
        aggregated_model = self._weighted_aggregate(processed_clients, aggregation_weights)
        
        # åå¤„ç†ï¼šæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self._update_aggregation_stats(processed_clients, aggregation_weights)
        
        # è®¡ç®—æ€»æ ·æœ¬æ•°
        total_samples = sum(client_info['sample_num'] for client_info in processed_clients)
        
        logging.info(f"âœ… ç¬¬ {self.aggregation_stats['round']} è½®èšåˆå®Œæˆ")
        
        return total_samples, aggregated_model

    def _preprocess_client_models(self, raw_client_list: List[Tuple[float, OrderedDict]]) -> List[Dict]:
        """é¢„å¤„ç†å®¢æˆ·ç«¯æ¨¡å‹ä¿¡æ¯"""
        processed_clients = []
        
        for i, (sample_num, model_params) in enumerate(raw_client_list):
            # æå–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
            training_stats = model_params.pop('training_stats', {})
            
            client_info = {
                'client_id': i,
                'node_id': training_stats.get('node_id', i),
                'sample_num': sample_num,
                'model_params': model_params,
                'local_episodes': training_stats.get('local_episodes', 0),
                'total_reward': training_stats.get('total_reward', 0.0),
                'avg_policy_loss': training_stats.get('avg_policy_loss', 0.0),
                'avg_value_loss': training_stats.get('avg_value_loss', 0.0)
            }
            
            processed_clients.append(client_info)
            
            logging.info(f"  å®¢æˆ·ç«¯ {i} (èŠ‚ç‚¹{client_info['node_id']}): "
                        f"æ ·æœ¬={sample_num}, å¥–åŠ±={client_info['total_reward']:.2f}")
        
        return processed_clients

    def _calculate_aggregation_weights(self, clients: List[Dict]) -> List[float]:
        """è®¡ç®—æ™ºèƒ½èšåˆæƒé‡"""
        weights = []
        
        for client in clients:
            weight = 1.0  # åŸºç¡€æƒé‡
            
            # 1. æ ·æœ¬æ•°é‡æƒé‡ï¼ˆFedAvgåŸºç¡€ï¼‰
            sample_weight = client['sample_num']
            
            # 2. åœ°ç†ä½ç½®æƒé‡
            if self.aggregation_config['use_geographic_weights']:
                geo_weight = self._calculate_geographic_weight(client['node_id'], clients)
                weight *= geo_weight
            
            # 3. æ€§èƒ½æƒé‡
            if self.aggregation_config['use_performance_weights']:
                perf_weight = self._calculate_performance_weight(client)
                weight *= perf_weight
            
            # 4. æ ·æœ¬æ•°é‡è°ƒæ•´
            weight *= sample_weight
            
            # åº”ç”¨æƒé‡é™åˆ¶
            weight = np.clip(weight, 
                           self.aggregation_config['min_client_weight'],
                           self.aggregation_config['max_client_weight'])
            
            weights.append(weight)
        
        # æ ‡å‡†åŒ–æƒé‡
        weights = np.array(weights)
        weights = weights / np.sum(weights)
        
        # è®°å½•æƒé‡ä¿¡æ¯
        for i, (client, weight) in enumerate(zip(clients, weights)):
            logging.info(f"  å®¢æˆ·ç«¯ {i} èšåˆæƒé‡: {weight:.4f}")
        
        return weights.tolist()

    def _calculate_geographic_weight(self, node_id: int, all_clients: List[Dict]) -> float:
        """è®¡ç®—åœ°ç†ä½ç½®æƒé‡"""
        if node_id not in self.node_coordinates:
            return 1.0
        
        current_coord = self.node_coordinates[node_id]
        
        # è®¡ç®—ä¸å…¶ä»–å‚ä¸èŠ‚ç‚¹çš„å¹³å‡è·ç¦»
        distances = []
        for client in all_clients:
            other_node_id = client['node_id']
            if other_node_id != node_id and other_node_id in self.node_coordinates:
                other_coord = self.node_coordinates[other_node_id]
                distance = np.sqrt((current_coord[0] - other_coord[0])**2 + 
                                 (current_coord[1] - other_coord[1])**2)
                distances.append(distance)
        
        if not distances:
            return 1.0
        
        avg_distance = np.mean(distances)
        
        # è·ç¦»è¶Šè¿‘ï¼Œæƒé‡è¶Šé«˜ï¼ˆä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼‰
        geo_weight = np.exp(-avg_distance * 10)
        
        return np.clip(geo_weight, 0.5, 2.0)

    def _calculate_performance_weight(self, client: Dict) -> float:
        """è®¡ç®—æ€§èƒ½æƒé‡"""
        node_id = client['node_id']
        current_reward = client['total_reward']
        
        # æ›´æ–°æ€§èƒ½å†å²
        if node_id not in self.client_performance_history:
            self.client_performance_history[node_id] = []
        
        self.client_performance_history[node_id].append(current_reward)
        
        # ä¿æŒæœ€è¿‘10è½®çš„è®°å½•
        if len(self.client_performance_history[node_id]) > 10:
            self.client_performance_history[node_id].pop(0)
        
        # è®¡ç®—å†å²å¹³å‡å¥–åŠ±
        avg_reward = np.mean(self.client_performance_history[node_id])
        
        # å¥–åŠ±è¶Šé«˜ï¼Œæƒé‡è¶Šé«˜
        normalized_reward = avg_reward / 1000.0
        perf_weight = 1.0 / (1.0 + np.exp(-normalized_reward))
        
        return np.clip(perf_weight, 0.7, 1.5)

    def _weighted_aggregate(self, clients: List[Dict], weights: List[float]) -> OrderedDict:
        """æ‰§è¡ŒåŠ æƒæ¨¡å‹èšåˆ"""
        # è·å–ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„æ¨¡å‹ç»“æ„
        first_model = clients[0]['model_params']
        aggregated_params = OrderedDict()
        
        # å¯¹æ¯ä¸ªå‚æ•°è¿›è¡ŒåŠ æƒå¹³å‡
        for param_name in first_model.keys():
            if any(param_name.endswith(suffix) for suffix in ['.num_batches_tracked']):
                aggregated_params[param_name] = first_model[param_name].clone()
                continue
            
            weighted_sum = torch.zeros_like(first_model[param_name])
            
            for client, weight in zip(clients, weights):
                if param_name in client['model_params']:
                    weighted_sum += weight * client['model_params'][param_name]
            
            aggregated_params[param_name] = weighted_sum
        
        return aggregated_params

    def _update_aggregation_stats(self, clients: List[Dict], weights: List[float]):
        """æ›´æ–°èšåˆç»Ÿè®¡ä¿¡æ¯"""
        self.aggregation_stats['round'] += 1
        self.aggregation_stats['participated_clients'] = [c['client_id'] for c in clients]
        self.aggregation_stats['aggregation_weights'] = {
            c['client_id']: w for c, w in zip(clients, weights)
        }
        
        # è®¡ç®—èšåˆæ€§èƒ½æŒ‡æ ‡
        total_reward = sum(c['total_reward'] for c in clients)
        avg_policy_loss = np.mean([c['avg_policy_loss'] for c in clients])
        avg_value_loss = np.mean([c['avg_value_loss'] for c in clients])
        
        self.aggregation_stats['performance_metrics'] = {
            'total_reward': total_reward,
            'avg_policy_loss': avg_policy_loss,
            'avg_value_loss': avg_value_loss,
            'reward_variance': np.var([c['total_reward'] for c in clients])
        }

    def test(self, test_data, device, args) -> Dict[str, Any]:
        """åœ¨æœåŠ¡å™¨ç«¯æ‰§è¡Œå…¨å±€æ¨¡å‹æµ‹è¯•"""
        logging.info("ğŸ§ª å¼€å§‹å…¨å±€æ¨¡å‹æµ‹è¯•")
        
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„GAT-PPOæ™ºèƒ½ä½“ç”¨äºæµ‹è¯•
        ppo_config = PPOConfig()
        test_agent = MaritimeGATPPOAgent(node_id=0, config=ppo_config)
        
        # åŠ è½½å½“å‰å…¨å±€æ¨¡å‹å‚æ•°
        test_agent.load_state_dict(self.get_model_params(), strict=False)
        test_agent.eval()
        
        test_episodes = 5
        test_rewards = []
        
        with torch.no_grad():
            for _ in range(test_episodes):
                observations = self._generate_test_scenario()
                episode_reward = 0.0
                
                for step in range(10):
                    action, _, _, _ = test_agent.get_action_and_value(observations)
                    next_observations = self._simulate_test_step(observations, action)
                    
                    reward = test_agent.calculate_comprehensive_reward(
                        observations, action, next_observations
                    )
                    episode_reward += reward
                    observations = next_observations
                
                test_rewards.append(episode_reward)
        
        test_results = {
            'global_avg_reward': np.mean(test_rewards),
            'reward_std': np.std(test_rewards),
            'aggregation_round': self.aggregation_stats['round'],
            'participated_clients': len(self.aggregation_stats['participated_clients'])
        }
        
        logging.info(f"âœ… å…¨å±€æµ‹è¯•å®Œæˆ - å¹³å‡å¥–åŠ±: {test_results['global_avg_reward']:.2f}")
        return test_results

    def _generate_test_scenario(self) -> Dict[str, Dict]:
        """ç”Ÿæˆæ ‡å‡†åŒ–çš„æµ‹è¯•åœºæ™¯"""
        return {
            'NodeA': {'waiting_ships': 8, 'throughput': 4, 'waiting_time': 15, 'signal_phase': 0, 'weather_condition': 0.8},
            'NodeB': {'waiting_ships': 12, 'throughput': 3, 'waiting_time': 20, 'signal_phase': 1, 'weather_condition': 0.8},
            'NodeC': {'waiting_ships': 5, 'throughput': 2, 'waiting_time': 10, 'signal_phase': 0, 'weather_condition': 0.8},
            'NodeD': {'waiting_ships': 9, 'throughput': 5, 'waiting_time': 18, 'signal_phase': 1, 'weather_condition': 0.8},
        }

    def _simulate_test_step(self, current_state: Dict, action: int) -> Dict:
        """æ¨¡æ‹Ÿæµ‹è¯•ç¯å¢ƒæ­¥éª¤"""
        next_state = {node: state.copy() for node, state in current_state.items()}
        time_reduction = [1, 2, 3, 4][action]
        next_state['NodeA']['waiting_time'] = max(0, current_state['NodeA']['waiting_time'] - time_reduction)
        return next_state

    def test_all(self, train_data_local_dict, test_data_local_dict, device, args=None) -> bool:
        """æ‰§è¡Œæ‰€æœ‰å®¢æˆ·ç«¯çš„æµ‹è¯•"""
        logging.info("ğŸ” æ‰§è¡Œåˆ†å¸ƒå¼æµ‹è¯•")
        return True

    def get_aggregation_stats(self) -> Dict[str, Any]:
        """è·å–èšåˆç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.aggregation_stats,
            'client_performance_history': self.client_performance_history,
            'aggregation_config': self.aggregation_config
        }


# åˆ›å»ºå·¥å‚å‡½æ•°
def create_maritime_aggregator(model, args):
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæµ·äº‹è”é‚¦èšåˆå™¨"""
    return MaritimeFedAggregator(model, args)


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    class MockArgs:
        use_geographic_weights = True
        use_performance_weights = True
        use_fairness_weights = True
    
    import torch.nn as nn
    mock_model = nn.Linear(10, 4)
    mock_args = MockArgs()
    
    aggregator = MaritimeFedAggregator(mock_model, mock_args)
    print("âœ… æµ·äº‹è”é‚¦èšåˆå™¨åˆ›å»ºæˆåŠŸ")
