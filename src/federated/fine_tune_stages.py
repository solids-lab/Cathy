#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¶æ®µå¾®è°ƒè„šæœ¬ - é’ˆå¯¹æ¥è¿‘é˜ˆå€¼çš„é˜¶æ®µè¿›è¡ŒçŸ­æœŸè¡¥è®­
åŸºäºè·¯çº¿Bçš„è®­ç»ƒæ”¹è¿›æ–¹æ¡ˆï¼Œç”¨äºæå‡æ¨¡å‹æ€§èƒ½
"""
import os
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import torch
import numpy as np

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from curriculum_trainer import CurriculumTrainer, build_agent
from consistency_test_fixed import eval_one_stage

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StageFinetuner:
    """é˜¶æ®µå¾®è°ƒå™¨"""
    
    def __init__(self, port_name: str, device: str = "cpu"):
        self.port_name = port_name
        self.device = torch.device(device)
        self.trainer = CurriculumTrainer(port_name)
        self.agent = build_agent(port_name, device=self.device)
        
        logger.info(f"åˆå§‹åŒ–é˜¶æ®µå¾®è°ƒå™¨ - æ¸¯å£: {port_name}, è®¾å¤‡: {device}")
    
    def get_stage_by_name(self, stage_name: str):
        """æ ¹æ®åç§°è·å–é˜¶æ®µé…ç½®"""
        for stage in self.trainer.curriculum_stages:
            if stage.name == stage_name:
                return stage
        return None
    
    def load_stage_model(self, stage_name: str) -> bool:
        """åŠ è½½é˜¶æ®µæ¨¡å‹"""
        model_path = Path(f"../../models/curriculum_v2/{self.port_name}/stage_{stage_name}_best.pt")
        
        if not model_path.exists():
            logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
        
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                self.agent.actor_critic.load_state_dict(checkpoint['model_state_dict'])
                if 'optimizer_state_dict' in checkpoint:
                    self.agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            else:
                self.agent.actor_critic.load_state_dict(checkpoint)
            
            logger.info(f"âœ… åŠ è½½æ¨¡å‹: {model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def evaluate_stage_performance(self, stage_name: str, n_samples: int = 200) -> Dict:
        """è¯„ä¼°é˜¶æ®µæ€§èƒ½"""
        stage = self.get_stage_by_name(stage_name)
        if stage is None:
            return {'error': f'æœªæ‰¾åˆ°é˜¶æ®µ: {stage_name}'}
        
        try:
            result = eval_one_stage(
                self.trainer, self.agent, stage,
                n_samples=n_samples, device=self.device
            )
            return result
        except Exception as e:
            logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def fine_tune_stage(self, stage_name: str, target_improvement: float = 0.05,
                       max_episodes: int = 40, eval_interval: int = 5) -> Dict:
        """å¾®è°ƒé˜¶æ®µæ¨¡å‹"""
        logger.info(f"ğŸ¯ å¼€å§‹å¾®è°ƒé˜¶æ®µ: {stage_name}")
        logger.info(f"ç›®æ ‡æå‡: {target_improvement*100:.1f}%, æœ€å¤§è½®æ•°: {max_episodes}")
        
        stage = self.get_stage_by_name(stage_name)
        if stage is None:
            return {'error': f'æœªæ‰¾åˆ°é˜¶æ®µ: {stage_name}'}
        
        # åŠ è½½æ¨¡å‹
        if not self.load_stage_model(stage_name):
            return {'error': 'æ¨¡å‹åŠ è½½å¤±è´¥'}
        
        # è¯„ä¼°åˆå§‹æ€§èƒ½
        logger.info("ğŸ“Š è¯„ä¼°åˆå§‹æ€§èƒ½...")
        initial_perf = self.evaluate_stage_performance(stage_name)
        if 'error' in initial_perf:
            return initial_perf
        
        initial_wr = initial_perf['win_rate']
        target_wr = initial_wr + target_improvement
        threshold = stage.success_threshold
        
        logger.info(f"åˆå§‹èƒœç‡: {initial_wr*100:.1f}%")
        logger.info(f"ç›®æ ‡èƒœç‡: {target_wr*100:.1f}%")
        logger.info(f"é˜ˆå€¼: {threshold*100:.1f}%")
        
        # å¾®è°ƒé…ç½®
        fine_tune_config = self._get_fine_tune_config(stage_name)
        
        # åº”ç”¨å¾®è°ƒé…ç½®
        self._apply_fine_tune_config(fine_tune_config)
        
        # è®­ç»ƒå†å²
        training_history = {
            'initial_performance': initial_perf,
            'episodes': [],
            'best_performance': initial_perf,
            'best_episode': 0
        }
        
        best_wr = initial_wr
        no_improvement_count = 0
        early_stop_patience = 5
        
        # å¾®è°ƒè®­ç»ƒå¾ªç¯
        for episode in range(1, max_episodes + 1):
            logger.info(f"ğŸ”„ å¾®è°ƒè½®æ¬¡ {episode}/{max_episodes}")
            
            # ç”Ÿæˆè®­ç»ƒæ•°æ®
            train_data = self.trainer._generate_stage_data(stage, num_samples=50)
            
            # è®­ç»ƒä¸€ä¸ªepisode
            episode_metrics = self._train_episode(train_data, fine_tune_config)
            
            # å®šæœŸè¯„ä¼°
            if episode % eval_interval == 0 or episode == max_episodes:
                logger.info(f"ğŸ“Š è¯„ä¼°è½®æ¬¡ {episode} æ€§èƒ½...")
                current_perf = self.evaluate_stage_performance(stage_name, n_samples=100)
                
                if 'error' not in current_perf:
                    current_wr = current_perf['win_rate']
                    
                    episode_record = {
                        'episode': episode,
                        'win_rate': current_wr,
                        'improvement': current_wr - initial_wr,
                        'metrics': episode_metrics,
                        'performance': current_perf
                    }
                    training_history['episodes'].append(episode_record)
                    
                    logger.info(f"å½“å‰èƒœç‡: {current_wr*100:.1f}% (æå‡: {(current_wr-initial_wr)*100:+.1f}%)")
                    
                    # æ›´æ–°æœ€ä½³æ€§èƒ½
                    if current_wr > best_wr:
                        best_wr = current_wr
                        training_history['best_performance'] = current_perf
                        training_history['best_episode'] = episode
                        no_improvement_count = 0
                        
                        # ä¿å­˜æœ€ä½³æ¨¡å‹
                        self._save_fine_tuned_model(stage_name, episode, current_perf)
                        logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ€§èƒ½! å·²ä¿å­˜æ¨¡å‹")
                    else:
                        no_improvement_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                    if current_wr >= target_wr:
                        logger.info(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡èƒœç‡! {current_wr*100:.1f}% >= {target_wr*100:.1f}%")
                        break
                    
                    # æ—©åœæ£€æŸ¥
                    if no_improvement_count >= early_stop_patience:
                        logger.info(f"â¹ï¸ æ—©åœ: {early_stop_patience} è½®æ— æ”¹å–„")
                        break
        
        # æœ€ç»ˆè¯„ä¼°
        logger.info("ğŸ“Š æœ€ç»ˆæ€§èƒ½è¯„ä¼°...")
        final_perf = self.evaluate_stage_performance(stage_name, n_samples=200)
        
        training_history['final_performance'] = final_perf
        training_history['total_episodes'] = episode
        training_history['target_achieved'] = (
            final_perf.get('win_rate', 0) >= target_wr if 'error' not in final_perf else False
        )
        
        # ä¿å­˜è®­ç»ƒå†å²
        self._save_training_history(stage_name, training_history)
        
        logger.info(f"âœ… å¾®è°ƒå®Œæˆ: {stage_name}")
        if 'error' not in final_perf:
            final_wr = final_perf['win_rate']
            total_improvement = final_wr - initial_wr
            logger.info(f"æœ€ç»ˆèƒœç‡: {final_wr*100:.1f}% (æ€»æå‡: {total_improvement*100:+.1f}%)")
        
        return training_history
    
    def _get_fine_tune_config(self, stage_name: str) -> Dict:
        """è·å–å¾®è°ƒé…ç½®"""
        base_config = {
            'learning_rate_factor': 0.7,  # é™ä½å­¦ä¹ ç‡
            'entropy_coef_end': 0.003,    # ç•¥æ”¶æ¢ç´¢
            'ppo_epochs': 8,              # å¢åŠ PPOè½®æ•°
            'freeze_encoder_episodes': 10, # å‰10è½®å†»ç»“ç¼–ç å™¨
            'advantage_normalization': True,
            'nan_to_num': True,
            'early_stopping_patience': 5
        }
        
        # æ ¹æ®é˜¶æ®µç‰¹ç‚¹è°ƒæ•´é…ç½®
        stage_specific = {
            'åŸºç¡€é˜¶æ®µ': {'learning_rate_factor': 0.8},
            'åˆçº§é˜¶æ®µ': {'learning_rate_factor': 0.7},
            'ä¸­çº§é˜¶æ®µ': {'learning_rate_factor': 0.6, 'ppo_epochs': 10},
            'é«˜çº§é˜¶æ®µ': {'learning_rate_factor': 0.5, 'ppo_epochs': 12},
            'ä¸“å®¶é˜¶æ®µ': {'learning_rate_factor': 0.4, 'ppo_epochs': 15}
        }
        
        if stage_name in stage_specific:
            base_config.update(stage_specific[stage_name])
        
        return base_config
    
    def _apply_fine_tune_config(self, config: Dict):
        """åº”ç”¨å¾®è°ƒé…ç½®"""
        # è°ƒæ•´å­¦ä¹ ç‡
        if 'learning_rate_factor' in config:
            for param_group in self.agent.optimizer.param_groups:
                param_group['lr'] *= config['learning_rate_factor']
            logger.info(f"è°ƒæ•´å­¦ä¹ ç‡: x{config['learning_rate_factor']}")
        
        # è°ƒæ•´å…¶ä»–è¶…å‚æ•°
        if hasattr(self.agent, 'entropy_coef'):
            self.agent.entropy_coef = config.get('entropy_coef_end', 0.003)
        
        if hasattr(self.agent, 'ppo_epochs'):
            self.agent.ppo_epochs = config.get('ppo_epochs', 8)
    
    def _train_episode(self, train_data: List, config: Dict) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        # è¿™é‡Œåº”è¯¥å®ç°å…·ä½“çš„è®­ç»ƒé€»è¾‘
        # ç”±äºåŸå§‹è®­ç»ƒä»£ç æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–çš„æ¡†æ¶
        
        metrics = {
            'avg_reward': 0.0,
            'policy_loss': 0.0,
            'value_loss': 0.0,
            'entropy': 0.0
        }
        
        # TODO: å®ç°å…·ä½“çš„è®­ç»ƒé€»è¾‘
        # 1. ä»train_dataç”Ÿæˆç»éªŒ
        # 2. è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        # 3. æ‰§è¡ŒPPOæ›´æ–°
        # 4. è®°å½•è®­ç»ƒæŒ‡æ ‡
        
        return metrics
    
    def _save_fine_tuned_model(self, stage_name: str, episode: int, performance: Dict):
        """ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹"""
        output_dir = Path(f"../../models/fine_tuned/{self.port_name}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = output_dir / f"stage_{stage_name}_fine_tuned_ep{episode}.pt"
        
        checkpoint = {
            'model_state_dict': self.agent.actor_critic.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'episode': episode,
            'performance': performance,
            'port_name': self.port_name,
            'stage_name': stage_name
        }
        
        torch.save(checkpoint, model_path)
        logger.info(f"ä¿å­˜å¾®è°ƒæ¨¡å‹: {model_path}")
    
    def _save_training_history(self, stage_name: str, history: Dict):
        """ä¿å­˜è®­ç»ƒå†å²"""
        output_dir = Path(f"../../models/fine_tuned/{self.port_name}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        history_path = output_dir / f"stage_{stage_name}_fine_tune_history.json"
        
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ä¿å­˜è®­ç»ƒå†å²: {history_path}")

def identify_risk_stages(results_dir: str, margin_threshold: float = 0.05) -> List[Tuple[str, str, float]]:
    """è¯†åˆ«é£é™©é˜¶æ®µï¼ˆä½™é‡å°äºé˜ˆå€¼çš„ï¼‰"""
    risk_stages = []
    
    results_path = Path(results_dir)
    for json_file in results_path.glob("consistency_*.json"):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            port_name = data.get('port', 'unknown')
            stages = data.get('stages', [])
            
            for stage in stages:
                if stage.get('pass', False):  # åªè€ƒè™‘é€šè¿‡çš„é˜¶æ®µ
                    win_rate = stage.get('win_rate', 0.0)
                    threshold = stage.get('threshold', 0.0)
                    margin = win_rate - threshold
                    
                    if 0 <= margin <= margin_threshold:  # ä½™é‡åœ¨0åˆ°é˜ˆå€¼ä¹‹é—´
                        risk_stages.append((port_name, stage.get('stage', ''), margin))
        
        except Exception as e:
            logger.warning(f"æ— æ³•è§£ææ–‡ä»¶ {json_file}: {e}")
    
    # æŒ‰ä½™é‡æ’åº
    risk_stages.sort(key=lambda x: x[2])
    return risk_stages

def main():
    parser = argparse.ArgumentParser(description="é˜¶æ®µå¾®è°ƒè„šæœ¬")
    parser.add_argument("--port", help="æ¸¯å£åç§°")
    parser.add_argument("--stage", help="é˜¶æ®µåç§°")
    parser.add_argument("--target-improvement", type=float, default=0.05,
                       help="ç›®æ ‡æå‡å¹…åº¦ (é»˜è®¤5%%)")
    parser.add_argument("--max-episodes", type=int, default=40,
                       help="æœ€å¤§è®­ç»ƒè½®æ•°")
    parser.add_argument("--device", default="cpu", help="è®¾å¤‡ç±»å‹")
    parser.add_argument("--auto-identify", action="store_true",
                       help="è‡ªåŠ¨è¯†åˆ«é£é™©é˜¶æ®µ")
    parser.add_argument("--results-dir", default="../../models/releases/2025-08-07",
                       help="æµ‹è¯•ç»“æœç›®å½•")
    parser.add_argument("--margin-threshold", type=float, default=0.05,
                       help="é£é™©ä½™é‡é˜ˆå€¼")
    
    args = parser.parse_args()
    
    if args.auto_identify:
        logger.info("ğŸ” è‡ªåŠ¨è¯†åˆ«é£é™©é˜¶æ®µ...")
        risk_stages = identify_risk_stages(args.results_dir, args.margin_threshold)
        
        if not risk_stages:
            logger.info("âœ… æœªå‘ç°é£é™©é˜¶æ®µ")
            return
        
        logger.info(f"âš ï¸ å‘ç° {len(risk_stages)} ä¸ªé£é™©é˜¶æ®µ:")
        for port, stage, margin in risk_stages:
            logger.info(f"  - {port}/{stage}: ä½™é‡ {margin*100:.1f}%")
        
        # å¾®è°ƒå‰å‡ ä¸ªæœ€é«˜é£é™©çš„é˜¶æ®µ
        for port, stage, margin in risk_stages[:3]:  # åªå¤„ç†å‰3ä¸ª
            logger.info(f"\nğŸ¯ å¾®è°ƒé£é™©é˜¶æ®µ: {port}/{stage}")
            
            finetuner = StageFinetuner(port, args.device)
            result = finetuner.fine_tune_stage(
                stage, 
                target_improvement=args.target_improvement,
                max_episodes=args.max_episodes
            )
            
            if 'error' in result:
                logger.error(f"âŒ å¾®è°ƒå¤±è´¥: {result['error']}")
            else:
                logger.info(f"âœ… å¾®è°ƒå®Œæˆ: {port}/{stage}")
    
    elif args.port and args.stage:
        # å¾®è°ƒæŒ‡å®šé˜¶æ®µ
        logger.info(f"ğŸ¯ å¾®è°ƒæŒ‡å®šé˜¶æ®µ: {args.port}/{args.stage}")
        
        finetuner = StageFinetuner(args.port, args.device)
        result = finetuner.fine_tune_stage(
            args.stage,
            target_improvement=args.target_improvement,
            max_episodes=args.max_episodes
        )
        
        if 'error' in result:
            logger.error(f"âŒ å¾®è°ƒå¤±è´¥: {result['error']}")
        else:
            logger.info(f"âœ… å¾®è°ƒå®Œæˆ")
            
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            if 'final_performance' in result and 'error' not in result['final_performance']:
                final_wr = result['final_performance']['win_rate']
                initial_wr = result['initial_performance']['win_rate']
                improvement = final_wr - initial_wr
                
                print(f"\nğŸ“Š å¾®è°ƒç»“æœæ‘˜è¦:")
                print(f"åˆå§‹èƒœç‡: {initial_wr*100:.1f}%")
                print(f"æœ€ç»ˆèƒœç‡: {final_wr*100:.1f}%")
                print(f"æå‡å¹…åº¦: {improvement*100:+.1f}%")
                print(f"è®­ç»ƒè½®æ•°: {result['total_episodes']}")
                print(f"ç›®æ ‡è¾¾æˆ: {'âœ…' if result['target_achieved'] else 'âŒ'}")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()