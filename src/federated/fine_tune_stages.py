#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é˜¶æ®µå¾®è°ƒè„šæœ¬ - é’ˆå¯¹æ¥è¿‘é˜ˆå€¼çš„é˜¶æ®µè¿›è¡ŒçŸ­æœŸè¡¥è®­
åŸºäºè·¯çº¿Bçš„è®­ç»ƒæ”¹è¿›æ–¹æ¡ˆï¼Œç”¨äºæå‡æ¨¡å‹æ€§èƒ½
"""
import os
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import torch
import numpy as np

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from curriculum_trainer import CurriculumTrainer, build_agent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def _eval_one_stage_compat(trainer, agent, stage, n_samples=200):
    """å…¼å®¹æ€§wrapper for eval_one_stage"""
    try:
        from consistency_test_fixed import eval_one_stage as _e
        out = _e(trainer, agent, stage, n_samples=n_samples)
        
        # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
        if isinstance(out, tuple):
            if len(out) == 4:
                # å½“å‰ç‰ˆæœ¬ï¼šè¿”å› (perf_dict, std, stable, n_samples) å…ƒç»„
                perf, std, stable, samples = out
                if isinstance(perf, dict):
                    result = perf.copy()
                    result["std"] = float(std)
                    result["stable"] = bool(stable)
                    result["n_samples"] = int(samples)
                    return result
            elif len(out) == 3:
                # è€ç‰ˆæœ¬ï¼šè¿”å› (perf, std, stable) å…ƒç»„
                perf, std, stable = out
                if isinstance(perf, dict):
                    result = perf.copy()
                else:
                    result = {"win_rate": float(perf)}
                result["std"] = float(std)
                result["stable"] = bool(stable)
                return result
        elif isinstance(out, dict):
            # ç›´æ¥è¿”å› dict
            return out
        else:
            # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢
            return {"win_rate": float(out), "std": 0.0, "stable": True}
    except Exception as e:
        logger.error(f"eval_one_stageè°ƒç”¨å¤±è´¥: {e}")
        return {"error": str(e)}

class StageFinetuner:
    """é˜¶æ®µå¾®è°ƒå™¨"""
    
    def __init__(self, port_name: str, device: str = "cpu"):
        self.port_name = port_name
        self.device = torch.device(device)
        self.trainer = CurriculumTrainer(port_name)
        self.agent = build_agent(port_name, device=self.device)
        self.current_stage = None
        
        logger.info(f"åˆå§‹åŒ–é˜¶æ®µå¾®è°ƒå™¨ - æ¸¯å£: {port_name}, è®¾å¤‡: {device}")
    
    def get_stage_by_name(self, stage_name: str):
        """æ ¹æ®åç§°è·å–é˜¶æ®µé…ç½®"""
        for stage in self.trainer.curriculum_stages:
            if stage.name == stage_name:
                return stage
        return None
    
    def load_stage_model(self, stage_name: str) -> bool:
        """åŠ è½½é˜¶æ®µæ¨¡å‹"""
        model_path = Path(f"../../models/curriculum_v2/{self.port_name}/stage_{stage_name}_best.pt")
        
        if not model_path.exists():
            logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
        
        try:
            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
            if 'model_state_dict' in checkpoint:
                self.agent.actor_critic.load_state_dict(checkpoint['model_state_dict'])
                if 'optimizer_state_dict' in checkpoint:
                    self.agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            else:
                self.agent.actor_critic.load_state_dict(checkpoint)
            
            logger.info(f"âœ… åŠ è½½æ¨¡å‹: {model_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def evaluate_stage_performance(self, stage_name: str, n_samples: int = 200) -> Dict:
        """è¯„ä¼°é˜¶æ®µæ€§èƒ½"""
        stage = self.get_stage_by_name(stage_name)
        if stage is None:
            return {'error': f'æœªæ‰¾åˆ°é˜¶æ®µ: {stage_name}'}
        
        try:
            result = _eval_one_stage_compat(
                self.trainer, self.agent, stage, n_samples=n_samples
            )
            return result
        except Exception as e:
            logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def fine_tune_stage(self, stage_name: str, target_improvement: float = 0.05,
                       max_episodes: int = 40, eval_interval: int = 5) -> Dict:
        """å¾®è°ƒé˜¶æ®µæ¨¡å‹"""
        logger.info(f"ğŸ¯ å¼€å§‹å¾®è°ƒé˜¶æ®µ: {stage_name}")
        logger.info(f"ç›®æ ‡æå‡: {target_improvement*100:.1f}%, æœ€å¤§è½®æ•°: {max_episodes}")
        
        stage = self.get_stage_by_name(stage_name)
        if stage is None:
            return {'error': f'æœªæ‰¾åˆ°é˜¶æ®µ: {stage_name}'}
        
        # åŠ è½½æ¨¡å‹
        if not self.load_stage_model(stage_name):
            return {'error': 'æ¨¡å‹åŠ è½½å¤±è´¥'}
        
        # è®¾ç½®å½“å‰é˜¶æ®µ
        self.current_stage = stage
        
        # è¯„ä¼°åˆå§‹æ€§èƒ½
        logger.info("ğŸ“Š è¯„ä¼°åˆå§‹æ€§èƒ½...")
        initial_perf = self.evaluate_stage_performance(stage_name)
        if 'error' in initial_perf:
            return initial_perf
        
        initial_wr = initial_perf['win_rate']
        target_wr = initial_wr + target_improvement
        threshold = stage.success_threshold
        
        logger.info(f"åˆå§‹èƒœç‡: {initial_wr*100:.1f}%")
        logger.info(f"ç›®æ ‡èƒœç‡: {target_wr*100:.1f}%")
        logger.info(f"é˜ˆå€¼: {threshold*100:.1f}%")
        
        # å¾®è°ƒé…ç½®
        fine_tune_config = self._get_fine_tune_config(stage_name)
        
        # åº”ç”¨å¾®è°ƒé…ç½®
        self._apply_fine_tune_config(fine_tune_config)
        
        # è®­ç»ƒå†å²
        training_history = {
            'initial_performance': initial_perf,
            'episodes': [],
            'best_performance': initial_perf,
            'best_episode': 0
        }
        
        best_wr = initial_wr
        no_improvement_count = 0
        early_stop_patience = 5
        
        # å¾®è°ƒè®­ç»ƒå¾ªç¯
        for episode in range(1, max_episodes + 1):
            logger.info(f"ğŸ”„ å¾®è°ƒè½®æ¬¡ {episode}/{max_episodes}")
            
            # ç”Ÿæˆè®­ç»ƒæ•°æ®
            train_data = self.trainer._generate_stage_data(stage, num_samples=50)
            
            # è®­ç»ƒä¸€ä¸ªepisode
            episode_metrics = self._train_episode(train_data, fine_tune_config)
            
            # å®šæœŸè¯„ä¼°
            if episode % eval_interval == 0 or episode == max_episodes:
                logger.info(f"ğŸ“Š è¯„ä¼°è½®æ¬¡ {episode} æ€§èƒ½...")
                current_perf = self.evaluate_stage_performance(stage_name, n_samples=100)
                
                if 'error' not in current_perf:
                    current_wr = current_perf['win_rate']
                    
                    episode_record = {
                        'episode': episode,
                        'win_rate': current_wr,
                        'improvement': current_wr - initial_wr,
                        'metrics': episode_metrics,
                        'performance': current_perf
                    }
                    training_history['episodes'].append(episode_record)
                    
                    logger.info(f"å½“å‰èƒœç‡: {current_wr*100:.1f}% (æå‡: {(current_wr-initial_wr)*100:+.1f}%)")
                    
                    # æ›´æ–°æœ€ä½³æ€§èƒ½
                    if current_wr > best_wr:
                        best_wr = current_wr
                        training_history['best_performance'] = current_perf
                        training_history['best_episode'] = episode
                        no_improvement_count = 0
                        
                        # ä¿å­˜æœ€ä½³æ¨¡å‹
                        self._save_fine_tuned_model(stage_name, episode, current_perf)
                        logger.info(f"ğŸ† æ–°çš„æœ€ä½³æ€§èƒ½! å·²ä¿å­˜æ¨¡å‹")
                    else:
                        no_improvement_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
                    if current_wr >= target_wr:
                        logger.info(f"ğŸ‰ è¾¾åˆ°ç›®æ ‡èƒœç‡! {current_wr*100:.1f}% >= {target_wr*100:.1f}%")
                        break
                    
                    # æ—©åœæ£€æŸ¥
                    if no_improvement_count >= early_stop_patience:
                        logger.info(f"â¹ï¸ æ—©åœ: {early_stop_patience} è½®æ— æ”¹å–„")
                        break
        
        # æœ€ç»ˆè¯„ä¼°
        logger.info("ğŸ“Š æœ€ç»ˆæ€§èƒ½è¯„ä¼°...")
        final_perf = self.evaluate_stage_performance(stage_name, n_samples=200)
        
        training_history['final_performance'] = final_perf
        training_history['total_episodes'] = episode
        training_history['target_achieved'] = (
            final_perf.get('win_rate', 0) >= target_wr if 'error' not in final_perf else False
        )
        
        # ä¿å­˜è®­ç»ƒå†å²
        self._save_training_history(stage_name, training_history)
        
        logger.info(f"âœ… å¾®è°ƒå®Œæˆ: {stage_name}")
        if 'error' not in final_perf:
            final_wr = final_perf['win_rate']
            total_improvement = final_wr - initial_wr
            logger.info(f"æœ€ç»ˆèƒœç‡: {final_wr*100:.1f}% (æ€»æå‡: {total_improvement*100:+.1f}%)")
        
        return training_history
    
    def _get_fine_tune_config(self, stage_name: str) -> Dict:
        """è·å–å¾®è°ƒé…ç½®"""
        base_config = {
            'learning_rate_factor': 0.7,  # é™ä½å­¦ä¹ ç‡
            'entropy_coef_end': 0.003,    # ç•¥æ”¶æ¢ç´¢
            'ppo_epochs': 8,              # å¢åŠ PPOè½®æ•°
            'freeze_encoder_episodes': 10, # å‰10è½®å†»ç»“ç¼–ç å™¨
            'advantage_normalization': True,
            'nan_to_num': True,
            'early_stopping_patience': 5
        }
        
        # æ ¹æ®é˜¶æ®µç‰¹ç‚¹è°ƒæ•´é…ç½®
        stage_specific = {
            'åŸºç¡€é˜¶æ®µ': {'learning_rate_factor': 0.8},
            'åˆçº§é˜¶æ®µ': {'learning_rate_factor': 0.7},
            'ä¸­çº§é˜¶æ®µ': {'learning_rate_factor': 0.6, 'ppo_epochs': 10},
            'é«˜çº§é˜¶æ®µ': {'learning_rate_factor': 0.5, 'ppo_epochs': 12},
            'ä¸“å®¶é˜¶æ®µ': {'learning_rate_factor': 0.4, 'ppo_epochs': 15}
        }
        
        if stage_name in stage_specific:
            base_config.update(stage_specific[stage_name])
        
        return base_config
    
    def _apply_fine_tune_config(self, config: Dict):
        """åº”ç”¨å¾®è°ƒé…ç½®"""
        # è°ƒæ•´å­¦ä¹ ç‡
        if 'learning_rate_factor' in config:
            for param_group in self.agent.optimizer.param_groups:
                param_group['lr'] *= config['learning_rate_factor']
            logger.info(f"è°ƒæ•´å­¦ä¹ ç‡: x{config['learning_rate_factor']}")
        
        # è°ƒæ•´å…¶ä»–è¶…å‚æ•°
        if hasattr(self.agent, 'entropy_coef'):
            self.agent.entropy_coef = config.get('entropy_coef_end', 0.003)
        
        if hasattr(self.agent, 'ppo_epochs'):
            self.agent.ppo_epochs = config.get('ppo_epochs', 8)
    
    def _train_episode(self, train_data: List, config: Dict) -> Dict:
        """
        æœ€å°å¯ç”¨çš„ PPO å¾®è°ƒå›åˆï¼š
        1) ç”¨å½“å‰ç­–ç•¥é‡‡æ ·è‹¥å¹²äº¤äº’æ ·æœ¬å¹¶å­˜å…¥ buffer
        2) è°ƒ agent.update() åšå¤šæ¬¡å°æ­¥æ›´æ–°
        """
        stage = self.current_stage
        assert stage is not None, "current_stage æœªè®¾ç½®"
        steps, total_reward = 0, 0.0
        num_updates, total_loss = 0, 0.0

        # é‡‡æ ·å­é›†ï¼Œé¿å…ä¸€æ¬¡å¡å¤ªå¤š
        batch = train_data[: min(80, len(train_data))]

        for dp in batch:
            # 1) å‰å‘ä¸åŠ¨ä½œé‡‡æ ·
            state = self.trainer._extract_state_from_data(dp)
            node_feats, adj = self.trainer._extract_graph_features_from_data(dp)
            try:
                with torch.no_grad():
                    ap, value = self.agent.actor_critic(
                        torch.as_tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0),
                        torch.as_tensor(node_feats, dtype=torch.float32, device=self.device).unsqueeze(0),
                        self.trainer._prep_adj_3d(adj)
                    )
                ap = torch.nan_to_num(ap, nan=0.0, posinf=0.0, neginf=0.0)
                num_actions = stage.max_berths
                if ap.shape[-1] != num_actions:
                    ap = ap[..., :num_actions]
                ap = ap / ap.sum(dim=-1, keepdim=True).clamp_min(1e-8)
                dist = torch.distributions.Categorical(ap)
                action = int(dist.sample().item())
                logp = dist.log_prob(torch.tensor(action, device=self.device)).item()
                val = float(value.squeeze(-1).item()) if hasattr(value, "item") else 0.0
            except Exception:
                # å…œåº•ï¼šå®Œå…¨éšæœºåŠ¨ä½œ
                num_actions = stage.max_berths
                action, logp, val = np.random.randint(0, num_actions), 0.0, 0.0

            # 2) ç¯å¢ƒ"å¥–åŠ±"è®¡ç®—ï¼ˆç”¨ä½ å·²æœ‰çš„ç«¯å£å¥–åŠ±ï¼‰
            reward = float(self.trainer._calculate_stage_reward(dp, action, stage))
            total_reward += reward
            steps += 1

            # 3) å­˜ç»éªŒï¼ˆç”¨ next_state=state, done=False ç®€åŒ–ï¼‰
            try:
                self.agent.store_experience(
                    np.asarray(state, dtype=np.float32),
                    np.asarray(node_feats, dtype=np.float32),
                    np.asarray(adj, dtype=np.float32),
                    int(action),
                    float(reward),
                    np.asarray(state, dtype=np.float32),
                    False,
                    float(logp),
                    float(val),
                )
            except Exception:
                continue

        # 4) å¤šæ¬¡å°æ­¥æ›´æ–°
        for _ in range(config.get('ppo_updates_per_episode', 4)):
            try:
                out = self.agent.update()
                if isinstance(out, dict):
                    total_loss += float(out.get('total_loss', out.get('loss', 0.0)))
                elif out is not None:
                    total_loss += float(out)
                num_updates += 1
            except Exception:
                break

        return {
            'avg_reward': (total_reward / max(1, steps)),
            'updates': num_updates,
            'loss': (total_loss / max(1, num_updates)),
            'steps': steps
        }
    
    def _save_fine_tuned_model(self, stage_name: str, episode: int, performance: Dict):
        """ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹"""
        output_dir = Path(f"../../models/fine_tuned/{self.port_name}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = output_dir / f"stage_{stage_name}_fine_tuned_ep{episode}.pt"
        
        checkpoint = {
            'model_state_dict': self.agent.actor_critic.state_dict(),
            'optimizer_state_dict': self.agent.optimizer.state_dict(),
            'episode': episode,
            'performance': performance,
            'port_name': self.port_name,
            'stage_name': stage_name
        }
        
        torch.save(checkpoint, model_path)
        logger.info(f"ä¿å­˜å¾®è°ƒæ¨¡å‹: {model_path}")
    
    def _save_training_history(self, stage_name: str, history: Dict):
        """ä¿å­˜è®­ç»ƒå†å²"""
        output_dir = Path(f"../../models/fine_tuned/{self.port_name}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        history_path = output_dir / f"stage_{stage_name}_fine_tune_history.json"
        
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ä¿å­˜è®­ç»ƒå†å²: {history_path}")

def identify_risk_stages(results_dir: str, margin_threshold: float = 0.05) -> List[Tuple[str, str, float]]:
    """è¯†åˆ«é£é™©é˜¶æ®µï¼ˆä½™é‡å°äºé˜ˆå€¼çš„ï¼‰"""
    risk_stages = []
    
    results_path = Path(results_dir)
    for json_file in results_path.glob("consistency_*.json"):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            port_name = data.get('port', 'unknown')
            stages = data.get('stages', [])
            
            for stage in stages:
                if stage.get('pass', False):  # åªè€ƒè™‘é€šè¿‡çš„é˜¶æ®µ
                    win_rate = stage.get('win_rate', 0.0)
                    threshold = stage.get('threshold', 0.0)
                    margin = win_rate - threshold
                    
                    if 0 <= margin <= margin_threshold:  # ä½™é‡åœ¨0åˆ°é˜ˆå€¼ä¹‹é—´
                        risk_stages.append((port_name, stage.get('stage', ''), margin))
        
        except Exception as e:
            logger.warning(f"æ— æ³•è§£ææ–‡ä»¶ {json_file}: {e}")
    
    # æŒ‰ä½™é‡æ’åº
    risk_stages.sort(key=lambda x: x[2])
    return risk_stages

def main():
    parser = argparse.ArgumentParser(
        description="é˜¶æ®µå¾®è°ƒè„šæœ¬",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--port", help="æ¸¯å£åç§°")
    parser.add_argument("--stage", help="é˜¶æ®µåç§°")
    parser.add_argument("--target-improvement", type=float, default=0.05,
                       help="ç›®æ ‡æå‡å¹…åº¦ï¼ˆé»˜è®¤=0.05ï¼Œå¯¹åº”+5ä¸ªç™¾åˆ†ç‚¹ï¼‰")
    parser.add_argument("--max-episodes", type=int, default=40,
                       help="æœ€å¤§è®­ç»ƒè½®æ•°")
    parser.add_argument("--device", default="cpu", help="è®¾å¤‡ç±»å‹")
    parser.add_argument("--auto-identify", action="store_true",
                       help="è‡ªåŠ¨è¯†åˆ«é£é™©é˜¶æ®µ")
    parser.add_argument("--results-dir", default="../../models/releases/2025-08-07",
                       help="æµ‹è¯•ç»“æœç›®å½•")
    parser.add_argument("--margin-threshold", type=float, default=0.05,
                       help="é£é™©ä½™é‡é˜ˆå€¼ï¼ˆé»˜è®¤=0.05ï¼Œå¯¹åº”5ä¸ªç™¾åˆ†ç‚¹ï¼‰")
    
    args = parser.parse_args()
    
    if args.auto_identify:
        logger.info("ğŸ” è‡ªåŠ¨è¯†åˆ«é£é™©é˜¶æ®µ...")
        risk_stages = identify_risk_stages(args.results_dir, args.margin_threshold)
        
        if not risk_stages:
            logger.info("âœ… æœªå‘ç°é£é™©é˜¶æ®µ")
            return
        
        logger.info(f"âš ï¸ å‘ç° {len(risk_stages)} ä¸ªé£é™©é˜¶æ®µ:")
        for port, stage, margin in risk_stages:
            logger.info(f"  - {port}/{stage}: ä½™é‡ {margin*100:.1f}%")
        
        # å¾®è°ƒå‰å‡ ä¸ªæœ€é«˜é£é™©çš„é˜¶æ®µ
        for port, stage, margin in risk_stages[:3]:  # åªå¤„ç†å‰3ä¸ª
            logger.info(f"\nğŸ¯ å¾®è°ƒé£é™©é˜¶æ®µ: {port}/{stage}")
            
            finetuner = StageFinetuner(port, args.device)
            result = finetuner.fine_tune_stage(
                stage, 
                target_improvement=args.target_improvement,
                max_episodes=args.max_episodes
            )
            
            if 'error' in result:
                logger.error(f"âŒ å¾®è°ƒå¤±è´¥: {result['error']}")
            else:
                logger.info(f"âœ… å¾®è°ƒå®Œæˆ: {port}/{stage}")
    
    elif args.port and args.stage:
        # å¾®è°ƒæŒ‡å®šé˜¶æ®µ
        logger.info(f"ğŸ¯ å¾®è°ƒæŒ‡å®šé˜¶æ®µ: {args.port}/{args.stage}")
        
        finetuner = StageFinetuner(args.port, args.device)
        result = finetuner.fine_tune_stage(
            args.stage,
            target_improvement=args.target_improvement,
            max_episodes=args.max_episodes
        )
        
        if 'error' in result:
            logger.error(f"âŒ å¾®è°ƒå¤±è´¥: {result['error']}")
        else:
            logger.info(f"âœ… å¾®è°ƒå®Œæˆ")
            
            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            if 'final_performance' in result and 'error' not in result['final_performance']:
                final_wr = result['final_performance']['win_rate']
                initial_wr = result['initial_performance']['win_rate']
                improvement = final_wr - initial_wr
                
                print(f"\nğŸ“Š å¾®è°ƒç»“æœæ‘˜è¦:")
                print(f"åˆå§‹èƒœç‡: {initial_wr*100:.1f}%")
                print(f"æœ€ç»ˆèƒœç‡: {final_wr*100:.1f}%")
                print(f"æå‡å¹…åº¦: {improvement*100:+.1f}%")
                print(f"è®­ç»ƒè½®æ•°: {result['total_episodes']}")
                print(f"ç›®æ ‡è¾¾æˆ: {'âœ…' if result['target_achieved'] else 'âŒ'}")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()