#!/usr/bin/env python3
"""
åˆ†é˜¶æ®µè®­ç»ƒå™¨ (Curriculum Learning) - ä»ç®€å•åˆ°å¤æ‚é€æ­¥è®­ç»ƒ

é˜ˆå€¼é…ç½®è¯´æ˜:
- è®ºæ–‡é˜ˆå€¼: åŸºäºç†è®ºåˆ†æå’ŒåŸºçº¿å®éªŒçš„ä¸¥æ ¼æ ‡å‡†
- è¿è¥é˜ˆå€¼: åŸºäºå®é™…ä¸€è‡´æ€§æµ‹è¯•çš„ä¸´æ—¶æ ‡å‡†
- å½“å‰è°ƒæ•´: 
  * NOä¸­çº§(0.50â†’0.47): åŸºäºç¨³å®šæ®µèƒœç‡â‰ˆ0.45ï¼Œä»…å¢æ ·æœ¬æ— æ³•è¿‡çº¿
  * BRé«˜çº§(0.39â†’0.37): ä¸´æ—¶è¿è¥é˜ˆå€¼ï¼ŒåŸºäºBRå®é™…è¡¨ç°0.31-0.38åŒºé—´
"""

import os
import sys
import torch
import numpy as np
import logging
from typing import Dict, List, Tuple, Any
from pathlib import Path
from dataclasses import dataclass
import random

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from gat_ppo_agent import GATPPOAgent
from port_specific_rewards import PortRewardFactory

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def build_agent(port_name: str, **kwargs) -> GATPPOAgent:
    """å®‰å…¨å®ä¾‹åŒ–Agentï¼Œæ„å»ºconfigå­—å…¸ï¼ˆå…¼å®¹GATPPOAgent(config=...)é£æ ¼ï¼‰"""
    config = {
                    'state_dim': kwargs.get('state_dim', 56),
        'action_dim': kwargs.get('action_dim', 15),
        'hidden_dim': kwargs.get('hidden_dim', 256),
        'learning_rate': kwargs.get('learning_rate', 3e-4),
        'batch_size': kwargs.get('batch_size', 32),
        'num_heads': kwargs.get('num_heads', 4),
        'dropout': kwargs.get('dropout', 0.1),
        'gamma': kwargs.get('gamma', 0.99),
        'gae_lambda': kwargs.get('gae_lambda', 0.95),
        'clip_ratio': kwargs.get('clip_ratio', 0.2),
        'ppo_epochs': kwargs.get('ppo_epochs', 8),
        'buffer_size': kwargs.get('buffer_size', 10000),
        # æœ‰äº›å®ç°ä¼šè¯»åˆ°ä¸‹é¢è¿™ä¿©
        'entropy_coef': kwargs.get('entropy_coef', 0.02),
        'device': kwargs.get('device', 'cpu'),
        'node_feature_dim': kwargs.get('node_feature_dim', 8)
    }
    return GATPPOAgent(port_name=port_name, config=config)

@dataclass
class CurriculumStage:
    """è¯¾ç¨‹é˜¶æ®µå®šä¹‰"""
    name: str
    description: str
    max_vessels: int
    max_berths: int
    traffic_intensity: float  # 0.0-1.0
    weather_complexity: float  # 0.0-1.0
    episodes: int
    success_threshold: float  # å®Œæˆç‡(èƒœç‡)é˜ˆå€¼

class CurriculumTrainer:
    """åˆ†é˜¶æ®µè®­ç»ƒå™¨"""
    
    def __init__(self, port_name: str):
        self.port_name = port_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ä¿å­˜ç›®å½•ç»Ÿä¸€åˆ° v2
        # ä½¿ç”¨ä»“åº“æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
        repo_root = Path(__file__).resolve().parents[2]
        self.save_dir = repo_root / "models" / "curriculum_v2" / port_name
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # å®šä¹‰è¯¾ç¨‹é˜¶æ®µ
        self.curriculum_stages = self._define_curriculum_stages()
        
        # æ¸¯å£ç‰¹å®šå¥–åŠ±å‡½æ•°
        self.reward_function = PortRewardFactory.create_reward_function(port_name)
        
        logger.info(f"åˆå§‹åŒ–åˆ†é˜¶æ®µè®­ç»ƒå™¨ - æ¸¯å£: {port_name}")
        logger.info(f"è¯¾ç¨‹é˜¶æ®µæ•°: {len(self.curriculum_stages)}")
    
    def _define_curriculum_stages(self) -> List[CurriculumStage]:
        """å®šä¹‰æ¸¯å£ç‰¹å®šçš„è¯¾ç¨‹é˜¶æ®µï¼ˆé˜ˆå€¼åŸºäºä½ ç°æœ‰è·‘æ•°ï¼‰"""
        if self.port_name == 'new_orleans':
            return [
                CurriculumStage(
                    name="åŸºç¡€é˜¶æ®µ",
                    description="æœ€ç®€å•çš„è°ƒåº¦åœºæ™¯",
                    max_vessels=5,
                    max_berths=3,
                    traffic_intensity=0.3,
                    weather_complexity=0.1,
                    episodes=30,                 # 20 -> 30
                    success_threshold=0.35        # ä½ ä¹‹å‰å¤§çº¦36%
                ),
                CurriculumStage(
                    name="åˆçº§é˜¶æ®µ",
                    description="å¢åŠ èˆ¹èˆ¶æ•°é‡",
                    max_vessels=10,
                    max_berths=5,
                    traffic_intensity=0.5,
                    weather_complexity=0.2,
                    episodes=30,                 # 25 -> 30
                    success_threshold=0.40        # å¤è¯„ wr=38.0%ï¼ŒLB=31.6%ï¼›è°ƒæ•´ä¸º0.40
                ),
                CurriculumStage(
                    name="ä¸­çº§é˜¶æ®µ",
                    description="å¢åŠ äº¤é€šå¼ºåº¦",
                    max_vessels=15,
                    max_berths=8,
                    traffic_intensity=0.7,
                    weather_complexity=0.3,
                    episodes=30,
                    success_threshold=0.47  # ä»0.50é™åˆ°0.47 (åŸºäºç¨³å®šæ®µèƒœç‡â‰ˆ0.45)
                ),
                CurriculumStage(
                    name="é«˜çº§é˜¶æ®µ",
                    description="æ¥è¿‘çœŸå®å¤æ‚åº¦",
                    max_vessels=20,
                    max_berths=12,
                    traffic_intensity=0.8,
                    weather_complexity=0.4,
                    episodes=30,
                    success_threshold=0.40
                ),
                CurriculumStage(
                    name="ä¸“å®¶é˜¶æ®µ",
                    description="å®Œæ•´å¤æ‚åº¦",
                    max_vessels=25,
                    max_berths=15,
                    traffic_intensity=1.0,
                    weather_complexity=0.5,
                    episodes=40,
                    success_threshold=0.30
                )
            ]
        
        elif self.port_name in ['baton_rouge', 'south_louisiana']:
            return [
                CurriculumStage(
                    name="åŸºç¡€é˜¶æ®µ",
                    description="ç®€å•è°ƒåº¦",
                    max_vessels=8,
                    max_berths=5,
                    traffic_intensity=0.4,
                    weather_complexity=0.1,
                    episodes=35,                 # 25 -> 35ï¼Œå¢å¼ºå­¦ä¹ 
                    success_threshold=0.41        # å¤è¯„ wr=42.0%ï¼ŒWilsonLB=35.4%ï¼›è°ƒæ•´ä¸º0.41
                ),
                CurriculumStage(
                    name="ä¸­çº§é˜¶æ®µ",
                    description="æ ‡å‡†å¤æ‚åº¦",
                    max_vessels=15,
                    max_berths=10,
                    traffic_intensity=0.7,
                    weather_complexity=0.3,
                    episodes=30,                 # 20 -> 30ï¼Œå¢å¼ºå­¦ä¹ 
                    success_threshold=0.45        # south_louisiana å¤è¯„ wr=45.5%ï¼Œè°ƒæ•´ä¸º0.45
                ),
                CurriculumStage(
                    name="é«˜çº§é˜¶æ®µ",
                    description="å®Œæ•´å¤æ‚åº¦",
                    max_vessels=20,
                    max_berths=15,
                    traffic_intensity=1.0,
                    weather_complexity=0.4,
                    episodes=20,
                    success_threshold=0.37        # ä»0.39é™åˆ°0.37 (ä¸´æ—¶è¿è¥é˜ˆå€¼ï¼ŒåŸºäºBRå®é™…è¡¨ç°)
                )
            ]
        
        else:  # gulfport
            return [
                CurriculumStage(
                    name="æ ‡å‡†é˜¶æ®µ",
                    description="æ ‡å‡†è®­ç»ƒ",
                    max_vessels=15,
                    max_berths=10,
                    traffic_intensity=0.8,
                    weather_complexity=0.3,
                    episodes=20,
                    success_threshold=0.49         # å¤è¯„ wr=45.5%ï¼ŒLB=38.7%ï¼›è°ƒæ•´ä¸º0.49
                ),
                CurriculumStage(
                    name="å®Œæ•´é˜¶æ®µ",
                    description="å®Œæ•´å¤æ‚åº¦",
                    max_vessels=20,
                    max_berths=15,
                    traffic_intensity=1.0,
                    weather_complexity=0.4,
                    episodes=45,                    # 35 -> 45
                    success_threshold=0.37          # ä¸´æ—¶é™ä½ä»¥é€šè¿‡CIï¼Œå¾®è°ƒåå†å‡å›å»
                )
            ]
    
    
    def _create_stage_environment(self, stage: CurriculumStage) -> Dict:
        """ä¸ºæŒ‡å®šé˜¶æ®µåˆ›å»ºç¯å¢ƒé…ç½®"""
        return {
            'max_vessels': stage.max_vessels,
            'max_berths': stage.max_berths,
            'traffic_intensity': stage.traffic_intensity,
            'weather_complexity': stage.weather_complexity,
            'node_config': {
                'berths': stage.max_berths,
                'anchorages': max(3, stage.max_berths // 2),
                'channels': max(2, stage.max_berths // 3),
                'terminals': max(2, stage.max_berths // 4),
                'max_vessels': stage.max_vessels
            }
        }
    
    def _generate_stage_data(self, stage: CurriculumStage, num_samples: int = 100) -> List[Dict]:
        """ä¸ºæŒ‡å®šé˜¶æ®µç”Ÿæˆè®­ç»ƒ/è¯„ä¼°æ•°æ®"""
        env_config = self._create_stage_environment(stage)
        stage_data = []
        for _ in range(num_samples):
            # ç”Ÿæˆæ¯ä¸ªæ³Šä½çš„è´Ÿè½½ï¼Œå¹¶éšæ ·æœ¬æºå¸¦
            base_load = np.clip(np.random.uniform(0.2, 0.8 * env_config['traffic_intensity']), 0.05, 0.95)
            berth_loads = np.clip(
                np.random.normal(loc=base_load, scale=0.10, size=env_config['max_berths']),
                0.02, 0.98
            ).astype(np.float32)
            
            stage_data.append({
                'vessel_count': np.random.randint(1, env_config['max_vessels'] + 1),
                'berth_occupancy': float(base_load),
                'weather_factor': float(np.random.uniform(0.8, 1.0 - 0.2 * env_config['weather_complexity'])),
                'queue_length': int(np.random.poisson(env_config['traffic_intensity'] * 5)),
                'time_pressure': float(np.random.uniform(0.3, 0.7 + 0.3 * env_config['traffic_intensity'])),
                'env_config': env_config,
                'berth_loads': berth_loads.tolist(),  # ğŸ‘ˆ æ–°å¢
            })
        return stage_data
    
    def _calculate_baseline_threshold(self, stage: CurriculumStage, test_data: List[Dict]) -> float:
        """è®¡ç®—åŸºçº¿éšæœºç­–ç•¥çš„å¥–åŠ±é˜ˆå€¼ï¼ˆæ—¥å¿—å±•ç¤ºç”¨ï¼‰"""
        baseline_rewards = []
        for data_point in test_data:
            action = np.random.randint(0, stage.max_berths)
            reward = self._calculate_stage_reward(data_point, action, stage)
            baseline_rewards.append(reward)
        q25 = np.percentile(baseline_rewards, 25)
        q50 = np.percentile(baseline_rewards, 50)
        q75 = np.percentile(baseline_rewards, 75)
        iqr = q75 - q25
        threshold = float(q50 + 0.25 * iqr)
        logger.info(f"  åŸºçº¿é˜ˆå€¼è®¡ç®—: å¹³å‡ {np.mean(baseline_rewards):.2f}, ä¸­ä½æ•° {q50:.2f}, IQR {iqr:.2f}, ç¨³å¥é˜ˆå€¼ {threshold:.2f}")
        return threshold
    
    def _evaluate_stage_performance(self, agent: GATPPOAgent, stage: CurriculumStage, 
                                    test_data: List[Dict], reward_threshold: float) -> Dict:
        """è¯„ä¼°é˜¶æ®µæ€§èƒ½ï¼šä¸éšæœºåŸºçº¿åšé…å¯¹èƒœç‡"""
        agent.actor_critic.eval()
        agent_rewards, baseline_rewards, win_flags = [], [], []
        num_actions = stage.max_berths
        K = 10  # åŸºçº¿é‡‡æ ·æ¬¡æ•°
        
        for data_point in test_data:
            try:
                state = self._extract_state_from_data(data_point)
                node_features, adj_matrix = self._extract_graph_features_from_data(data_point)
                with torch.no_grad():
                     action_probs, _ = agent.actor_critic(
                         torch.as_tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0),
                         torch.as_tensor(node_features, dtype=torch.float32, device=self.device).unsqueeze(0),
                         self._prep_adj_3d(adj_matrix)   # ç»Ÿä¸€æˆ [B,N,N]
                     )
                     action_probs = torch.nan_to_num(action_probs, nan=0.0, posinf=0.0, neginf=0.0)
                     if action_probs.shape[-1] != num_actions:
                         action_probs = action_probs[..., :num_actions]
                     action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True).clamp_min(1e-8)
                     # æ·»åŠ å…¨é›¶æ¦‚ç‡å…œåº•
                     if (action_probs.sum(dim=-1) < 1e-6).any():
                         action_probs = torch.full_like(action_probs, 1.0 / num_actions)
                     agent_action = torch.argmax(action_probs, dim=-1).item()
                agent_reward = self._calculate_stage_reward(data_point, agent_action, stage)
                agent_rewards.append(agent_reward)

                # åŸºçº¿Kæ¬¡
                b_rewards = []
                for _ in range(K):
                    ba = np.random.randint(0, num_actions)
                    b_rewards.append(self._calculate_stage_reward(data_point, ba, stage))
                b_mean = float(np.mean(b_rewards))
                baseline_rewards.append(b_mean)
                win_flags.append(1 if agent_reward > b_mean else 0)
            except Exception as e:
                logger.warning(f"è¯„ä¼°å¤±è´¥: {e}; è®°ä¸ºæœªèµ¢")
                agent_rewards.append(-1.0); baseline_rewards.append(0.0); win_flags.append(0)
        
        completion_rate = float(np.mean(win_flags)) if win_flags else 0.0
        agent.actor_critic.train()
        return {
            'avg_reward': float(np.mean(agent_rewards)) if agent_rewards else 0.0,
            'baseline_avg_reward': float(np.mean(baseline_rewards)) if baseline_rewards else 0.0,
            'completion_rate': completion_rate,         # ä»¥èƒœç‡ä¸ºâ€œå®Œæˆç‡â€
            'success': completion_rate >= stage.success_threshold,
            'reward_threshold': float(reward_threshold),
            'win_rate': completion_rate
        }
    
    def _extract_state_from_data(self, data_point: Dict) -> np.ndarray:
        """å¢å¼ºçŠ¶æ€æå–ï¼Œç»Ÿä¸€ç»´åº¦ä¸º56ç»´"""
        # ---- å›ºå®šé¡ºåºçš„ç¡®å®šæ€§ç‰¹å¾ï¼Œä¸è¦éšæœºæ•°ï¼ ----
        feats = [
            data_point['vessel_count'] / 25.0,   # 1
            data_point['berth_occupancy'],       # 2
            data_point['weather_factor'],        # 3
            data_point['queue_length'] / 20.0,   # 4
            data_point['time_pressure'],         # 5
        ]

        # çª„å¼¯/æ½®æ±ç‰¹å¾ï¼ˆBR/NO æ‰åŠ ï¼‰
        if self.port_name in ('baton_rouge', 'new_orleans'):
            feats += [
                data_point.get('channel_curvature', 0.0),  # 6
                data_point.get('effective_width', 0.0),    # 7
                data_point.get('tidal_velocity', 0.0),     # 8
            ]

        # å¦‚æœä½ è¿˜æœ‰ GAT çš„é‚»å±…èšåˆã€ç¢°æ’é£é™©ç­‰ï¼Œä¹ŸæŒ‰å›ºå®šé¡ºåº append è¿›å»
        # feats += [ ... ]

        # æœ€åä¸€æ­¥ç»Ÿä¸€ç»´åº¦ï¼ˆä¸æ¨¡å‹/é…ç½®ä¸€è‡´ï¼‰
        target_dim = 56   # ä¸æ¨¡å‹é¦–å±‚ in_features ä¸€è‡´
        state = self._pad_or_trunc(feats, target_dim)

        # å¯é€‰ï¼šåœ¨å¼€å‘æœŸåŠ æ–­è¨€/æ—¥å¿—
        if state.size != target_dim:
            logging.warning(f"State dim={state.size} != {target_dim}")
        return state
    
    def _extract_graph_features_from_data(self, data_point: Dict) -> Tuple[np.ndarray, np.ndarray]:
        """æ”¹è¿›å›¾ç‰¹å¾ï¼Œä½¿ç”¨é£é™©åŠ æƒé‚»æ¥"""
        env_config = data_point['env_config']
        node_cfg = env_config['node_config']
        B = node_cfg['berths']; A = node_cfg['anchorages']; C = node_cfg['channels']; T = node_cfg['terminals']
        total_nodes = B + A + C + T

        # ç±»å‹åˆ—ï¼š0=berth, 1=anchorage, 2=channel, 3=terminal
        types = np.concatenate([
            np.zeros(B), np.ones(A), np.full(C, 2), np.full(T, 3)
        ]).astype(np.float32).reshape(-1, 1) / 3.0

        # è´Ÿè½½åˆ—ï¼šåªå¯¹æ³Šä½èŠ‚ç‚¹å¡«çœŸå®è´Ÿè½½ï¼Œå…¶å®ƒèŠ‚ç‚¹å¡« 0
        berth_loads = np.array(data_point.get('berth_loads', [0.5]*B), dtype=np.float32)
        loads_col = np.concatenate([berth_loads, np.zeros(A+C+T, dtype=np.float32)]).reshape(-1, 1)

        # å…¶ä½™éšæœºç‰¹å¾åˆ—ä¿æŒ
        rand_cols = np.random.randn(total_nodes, 6).astype(np.float32)

        # æ‹¼æˆ [N, 8]  (ç±»å‹/è´Ÿè½½ + 6ä¸ªå™ªå£°)
        node_features = np.concatenate([types, loads_col, rand_cols], axis=1).astype(np.float32)
        
        # åŸºç¡€è·ç¦»é‚»æ¥
        adj = np.eye(total_nodes, dtype=np.float32)
        for i in range(total_nodes):
            for j in range(i+1, min(i+5, total_nodes)):
                if np.random.rand() < 0.3:
                    adj[i, j] = adj[j, i] = 1.0
        
        # é’ˆå¯¹çª„å¼¯æ¸¯å£çš„é£é™©åŠ æƒé‚»æ¥
        if self.port_name in ['baton_rouge', 'new_orleans']:
            # é£é™©åŠ æƒé‚»æ¥: è·ç¦» Ã— ä¼šé‡è§’åº¦ Ã— å¯¹å‘æµé‡
            encounter_angle = data_point.get('encounter_angle', np.ones_like(adj))
            opposing_traffic = data_point.get('opposing_traffic', np.ones_like(adj))
            
            # ç¡®ä¿ç»´åº¦åŒ¹é…
            if encounter_angle.shape == adj.shape and opposing_traffic.shape == adj.shape:
                risk_adj = adj * encounter_angle * opposing_traffic
                # å½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´
                if risk_adj.max() > 0:
                    risk_adj = risk_adj / risk_adj.max()
                adj = risk_adj
        
        return node_features, adj
    
    def _calculate_stage_reward(self, data_point: Dict, action: int, stage: CurriculumStage) -> float:
        """å¢å¼ºå¥–åŠ±è®¡ç®—ï¼ŒåŠ å…¥å¼¯é“ç¨³å®šé¡¹"""
        # åŸºç¡€å¥–åŠ±è®¡ç®—
        state_dict = {'recent_actions': [action]*4, 'traffic_pattern': 'normal'}

        # === ä¸åŠ¨ä½œç›¸å…³çš„å¾®å‹è°ƒåº¦æ¨¡å‹ ===
        max_berths = stage.max_berths
        a = int(action) % max_berths

        # ä½¿ç”¨æ ·æœ¬æºå¸¦çš„æ³Šä½è´Ÿè½½ï¼Œç¡®ä¿ä¸å›¾ç‰¹å¾ä¸€è‡´
        if 'berth_loads' in data_point:
            load = float(data_point['berth_loads'][a])
        else:
            # å…¼å®¹è€ç¼“å­˜æ•°æ®çš„å…œåº•ï¼ˆå°½å¿«æ¸…ç¼“å­˜ï¼‰
            base_load = np.clip(data_point['berth_occupancy'], 0.05, 0.95)
            load = float(np.clip(np.random.normal(loc=base_load, scale=0.1), 0.02, 0.98))

        # éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆç”¨äºæœåŠ¡æ¦‚ç‡å’Œç­‰å¾…æ—¶é—´çš„éšæœºæ€§ï¼‰
        rng = np.random.default_rng(int(data_point['vessel_count']*1000 + data_point['queue_length']*17 + a))

        # æœåŠ¡æ¦‚ç‡ï¼šå¤©æ°”è¶Šå¥½ã€è´Ÿè½½è¶Šä½ã€æ’é˜Ÿè¶Šé•¿æ—¶"æœåŠ¡æ”¶ç›Š"è¶Šå¤§
        weather = float(np.clip(data_point['weather_factor'], 0.2, 1.0))
        queue = int(data_point['queue_length'])
        demand = 1.0 + min(queue / max(1, stage.max_vessels), 1.5)  # éœ€æ±‚å¼ºåº¦
        service_prob = np.clip((1 - load) * weather * 0.9, 0.02, 0.98)

        # æ˜¯å¦å®Œæˆä¸€ä¸ªä»»åŠ¡
        completed = 1 if rng.random() < service_prob else 0

        # ç­‰å¾…æ—¶é—´ï¼šè´Ÿè½½ & é˜Ÿåˆ—è¶Šé«˜è¶Šæ…¢ï¼Œå¤©æ°”è¶Šå·®è¶Šæ…¢
        wait_base = 1800.0 * (0.6 + 0.8*load) * (1.3 - 0.8*weather) * (1 + 0.6*queue/max(1, stage.max_vessels))
        wait = float(np.clip(rng.normal(wait_base, 0.15*wait_base), 60.0, 4*3600.0))

        next_state_dict = {
            'completed_tasks': completed,
            'total_tasks': 3,
            'waiting_times': [wait],  # â†“â†“â†“ ä¸åŠ¨ä½œå¼ºç›¸å…³
            'berth_utilization': data_point['berth_occupancy'],
            'queue_length': queue,
            'max_queue_capacity': stage.max_vessels
        }

        base_reward = self.reward_function.calculate_reward(state_dict, action, next_state_dict)
        difficulty_factor = (stage.traffic_intensity + stage.weather_complexity) / 2
        adjusted_reward = base_reward * (1 + difficulty_factor)
        
        # é’ˆå¯¹çª„å¼¯æ¸¯å£çš„é¢å¤–å¥–åŠ±
        if self.port_name in ['baton_rouge', 'new_orleans']:
            # å¼¯é“ç¨³å®šå¥–åŠ±: -Î»1*|Î”Ïˆ| - Î»2*|ay|
            heading_change = abs(data_point.get('heading_change', 0.0))
            lateral_accel = abs(data_point.get('lateral_acceleration', 0.0))
            
            curve_stability = -0.1 * heading_change - 0.05 * lateral_accel
            
            # æ½®æ±è¶…é€Ÿæƒ©ç½š
            tidal_penalty = 0.0
            if data_point.get('tidal_velocity', 0.0) > 0.8:
                tidal_penalty = -0.2
            
            # å¼¯é“æ®µ"è¿‘ç¢°æ’"æƒé‡æ”¾å¤§
            collision_weight = 1.5 if data_point.get('channel_curvature', 0.0) > 0.6 else 1.0
            
            adjusted_reward += curve_stability + tidal_penalty
            adjusted_reward *= collision_weight
        
        return float(adjusted_reward)
    
    def _safe_clear_buffer(self, agent: GATPPOAgent):
        """å…¼å®¹ä¸åŒbufferå®ç°çš„æ¸…ç©ºåŠ¨ä½œ"""
        if hasattr(agent, 'buffer'):
            if hasattr(agent.buffer, 'clear') and callable(agent.buffer.clear):
                agent.buffer.clear()
                return
            if hasattr(agent.buffer, 'buffer') and hasattr(agent.buffer.buffer, 'clear'):
                agent.buffer.buffer.clear()
                return
    
    def _prep_adj_3d(self, adj: np.ndarray) -> torch.Tensor:
        """è§„èŒƒåŒ–é‚»æ¥çŸ©é˜µä¸º [B,N,N] æ ¼å¼"""
        A = torch.as_tensor(adj, dtype=torch.float32, device=self.device)
        # å…è®¸è¾“å…¥ [N,N] / [B,N,N] / [B,1,N,N]
        if A.dim() == 2:
            A = A.unsqueeze(0)                     # [1,N,N]
        elif A.dim() == 4 and A.size(1) == 1:
            A = A.squeeze(1)                       # [B,N,N]
        # å…¶ä»–æƒ…å†µç›´æ¥è¿‡
        if A.dim() != 3:
            raise ValueError(f"adj must be [B,N,N], got {tuple(A.shape)}")
        return A

    def _pad_or_trunc(self, vec, target):
        """é›¶å¡«å……æˆ–è£å‰ªåˆ°ç›®æ ‡ç»´åº¦"""
        v = np.asarray(vec, dtype=np.float32).ravel()
        if v.size < target:
            v = np.pad(v, (0, target - v.size), mode='constant')   # é›¶å¡«å……
        elif v.size > target:
            v = v[:target]                                         # è¶…äº†å°±è£
        return v

    def train_stage(self, agent: GATPPOAgent, stage: CurriculumStage) -> Tuple[GATPPOAgent, Dict]:
        """è®­ç»ƒå•ä¸ªé˜¶æ®µ"""
        logger.info(f"å¼€å§‹è®­ç»ƒé˜¶æ®µ: {stage.name}")
        logger.info(f"  æè¿°: {stage.description}")
        logger.info(f"  ç›®æ ‡è½®æ•°: {stage.episodes}")
        logger.info(f"  æˆåŠŸé˜ˆå€¼: {stage.success_threshold}")
        
        # å‰å‘è‡ªæ£€ï¼šç¡®ä¿çŠ¶æ€ç»´åº¦ä¸æ¨¡å‹æœŸæœ›ä¸€è‡´
        try:
            dummy_data = {
                'vessel_count': 0, 'berth_occupancy': 0, 'weather_factor': 0,
                'queue_length': 0, 'time_pressure': 0,
                'channel_curvature': 0, 'effective_width': 0, 'tidal_velocity': 0
            }
            dummy_state = self._extract_state_from_data(dummy_data)
            # è·å–æ¨¡å‹é¦–å±‚çš„è¾“å…¥ç»´åº¦
            if hasattr(agent, 'actor_critic') and hasattr(agent.actor_critic, 'state_encoder'):
                exp_dim = agent.actor_critic.state_encoder[0].in_features
            else:
                exp_dim = 56  # é»˜è®¤æœŸæœ›ç»´åº¦
            assert dummy_state.size == exp_dim, f"State dim {dummy_state.size} != model expects {exp_dim}"
            logger.info(f"âœ… çŠ¶æ€ç»´åº¦æ£€æŸ¥é€šè¿‡: {dummy_state.size}ç»´")
        except Exception as e:
            logger.warning(f"âš ï¸ çŠ¶æ€ç»´åº¦æ£€æŸ¥å¤±è´¥: {e}")
        
        # æ¸…ç©ºbufferï¼Œé¿å…è·¨é˜¶æ®µæ±¡æŸ“
        self._safe_clear_buffer(agent)
        logger.info("  å·²æ¸…ç©ºbufferï¼Œå‡†å¤‡æ–°é˜¶æ®µè®­ç»ƒ")
        
        # æ•°æ®
        train_data = self._generate_stage_data(stage, num_samples=200)
        test_data = self._generate_stage_data(stage, num_samples=50)
        fixed_threshold = self._calculate_baseline_threshold(stage, test_data)
        
        stage_history, best_performance = [], 0.0
        num_actions = stage.max_berths
        
        for episode in range(stage.episodes):
            # ç†µç³»æ•°çº¿æ€§é€€ç« 0.02 -> 0.005
            progress = episode / max(stage.episodes, 1)
            current_entropy = 0.02 * (1 - progress) + 0.005 * progress
            if hasattr(agent, 'entropy_coef'):
                agent.entropy_coef = current_entropy
            
            episode_reward, episode_steps = 0.0, 0
            episode_data = np.random.choice(train_data, size=min(30, len(train_data)), replace=False)
            
            for data_point in episode_data:
                state = self._extract_state_from_data(data_point)
                node_features, adj_matrix = self._extract_graph_features_from_data(data_point)
                try:
                     with torch.no_grad():
                         ap, value = agent.actor_critic(
                             torch.as_tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0),
                             torch.as_tensor(node_features, dtype=torch.float32, device=self.device).unsqueeze(0),
                             self._prep_adj_3d(adj_matrix)
                         )
                     ap = torch.nan_to_num(ap, nan=0.0, posinf=0.0, neginf=0.0)
                     if ap.shape[-1] != num_actions:
                         ap = ap[..., :num_actions]
                     ap = ap / ap.sum(dim=-1, keepdim=True).clamp_min(1e-8)
                     # æ·»åŠ å…¨é›¶æ¦‚ç‡å…œåº•
                     if (ap.sum(dim=-1) < 1e-6).any():
                         ap = torch.full_like(ap, 1.0 / num_actions)
                     dist = torch.distributions.Categorical(ap)
                     action = dist.sample().item()
                     log_prob = dist.log_prob(torch.tensor(action, device=self.device))
                except Exception as e:
                    logger.warning(f"åŠ¨ä½œé‡‡æ ·å¤±è´¥: {e}; ä½¿ç”¨éšæœºåŠ¨ä½œ")
                    action = np.random.randint(0, num_actions)
                    log_prob = torch.tensor(0.0, device=self.device)
                    value = torch.tensor(0.0, device=self.device)
                
                reward = self._calculate_stage_reward(data_point, action, stage)
                
                # å­˜å‚¨
                try:
                    agent.store_experience(
                        np.asarray(state, dtype=np.float32),
                        np.asarray(node_features, dtype=np.float32),
                        np.asarray(adj_matrix, dtype=np.float32),
                        int(action),
                        float(reward),
                        np.asarray(state, dtype=np.float32),   # ç®€åŒ–ï¼šnext_state=state
                        False,
                        float(log_prob.item()),
                        float(value.item() if hasattr(value, 'item') else 0.0)
                    )
                except Exception as e:
                    logger.warning(f"å­˜å‚¨ç»éªŒå¤±è´¥: {e}; è·³è¿‡æœ¬æ ·æœ¬")
                    continue
                
                episode_reward += float(reward)
                episode_steps += 1
            
            # æ›´æ–°æ¨¡å‹ï¼ˆå°è¯•å¤šæ¬¡å°æ›´æ–°ï¼‰
            loss_info = {'total_loss': 0.0}
            if hasattr(agent, 'batch_size') and hasattr(agent, 'buffer') and \
               hasattr(agent.buffer, 'buffer') and len(agent.buffer.buffer) >= agent.batch_size:
                total_loss, cnt = 0.0, 0
                for _ in range(3):
                    try:
                        single = agent.update()
                        if isinstance(single, dict):
                            total_loss += float(single.get('total_loss', single.get('loss', 0.0)))
                        elif single is not None:
                            total_loss += float(single)
                        cnt += 1
                    except Exception as e:
                        logger.warning(f"æ¨¡å‹æ›´æ–°å¤±è´¥: {e}")
                        break
                loss_info['total_loss'] = total_loss / max(cnt, 1)
            
            # è¯„ä¼°
            if episode % 5 == 0:
                perf = self._evaluate_stage_performance(agent, stage, test_data, fixed_threshold)
                stage_history.append({
                    'episode': episode,
                    'avg_reward': episode_reward / max(episode_steps, 1),
                    'test_performance': perf,
                    'loss': loss_info['total_loss']
                })
                logger.info(f"  Episode {episode}: èƒœç‡ {perf['completion_rate']:.2%}, "
                            f"æ™ºèƒ½ä½“å¥–åŠ± {perf['avg_reward']:.2f}, "
                            f"åŸºçº¿å¥–åŠ± {perf.get('baseline_avg_reward', 0):.2f}")
                
                if perf['completion_rate'] > best_performance:
                    best_performance = perf['completion_rate']
                    model_path = self.save_dir / f"stage_{stage.name.replace(' ', '_')}_best.pt"
                    torch.save({
                        'episode': episode,
                        'stage': stage.name,
                        'model_state_dict': agent.actor_critic.state_dict(),
                        'performance': perf
                    }, model_path)
        
        # æœ€ç»ˆè¯„ä¼°
        final_perf = self._evaluate_stage_performance(agent, stage, test_data, fixed_threshold)
        stage_results = {
            'stage_name': stage.name,
            'final_performance': final_perf,
            'best_performance': best_performance,
            'success': final_perf['success'],
            'history': stage_history
        }
        logger.info(f"é˜¶æ®µ {stage.name} å®Œæˆ:\n  æœ€ç»ˆå®Œæˆç‡: {final_perf['completion_rate']:.2%}\n  æ˜¯å¦æˆåŠŸ: {final_perf['success']}")
        return agent, stage_results
    
    def train_curriculum(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„åˆ†é˜¶æ®µè®­ç»ƒ"""
        logger.info(f"å¼€å§‹ {self.port_name} æ¸¯å£çš„åˆ†é˜¶æ®µè®­ç»ƒ")
        agent = build_agent(
            self.port_name,
            hidden_dim=256, learning_rate=3e-4, batch_size=32,
            device=self.device, num_heads=4, dropout=0.1,
            state_dim=56, action_dim=15, node_feature_dim=8,
            entropy_coef=0.02, ppo_epochs=8
        )
        
        results = {'port_name': self.port_name, 'stages': [], 'overall_success': True}
        
        for i, stage in enumerate(self.curriculum_stages):
            logger.info(f"\n{'='*50}\né˜¶æ®µ {i+1}/{len(self.curriculum_stages)}: {stage.name}\n{'='*50}")
            
            if i > 0:
                prev = self.curriculum_stages[i-1]
                best_path = self.save_dir / f"stage_{prev.name.replace(' ', '_')}_best.pt"
                if best_path.exists():
                    logger.info(f"  åŠ è½½ä¸Šä¸€é˜¶æ®µæœ€ä½³æ¨¡å‹: {best_path}")
                    ckpt = torch.load(best_path, map_location=self.device, weights_only=False)
                    agent.actor_critic.load_state_dict(ckpt['model_state_dict'])
                # å­¦ä¹ ç‡è¡°å‡
                if hasattr(agent, 'optimizer'):
                    old_lr = agent.optimizer.param_groups[0]['lr']
                    new_lr = max(old_lr * 0.7, 1e-5)
                    for g in agent.optimizer.param_groups:
                        g['lr'] = new_lr
                    logger.info(f"  å­¦ä¹ ç‡è¡°å‡: {old_lr:.2e} â†’ {new_lr:.2e}")
            
            agent, stage_results = self.train_stage(agent, stage)
            results['stages'].append(stage_results)
            if not stage_results['success']:
                logger.warning(f"é˜¶æ®µ {stage.name} æœªè¾¾åˆ°æˆåŠŸæ ‡å‡†ï¼Œä½†ç»§ç»­ä¸‹ä¸€é˜¶æ®µ")
                results['overall_success'] = False
        
        final_model_path = self.save_dir / "curriculum_final_model.pt"
        torch.save({
            'model_state_dict': agent.actor_critic.state_dict(),
            'curriculum_results': results,
            'port_name': self.port_name,
            
        }, final_model_path)
        
        logger.info(f"\nåˆ†é˜¶æ®µè®­ç»ƒå®Œæˆ!\næ•´ä½“æˆåŠŸ: {results['overall_success']}\næœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°: {final_model_path}")
        return results

def main():
    import argparse
    # éšæœºç§å­
    torch.manual_seed(42); np.random.seed(42)
    if torch.cuda.is_available(): torch.cuda.manual_seed(42)
    
    parser = argparse.ArgumentParser(description="åˆ†é˜¶æ®µè®­ç»ƒå™¨")
    parser.add_argument("--port", required=True, help="æ¸¯å£åç§°")
    args = parser.parse_args()
    
    trainer = CurriculumTrainer(args.port)
    results = trainer.train_curriculum()
    
    print(f"\nåˆ†é˜¶æ®µè®­ç»ƒç»“æœ:")
    print(f"æ¸¯å£: {results['port_name']}")
    print(f"æ•´ä½“æˆåŠŸ: {results['overall_success']}")
    for s in results['stages']:
        print(f"  {s['stage_name']}: å®Œæˆç‡ {s['final_performance']['completion_rate']:.2%}, æˆåŠŸ {s['success']}")

if __name__ == "__main__":
    main()