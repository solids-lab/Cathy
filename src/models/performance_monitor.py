#!/usr/bin/env python3
"""
å®Œæ•´çš„æ€§èƒ½ç›‘æ§æ¨¡å—
æ”¯æŒæ¨ç†å»¶è¿Ÿæµ‹è¯•ã€èµ„æºæ¶ˆè€—è¯„ä¼°ã€æ¨¡å‹åˆ†æã€å®æ—¶ç›‘æ§ç­‰
"""

import time
import psutil
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Callable, Any, Union
import threading
import multiprocessing
from contextlib import contextmanager
from dataclasses import dataclass, field
from pathlib import Path
import json
import pickle
import logging
from collections import defaultdict, deque
import gc
import sys
import tracemalloc
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import GPUtil
from datetime import datetime, timedelta
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®å›¾è¡¨æ ·å¼
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")


@dataclass
class PerformanceConfig:
    """æ€§èƒ½ç›‘æ§é…ç½®"""
    monitor_interval: float = 0.1
    warmup_runs: int = 10
    test_runs: int = 100
    max_history_size: int = 10000
    enable_gpu_monitoring: bool = True
    enable_memory_profiling: bool = True
    enable_realtime_plot: bool = False
    save_detailed_logs: bool = True
    log_level: str = "INFO"

    # æµ‹è¯•é…ç½®
    batch_sizes: List[int] = field(default_factory=lambda: [1, 2, 4, 8, 16, 32, 64])
    sequence_lengths: List[int] = field(default_factory=lambda: [10, 50, 100, 200])
    precision_modes: List[str] = field(default_factory=lambda: ['float32', 'float16'])

    # é˜ˆå€¼è®¾ç½®
    latency_warning_threshold: float = 100.0  # ms
    memory_warning_threshold: float = 80.0  # %
    cpu_warning_threshold: float = 90.0  # %

    # è¾“å‡ºé…ç½®
    output_dir: str = "performance_reports"
    report_format: List[str] = field(default_factory=lambda: ['json', 'csv', 'html'])


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: float
    latency_ms: float
    cpu_percent: float
    memory_mb: float
    memory_percent: float
    gpu_memory_mb: Optional[float] = None
    gpu_utilization: Optional[float] = None
    throughput_qps: Optional[float] = None
    batch_size: Optional[int] = None
    model_flops: Optional[int] = None

    def to_dict(self) -> Dict:
        return {
            'timestamp': self.timestamp,
            'latency_ms': self.latency_ms,
            'cpu_percent': self.cpu_percent,
            'memory_mb': self.memory_mb,
            'memory_percent': self.memory_percent,
            'gpu_memory_mb': self.gpu_memory_mb,
            'gpu_utilization': self.gpu_utilization,
            'throughput_qps': self.throughput_qps,
            'batch_size': self.batch_size,
            'model_flops': self.model_flops
        }


class SystemMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""

    def __init__(self, config: PerformanceConfig):
        self.config = config
        self.monitoring = False
        self.metrics_history = deque(maxlen=config.max_history_size)
        self.monitor_thread = None

        # GPUæ£€æµ‹
        self.gpu_available = torch.cuda.is_available() and config.enable_gpu_monitoring
        if self.gpu_available:
            try:
                import GPUtil
                self.gpus = GPUtil.getGPUs()
            except ImportError:
                self.gpu_available = False
                logging.warning("GPUtil not available, GPU monitoring disabled")

        # è¿›ç¨‹ç›‘æ§
        self.process = psutil.Process()

        # æ€§èƒ½è®¡æ•°å™¨
        self.start_time = None
        self.total_inferences = 0

    @contextmanager
    def monitor_context(self, description: str = "performance_test"):
        """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        print(f"ğŸ” å¼€å§‹ç›‘æ§: {description}")

        self.start_monitoring()
        start_time = time.time()

        try:
            yield self
        finally:
            end_time = time.time()
            self.stop_monitoring()

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            duration = end_time - start_time
            avg_metrics = self.get_average_metrics()

            print(f"ğŸ“Š {description} æ€§èƒ½æŠ¥å‘Š:")
            print(f"  æ€»è€—æ—¶: {duration:.4f}ç§’")
            print(f"  å¹³å‡CPU: {avg_metrics.get('cpu_percent', 0):.2f}%")
            print(f"  å¹³å‡å†…å­˜: {avg_metrics.get('memory_mb', 0):.1f}MB ({avg_metrics.get('memory_percent', 0):.1f}%)")

            if self.gpu_available and 'gpu_memory_mb' in avg_metrics:
                print(f"  å¹³å‡GPUå†…å­˜: {avg_metrics.get('gpu_memory_mb', 0):.1f}MB")
                print(f"  å¹³å‡GPUåˆ©ç”¨ç‡: {avg_metrics.get('gpu_utilization', 0):.1f}%")

            if 'throughput_qps' in avg_metrics:
                print(f"  å¹³å‡ååé‡: {avg_metrics.get('throughput_qps', 0):.2f} QPS")

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.metrics_history.clear()
        self.start_time = time.time()
        self.total_inferences = 0

        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)

    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                metrics = self._collect_system_metrics()
                self.metrics_history.append(metrics)

                # æ£€æŸ¥è­¦å‘Šé˜ˆå€¼
                self._check_warning_thresholds(metrics)

            except Exception as e:
                logging.error(f"ç›‘æ§è¿‡ç¨‹å‡ºé”™: {e}")

            time.sleep(self.config.monitor_interval)

    def _collect_system_metrics(self) -> PerformanceMetrics:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        # CPUå’Œå†…å­˜
        cpu_percent = self.process.cpu_percent()
        memory_info = self.process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        memory_percent = self.process.memory_percent()

        # GPUæŒ‡æ ‡
        gpu_memory_mb = None
        gpu_utilization = None

        if self.gpu_available and torch.cuda.is_available():
            try:
                gpu_memory_mb = torch.cuda.memory_allocated() / 1024 / 1024
                if self.gpus:
                    gpu_utilization = self.gpus[0].load * 100
            except Exception as e:
                logging.warning(f"GPUç›‘æ§å¤±è´¥: {e}")

        # ååé‡è®¡ç®—
        throughput_qps = None
        if self.start_time and self.total_inferences > 0:
            elapsed = time.time() - self.start_time
            throughput_qps = self.total_inferences / elapsed if elapsed > 0 else 0

        return PerformanceMetrics(
            timestamp=time.time(),
            latency_ms=0,  # åœ¨æ¨ç†æµ‹è¯•ä¸­å•ç‹¬è®¾ç½®
            cpu_percent=cpu_percent,
            memory_mb=memory_mb,
            memory_percent=memory_percent,
            gpu_memory_mb=gpu_memory_mb,
            gpu_utilization=gpu_utilization,
            throughput_qps=throughput_qps
        )

    def _check_warning_thresholds(self, metrics: PerformanceMetrics):
        """æ£€æŸ¥è­¦å‘Šé˜ˆå€¼"""
        if metrics.cpu_percent > self.config.cpu_warning_threshold:
            logging.warning(f"é«˜CPUä½¿ç”¨ç‡: {metrics.cpu_percent:.1f}%")

        if metrics.memory_percent > self.config.memory_warning_threshold:
            logging.warning(f"é«˜å†…å­˜ä½¿ç”¨ç‡: {metrics.memory_percent:.1f}%")

        if metrics.latency_ms > self.config.latency_warning_threshold:
            logging.warning(f"é«˜å»¶è¿Ÿ: {metrics.latency_ms:.2f}ms")

    def record_inference(self, latency_ms: float):
        """è®°å½•æ¨ç†"""
        self.total_inferences += 1

        # æ›´æ–°æœ€æ–°æŒ‡æ ‡çš„å»¶è¿Ÿ
        if self.metrics_history:
            latest_metrics = self.metrics_history[-1]
            latest_metrics.latency_ms = latency_ms

    def get_average_metrics(self) -> Dict[str, float]:
        """è·å–å¹³å‡æŒ‡æ ‡"""
        if not self.metrics_history:
            return {}

        # è®¡ç®—å¹³å‡å€¼
        metrics_dict = defaultdict(list)
        for metrics in self.metrics_history:
            for key, value in metrics.to_dict().items():
                if value is not None and key != 'timestamp':
                    metrics_dict[key].append(value)

        avg_metrics = {}
        for key, values in metrics_dict.items():
            if values:
                avg_metrics[key] = np.mean(values)

        return avg_metrics

    def get_metrics_dataframe(self) -> pd.DataFrame:
        """è·å–æŒ‡æ ‡DataFrame"""
        if not self.metrics_history:
            return pd.DataFrame()

        data = [metrics.to_dict() for metrics in self.metrics_history]
        df = pd.DataFrame(data)
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
        return df


class InferenceLatencyTester:
    """æ¨ç†å»¶è¿Ÿæµ‹è¯•å™¨"""

    def __init__(self, config: PerformanceConfig):
        self.config = config
        self.monitor = SystemMonitor(config)

    def test_model_latency(self,
                           model: nn.Module,
                           input_generator: Callable,
                           input_args: Dict = None,
                           device: str = 'cpu') -> Dict[str, Any]:
        """
        æµ‹è¯•æ¨¡å‹æ¨ç†å»¶è¿Ÿ

        Args:
            model: è¦æµ‹è¯•çš„æ¨¡å‹
            input_generator: è¾“å…¥ç”Ÿæˆå‡½æ•°
            input_args: è¾“å…¥ç”Ÿæˆå‚æ•°
            device: è®¾å¤‡

        Returns:
            è¯¦ç»†çš„å»¶è¿Ÿç»Ÿè®¡ç»“æœ
        """

        input_args = input_args or {}
        model = model.to(device)
        model.eval()

        print(f"ğŸš€ æµ‹è¯•æ¨¡å‹æ¨ç†å»¶è¿Ÿ")
        print(f"  è®¾å¤‡: {device}")
        print(f"  é¢„çƒ­è½®æ¬¡: {self.config.warmup_runs}")
        print(f"  æµ‹è¯•è½®æ¬¡: {self.config.test_runs}")

        # é¢„çƒ­
        print("ğŸ”¥ æ¨¡å‹é¢„çƒ­ä¸­...")
        for _ in range(self.config.warmup_runs):
            with torch.no_grad():
                inputs = input_generator(**input_args)
                if isinstance(inputs, (list, tuple)):
                    inputs = [inp.to(device) if torch.is_tensor(inp) else inp for inp in inputs]
                elif torch.is_tensor(inputs):
                    inputs = inputs.to(device)

                if isinstance(inputs, (list, tuple)):
                    _ = model(*inputs)
                else:
                    _ = model(inputs)

        if device.startswith('cuda'):
            torch.cuda.synchronize()

        # æµ‹è¯•å»¶è¿Ÿ
        latencies = []

        with self.monitor.monitor_context("æ¨ç†å»¶è¿Ÿæµ‹è¯•"):
            for i in range(self.config.test_runs):
                # ç”Ÿæˆè¾“å…¥
                inputs = input_generator(**input_args)
                if isinstance(inputs, (list, tuple)):
                    inputs = [inp.to(device) if torch.is_tensor(inp) else inp for inp in inputs]
                elif torch.is_tensor(inputs):
                    inputs = inputs.to(device)

                if device.startswith('cuda'):
                    torch.cuda.synchronize()

                start_time = time.time()

                with torch.no_grad():
                    if isinstance(inputs, (list, tuple)):
                        output = model(*inputs)
                    else:
                        output = model(inputs)

                if device.startswith('cuda'):
                    torch.cuda.synchronize()

                end_time = time.time()
                latency_ms = (end_time - start_time) * 1000
                latencies.append(latency_ms)

                # è®°å½•åˆ°ç›‘æ§å™¨
                self.monitor.record_inference(latency_ms)

                if (i + 1) % (self.config.test_runs // 10) == 0:
                    print(f"  å®Œæˆ {i + 1}/{self.config.test_runs} æ¬¡æ¨ç†")

        # è®¡ç®—è¯¦ç»†ç»Ÿè®¡
        latencies = np.array(latencies)

        results = {
            'device': device,
            'test_runs': self.config.test_runs,
            'warmup_runs': self.config.warmup_runs,

            # å»¶è¿Ÿç»Ÿè®¡
            'mean_latency_ms': float(np.mean(latencies)),
            'std_latency_ms': float(np.std(latencies)),
            'min_latency_ms': float(np.min(latencies)),
            'max_latency_ms': float(np.max(latencies)),
            'median_latency_ms': float(np.median(latencies)),
            'p95_latency_ms': float(np.percentile(latencies, 95)),
            'p99_latency_ms': float(np.percentile(latencies, 99)),

            # ååé‡
            'throughput_qps': 1000 / np.mean(latencies),
            'peak_throughput_qps': 1000 / np.min(latencies),

            # èµ„æºä½¿ç”¨
            'avg_system_metrics': self.monitor.get_average_metrics(),

            # åŸå§‹æ•°æ®
            'raw_latencies': latencies.tolist(),
            'metrics_history': [m.to_dict() for m in self.monitor.metrics_history]
        }

        return results

    def benchmark_batch_sizes(self,
                              model: nn.Module,
                              input_generator: Callable,
                              batch_sizes: List[int] = None,
                              device: str = 'cpu') -> Dict[int, Dict]:
        """
        æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°çš„æ€§èƒ½

        Args:
            model: æ¨¡å‹
            input_generator: è¾“å…¥ç”Ÿæˆå‡½æ•°ï¼Œéœ€è¦æ¥å—batch_sizeå‚æ•°
            batch_sizes: æ‰¹é‡å¤§å°åˆ—è¡¨
            device: è®¾å¤‡

        Returns:
            å„æ‰¹é‡å¤§å°çš„æ€§èƒ½ç»“æœ
        """

        batch_sizes = batch_sizes or self.config.batch_sizes
        results = {}

        print("ğŸ“Š æ‰¹é‡å¤§å°æ€§èƒ½æµ‹è¯•")
        print(f"  æµ‹è¯•æ‰¹é‡: {batch_sizes}")

        for batch_size in batch_sizes:
            print(f"\nğŸ” æµ‹è¯•æ‰¹é‡å¤§å°: {batch_size}")

            # æµ‹è¯•å½“å‰æ‰¹é‡å¤§å°
            batch_results = self.test_model_latency(
                model=model,
                input_generator=input_generator,
                input_args={'batch_size': batch_size},
                device=device
            )

            # æ·»åŠ æ‰¹é‡ç›¸å…³æŒ‡æ ‡
            batch_results['batch_size'] = batch_size
            batch_results['per_sample_latency_ms'] = batch_results['mean_latency_ms'] / batch_size
            batch_results['samples_per_second'] = batch_size * batch_results['throughput_qps']

            results[batch_size] = batch_results

            print(f"  å¹³å‡å»¶è¿Ÿ: {batch_results['mean_latency_ms']:.2f}ms")
            print(f"  å•æ ·æœ¬å»¶è¿Ÿ: {batch_results['per_sample_latency_ms']:.2f}ms")
            print(f"  æ ·æœ¬ååé‡: {batch_results['samples_per_second']:.1f} samples/s")

            # æ¸…ç†GPUç¼“å­˜
            if device.startswith('cuda'):
                torch.cuda.empty_cache()
                gc.collect()

        return results

    def benchmark_precision_modes(self,
                                  model: nn.Module,
                                  input_generator: Callable,
                                  precision_modes: List[str] = None,
                                  device: str = 'cpu') -> Dict[str, Dict]:
        """
        æµ‹è¯•ä¸åŒç²¾åº¦æ¨¡å¼çš„æ€§èƒ½

        Args:
            model: æ¨¡å‹
            input_generator: è¾“å…¥ç”Ÿæˆå‡½æ•°
            precision_modes: ç²¾åº¦æ¨¡å¼åˆ—è¡¨ ['float32', 'float16', 'int8']
            device: è®¾å¤‡

        Returns:
            å„ç²¾åº¦æ¨¡å¼çš„æ€§èƒ½ç»“æœ
        """

        precision_modes = precision_modes or self.config.precision_modes
        results = {}

        print("ğŸ¯ ç²¾åº¦æ¨¡å¼æ€§èƒ½æµ‹è¯•")
        print(f"  æµ‹è¯•ç²¾åº¦: {precision_modes}")

        original_model = model

        for precision in precision_modes:
            print(f"\nğŸ” æµ‹è¯•ç²¾åº¦æ¨¡å¼: {precision}")

            try:
                # è½¬æ¢æ¨¡å‹ç²¾åº¦
                if precision == 'float16' and device.startswith('cuda'):
                    test_model = original_model.half()
                elif precision == 'float32':
                    test_model = original_model.float()
                else:
                    print(f"  è·³è¿‡ä¸æ”¯æŒçš„ç²¾åº¦: {precision}")
                    continue

                # æµ‹è¯•æ€§èƒ½
                precision_results = self.test_model_latency(
                    model=test_model,
                    input_generator=lambda **kwargs: self._convert_input_precision(
                        input_generator(**kwargs), precision, device
                    ),
                    device=device
                )

                precision_results['precision'] = precision
                results[precision] = precision_results

                print(f"  å¹³å‡å»¶è¿Ÿ: {precision_results['mean_latency_ms']:.2f}ms")
                print(f"  ååé‡: {precision_results['throughput_qps']:.1f} QPS")

            except Exception as e:
                print(f"  ç²¾åº¦ {precision} æµ‹è¯•å¤±è´¥: {e}")
                continue

        return results

    def _convert_input_precision(self, inputs, precision: str, device: str):
        """è½¬æ¢è¾“å…¥ç²¾åº¦"""
        if isinstance(inputs, torch.Tensor):
            if precision == 'float16':
                return inputs.half().to(device)
            elif precision == 'float32':
                return inputs.float().to(device)
            else:
                return inputs.to(device)
        elif isinstance(inputs, (list, tuple)):
            return [self._convert_input_precision(inp, precision, device) for inp in inputs]
        else:
            return inputs


class ModelProfiler:
    """æ¨¡å‹åˆ†æå™¨"""

    def __init__(self, config: PerformanceConfig):
        self.config = config

    def profile_model_complexity(self,
                                 model: nn.Module,
                                 input_generator: Callable,
                                 input_args: Dict = None) -> Dict[str, Any]:
        """
        åˆ†ææ¨¡å‹å¤æ‚åº¦

        Returns:
            æ¨¡å‹å¤æ‚åº¦åˆ†æç»“æœ
        """

        input_args = input_args or {}

        print("ğŸ”¬ åˆ†ææ¨¡å‹å¤æ‚åº¦")

        # è·å–æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        # è·å–æ¨¡å‹å¤§å°
        param_size = sum(p.numel() * p.element_size() for p in model.parameters())
        buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
        model_size_mb = (param_size + buffer_size) / 1024 / 1024

        # åˆ†ææ¨¡å‹ç»“æ„
        layer_info = self._analyze_model_layers(model)

        # ä¼°ç®—FLOPs
        try:
            sample_input = input_generator(**input_args)
            flops = self._estimate_flops(model, sample_input)
        except Exception as e:
            print(f"FLOPsä¼°ç®—å¤±è´¥: {e}")
            flops = None

        results = {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': model_size_mb,
            'estimated_flops': flops,
            'layer_analysis': layer_info,
            'model_summary': str(model)
        }

        print(f"  æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  æ¨¡å‹å¤§å°: {model_size_mb:.2f}MB")
        if flops:
            print(f"  ä¼°ç®—FLOPs: {flops:,}")

        return results

    def _analyze_model_layers(self, model: nn.Module) -> Dict[str, Any]:
        """åˆ†ææ¨¡å‹å±‚"""
        layer_info = {
            'total_layers': 0,
            'layer_types': defaultdict(int),
            'layer_details': []
        }

        for name, module in model.named_modules():
            if len(list(module.children())) == 0:  # å¶å­èŠ‚ç‚¹
                layer_info['total_layers'] += 1
                layer_type = type(module).__name__
                layer_info['layer_types'][layer_type] += 1

                # è¯¦ç»†ä¿¡æ¯
                layer_detail = {
                    'name': name,
                    'type': layer_type,
                    'parameters': sum(p.numel() for p in module.parameters())
                }

                # ç‰¹å®šå±‚çš„ä¿¡æ¯
                if isinstance(module, nn.Linear):
                    layer_detail['in_features'] = module.in_features
                    layer_detail['out_features'] = module.out_features
                elif isinstance(module, nn.Conv2d):
                    layer_detail['in_channels'] = module.in_channels
                    layer_detail['out_channels'] = module.out_channels
                    layer_detail['kernel_size'] = module.kernel_size

                layer_info['layer_details'].append(layer_detail)

        return layer_info

    def _estimate_flops(self, model: nn.Module, sample_input) -> Optional[int]:
        """ä¼°ç®—FLOPs"""
        try:
            # è¿™é‡Œå¯ä»¥é›†æˆ thop, fvcore ç­‰åº“æ¥ç²¾ç¡®è®¡ç®—FLOPs
            # ç®€åŒ–å®ç°ï¼šåŸºäºå‚æ•°é‡çš„ç²—ç•¥ä¼°ç®—
            total_params = sum(p.numel() for p in model.parameters())

            # å‡è®¾æ¯ä¸ªå‚æ•°å¹³å‡æ‰§è¡Œ2æ¬¡è¿ç®—ï¼ˆ1æ¬¡ä¹˜æ³•ï¼Œ1æ¬¡åŠ æ³•ï¼‰
            estimated_flops = total_params * 2

            return estimated_flops
        except Exception:
            return None

    def memory_profiling(self,
                         model: nn.Module,
                         input_generator: Callable,
                         input_args: Dict = None,
                         device: str = 'cpu') -> Dict[str, Any]:
        """
        å†…å­˜ä½¿ç”¨åˆ†æ

        Returns:
            å†…å­˜ä½¿ç”¨åˆ†æç»“æœ
        """

        input_args = input_args or {}
        model = model.to(device)

        print("ğŸ§  åˆ†æå†…å­˜ä½¿ç”¨")

        # å¯ç”¨å†…å­˜è¿½è¸ª
        if self.config.enable_memory_profiling:
            tracemalloc.start()

        # åŸºå‡†å†…å­˜
        if device.startswith('cuda'):
            torch.cuda.reset_peak_memory_stats()
            baseline_gpu_memory = torch.cuda.memory_allocated()

        baseline_cpu_memory = psutil.Process().memory_info().rss / 1024 / 1024

        # å‰å‘ä¼ æ’­
        inputs = input_generator(**input_args)
        if isinstance(inputs, (list, tuple)):
            inputs = [inp.to(device) if torch.is_tensor(inp) else inp for inp in inputs]
        elif torch.is_tensor(inputs):
            inputs = inputs.to(device)

        with torch.no_grad():
            if isinstance(inputs, (list, tuple)):
                output = model(*inputs)
            else:
                output = model(inputs)

        # å†…å­˜ä½¿ç”¨
        peak_cpu_memory = psutil.Process().memory_info().rss / 1024 / 1024
        cpu_memory_usage = peak_cpu_memory - baseline_cpu_memory

        gpu_memory_usage = None
        peak_gpu_memory = None

        if device.startswith('cuda'):
            peak_gpu_memory = torch.cuda.max_memory_allocated()
            gpu_memory_usage = (peak_gpu_memory - baseline_gpu_memory) / 1024 / 1024

        # CPUå†…å­˜è¿½è¸ª
        cpu_memory_trace = None
        if self.config.enable_memory_profiling:
            current, peak = tracemalloc.get_traced_memory()
            cpu_memory_trace = {
                'current_mb': current / 1024 / 1024,
                'peak_mb': peak / 1024 / 1024
            }
            tracemalloc.stop()

        results = {
            'device': device,
            'cpu_memory_usage_mb': cpu_memory_usage,
            'gpu_memory_usage_mb': gpu_memory_usage,
            'peak_gpu_memory_mb': peak_gpu_memory / 1024 / 1024 if peak_gpu_memory else None,
            'cpu_memory_trace': cpu_memory_trace,
            'baseline_cpu_memory_mb': baseline_cpu_memory,
            'peak_cpu_memory_mb': peak_cpu_memory
        }

        print(f"  CPUå†…å­˜ä½¿ç”¨: {cpu_memory_usage:.2f}MB")
        if gpu_memory_usage is not None:
            print(f"  GPUå†…å­˜ä½¿ç”¨: {gpu_memory_usage:.2f}MB")

        return results


class PerformanceReporter:
    """æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, config: PerformanceConfig):
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True)

    def generate_comprehensive_report(self,
                                      latency_results: Dict,
                                      batch_results: Dict = None,
                                      precision_results: Dict = None,
                                      complexity_results: Dict = None,
                                      memory_results: Dict = None) -> str:
        """
        ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_name = f"performance_report_{timestamp}"

        print(f"ğŸ“ ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š: {report_name}")

        # ç”Ÿæˆå„ç§æ ¼å¼çš„æŠ¥å‘Š
        report_files = []

        if 'json' in self.config.report_format:
            json_file = self._generate_json_report(
                report_name, latency_results, batch_results,
                precision_results, complexity_results, memory_results
            )
            report_files.append(json_file)

        if 'csv' in self.config.report_format:
            csv_file = self._generate_csv_report(
                report_name, latency_results, batch_results, precision_results
            )
            report_files.append(csv_file)

        if 'html' in self.config.report_format:
            html_file = self._generate_html_report(
                report_name, latency_results, batch_results,
                precision_results, complexity_results, memory_results
            )
            report_files.append(html_file)

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        plots_dir = self.output_dir / f"{report_name}_plots"
        plots_dir.mkdir(exist_ok=True)

        self._generate_performance_plots(
            plots_dir, latency_results, batch_results, precision_results
        )

        print(f"  æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.output_dir}")
        for file in report_files:
            print(f"    - {file}")

        return str(self.output_dir / f"{report_name}.html")

    def _generate_json_report(self, report_name: str, *results) -> str:
        """ç”ŸæˆJSONæŠ¥å‘Š"""
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'config': self.config.__dict__,
            'latency_results': results[0] if len(results) > 0 else None,
            'batch_results': results[1] if len(results) > 1 else None,
            'precision_results': results[2] if len(results) > 2 else None,
            'complexity_results': results[3] if len(results) > 3 else None,
            'memory_results': results[4] if len(results) > 4 else None
        }

        json_file = self.output_dir / f"{report_name}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)

        return str(json_file)

    def _generate_csv_report(self, report_name: str, *results) -> str:
        """ç”ŸæˆCSVæŠ¥å‘Š"""
        csv_data = []

        # å»¶è¿Ÿç»“æœ
        if results[0]:
            csv_data.append({
                'test_type': 'latency',
                'metric': 'mean_latency_ms',
                'value': results[0]['mean_latency_ms']
            })
            csv_data.append({
                'test_type': 'latency',
                'metric': 'throughput_qps',
                'value': results[0]['throughput_qps']
            })

        # æ‰¹é‡å¤§å°ç»“æœ
        if results[1]:
            for batch_size, data in results[1].items():
                csv_data.append({
                    'test_type': 'batch_size',
                    'batch_size': batch_size,
                    'metric': 'mean_latency_ms',
                    'value': data['mean_latency_ms']
                })
                csv_data.append({
                    'test_type': 'batch_size',
                    'batch_size': batch_size,
                    'metric': 'samples_per_second',
                    'value': data['samples_per_second']
                })

        # ç²¾åº¦ç»“æœ
        if results[2]:
            for precision, data in results[2].items():
                csv_data.append({
                    'test_type': 'precision',
                    'precision': precision,
                    'metric': 'mean_latency_ms',
                    'value': data['mean_latency_ms']
                })

        df = pd.DataFrame(csv_data)
        csv_file = self.output_dir / f"{report_name}.csv"
        df.to_csv(csv_file, index=False)

        return str(csv_file)

    def _generate_html_report(self, report_name: str, *results) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>æ€§èƒ½æµ‹è¯•æŠ¥å‘Š - {report_name}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f8ff; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .metric {{ margin: 10px 0; }}
                .value {{ font-weight: bold; color: #2e8b57; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸš€ æ€§èƒ½æµ‹è¯•æŠ¥å‘Š</h1>
                <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
        """

        # å»¶è¿Ÿæµ‹è¯•ç»“æœ
        if results[0]:
            latency_data = results[0]
            html_content += f"""
            <div class="section">
                <h2>ğŸ“Š å»¶è¿Ÿæµ‹è¯•ç»“æœ</h2>
                <div class="metric">å¹³å‡å»¶è¿Ÿ: <span class="value">{latency_data['mean_latency_ms']:.2f}ms</span></div>
                <div class="metric">P95å»¶è¿Ÿ: <span class="value">{latency_data['p95_latency_ms']:.2f}ms</span></div>
                <div class="metric">P99å»¶è¿Ÿ: <span class="value">{latency_data['p99_latency_ms']:.2f}ms</span></div>
                <div class="metric">ååé‡: <span class="value">{latency_data['throughput_qps']:.2f} QPS</span></div>
                <div class="metric">è®¾å¤‡: <span class="value">{latency_data['device']}</span></div>
            </div>
            """

        # æ‰¹é‡å¤§å°æµ‹è¯•ç»“æœ
        if results[1]:
            html_content += """
            <div class="section">
                <h2>ğŸ“ˆ æ‰¹é‡å¤§å°æµ‹è¯•ç»“æœ</h2>
                <table>
                    <tr>
                        <th>æ‰¹é‡å¤§å°</th>
                        <th>å¹³å‡å»¶è¿Ÿ (ms)</th>
                        <th>å•æ ·æœ¬å»¶è¿Ÿ (ms)</th>
                        <th>æ ·æœ¬ååé‡ (samples/s)</th>
                    </tr>
            """

            for batch_size, data in results[1].items():
                html_content += f"""
                    <tr>
                        <td>{batch_size}</td>
                        <td>{data['mean_latency_ms']:.2f}</td>
                        <td>{data['per_sample_latency_ms']:.2f}</td>
                        <td>{data['samples_per_second']:.1f}</td>
                    </tr>
                """

            html_content += "</table></div>"

        # æ¨¡å‹å¤æ‚åº¦ç»“æœ
        if results[3]:
            complexity_data = results[3]
            html_content += f"""
            <div class="section">
                <h2>ğŸ”¬ æ¨¡å‹å¤æ‚åº¦åˆ†æ</h2>
                <div class="metric">æ€»å‚æ•°é‡: <span class="value">{complexity_data['total_parameters']:,}</span></div>
                <div class="metric">å¯è®­ç»ƒå‚æ•°: <span class="value">{complexity_data['trainable_parameters']:,}</span></div>
                <div class="metric">æ¨¡å‹å¤§å°: <span class="value">{complexity_data['model_size_mb']:.2f}MB</span></div>
                <div class="metric">æ€»å±‚æ•°: <span class="value">{complexity_data['layer_analysis']['total_layers']}</span></div>
            """

            if complexity_data['estimated_flops']:
                html_content += f"""
                <div class="metric">ä¼°ç®—FLOPs: <span class="value">{complexity_data['estimated_flops']:,}</span></div>
                """

            html_content += "</div>"

        html_content += "</body></html>"

        html_file = self.output_dir / f"{report_name}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        return str(html_file)

    def _generate_performance_plots(self,
                                    plots_dir: Path,
                                    latency_results: Dict,
                                    batch_results: Dict = None,
                                    precision_results: Dict = None):
        """ç”Ÿæˆæ€§èƒ½å¯è§†åŒ–å›¾è¡¨"""

        # å»¶è¿Ÿåˆ†å¸ƒå›¾
        if latency_results and 'raw_latencies' in latency_results:
            plt.figure(figsize=(10, 6))
            plt.hist(latency_results['raw_latencies'], bins=50, alpha=0.7, edgecolor='black')
            plt.axvline(latency_results['mean_latency_ms'], color='red', linestyle='--',
                        label=f"å¹³å‡å€¼: {latency_results['mean_latency_ms']:.2f}ms")
            plt.axvline(latency_results['p95_latency_ms'], color='orange', linestyle='--',
                        label=f"P95: {latency_results['p95_latency_ms']:.2f}ms")
            plt.xlabel('å»¶è¿Ÿ (ms)')
            plt.ylabel('é¢‘æ¬¡')
            plt.title('æ¨ç†å»¶è¿Ÿåˆ†å¸ƒ')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig(plots_dir / 'latency_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()

        # æ‰¹é‡å¤§å°æ€§èƒ½å›¾
        if batch_results:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

            batch_sizes = list(batch_results.keys())
            latencies = [batch_results[bs]['mean_latency_ms'] for bs in batch_sizes]
            throughputs = [batch_results[bs]['samples_per_second'] for bs in batch_sizes]

            # å»¶è¿Ÿå›¾
            ax1.plot(batch_sizes, latencies, 'o-', linewidth=2, markersize=8)
            ax1.set_xlabel('æ‰¹é‡å¤§å°')
            ax1.set_ylabel('å¹³å‡å»¶è¿Ÿ (ms)')
            ax1.set_title('æ‰¹é‡å¤§å° vs å»¶è¿Ÿ')
            ax1.grid(True, alpha=0.3)
            ax1.set_xscale('log', base=2)

            # ååé‡å›¾
            ax2.plot(batch_sizes, throughputs, 'o-', linewidth=2, markersize=8, color='green')
            ax2.set_xlabel('æ‰¹é‡å¤§å°')
            ax2.set_ylabel('æ ·æœ¬ååé‡ (samples/s)')
            ax2.set_title('æ‰¹é‡å¤§å° vs ååé‡')
            ax2.grid(True, alpha=0.3)
            ax2.set_xscale('log', base=2)

            plt.tight_layout()
            plt.savefig(plots_dir / 'batch_size_performance.png', dpi=300, bbox_inches='tight')
            plt.close()

        # ç²¾åº¦æ¨¡å¼å¯¹æ¯”å›¾
        if precision_results:
            precisions = list(precision_results.keys())
            latencies = [precision_results[p]['mean_latency_ms'] for p in precisions]
            throughputs = [precision_results[p]['throughput_qps'] for p in precisions]

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

            # å»¶è¿Ÿå¯¹æ¯”
            bars1 = ax1.bar(precisions, latencies, color=['skyblue', 'lightcoral'])
            ax1.set_ylabel('å¹³å‡å»¶è¿Ÿ (ms)')
            ax1.set_title('ç²¾åº¦æ¨¡å¼å»¶è¿Ÿå¯¹æ¯”')
            ax1.grid(True, alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, latency in zip(bars1, latencies):
                ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,
                         f'{latency:.2f}', ha='center', va='bottom')

            # ååé‡å¯¹æ¯”
            bars2 = ax2.bar(precisions, throughputs, color=['lightgreen', 'orange'])
            ax2.set_ylabel('ååé‡ (QPS)')
            ax2.set_title('ç²¾åº¦æ¨¡å¼ååé‡å¯¹æ¯”')
            ax2.grid(True, alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, throughput in zip(bars2, throughputs):
                ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,
                         f'{throughput:.1f}', ha='center', va='bottom')

            plt.tight_layout()
            plt.savefig(plots_dir / 'precision_comparison.png', dpi=300, bbox_inches='tight')
            plt.close()


class ComprehensivePerformanceTester:
    """ç»¼åˆæ€§èƒ½æµ‹è¯•å™¨"""

    def __init__(self, config: PerformanceConfig = None):
        self.config = config or PerformanceConfig()
        self.latency_tester = InferenceLatencyTester(self.config)
        self.profiler = ModelProfiler(self.config)
        self.reporter = PerformanceReporter(self.config)

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, self.config.log_level),
            format='%(asctime)s - %(levelname)s - %(message)s'
        )

    def run_full_benchmark(self,
                           model: nn.Module,
                           input_generator: Callable,
                           device: str = 'cpu',
                           run_batch_test: bool = True,
                           run_precision_test: bool = True,
                           run_complexity_analysis: bool = True,
                           run_memory_analysis: bool = True) -> str:
        """
        è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•

        Args:
            model: è¦æµ‹è¯•çš„æ¨¡å‹
            input_generator: è¾“å…¥ç”Ÿæˆå‡½æ•°
            device: æµ‹è¯•è®¾å¤‡
            run_batch_test: æ˜¯å¦è¿è¡Œæ‰¹é‡å¤§å°æµ‹è¯•
            run_precision_test: æ˜¯å¦è¿è¡Œç²¾åº¦æµ‹è¯•
            run_complexity_analysis: æ˜¯å¦è¿è¡Œå¤æ‚åº¦åˆ†æ
            run_memory_analysis: æ˜¯å¦è¿è¡Œå†…å­˜åˆ†æ

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """

        print("ğŸš€ å¼€å§‹ç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)

        results = {}

        # 1. åŸºç¡€å»¶è¿Ÿæµ‹è¯•
        print("\n1ï¸âƒ£ åŸºç¡€å»¶è¿Ÿæµ‹è¯•")
        results['latency'] = self.latency_tester.test_model_latency(
            model=model,
            input_generator=input_generator,
            device=device
        )

        # 2. æ‰¹é‡å¤§å°æµ‹è¯•
        if run_batch_test:
            print("\n2ï¸âƒ£ æ‰¹é‡å¤§å°æ€§èƒ½æµ‹è¯•")
            results['batch_sizes'] = self.latency_tester.benchmark_batch_sizes(
                model=model,
                input_generator=input_generator,
                device=device
            )

        # 3. ç²¾åº¦æ¨¡å¼æµ‹è¯•
        if run_precision_test and device.startswith('cuda'):
            print("\nç²¾åº¦æ¨¡å¼æ€§èƒ½æµ‹è¯•")
            results['precision_modes'] = self.latency_tester.benchmark_precision_modes(
                model=model,
                input_generator=input_generator,
                device=device
            )

        # 4. æ¨¡å‹å¤æ‚åº¦åˆ†æ
        if run_complexity_analysis:
            print("\næ¨¡å‹å¤æ‚åº¦åˆ†æ")
            results['complexity'] = self.profiler.profile_model_complexity(
                model=model,
                input_generator=input_generator
            )

        # 5. å†…å­˜ä½¿ç”¨åˆ†æ
        if run_memory_analysis:
            print("\nå†…å­˜ä½¿ç”¨åˆ†æ")
            results['memory'] = self.profiler.memory_profiling(
                model=model,
                input_generator=input_generator,
                device=device
            )

        # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        print("\nç”Ÿæˆç»¼åˆæŠ¥å‘Š")
        report_path = self.reporter.generate_comprehensive_report(
            latency_results=results.get('latency'),
            batch_results=results.get('batch_sizes'),
            precision_results=results.get('precision_modes'),
            complexity_results=results.get('complexity'),
            memory_results=results.get('memory')
        )

        print("\nç»¼åˆæ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

        return report_path


def create_test_input_generator():
    """åˆ›å»ºæµ‹è¯•è¾“å…¥ç”Ÿæˆå™¨ï¼ˆç”¨äºç¤ºä¾‹ï¼‰"""

    def generator(batch_size=1, seq_length=50, feature_dim=128):
        return torch.randn(batch_size, seq_length, feature_dim)

    return generator


def comprehensive_test():
    """å®Œæ•´çš„æ€§èƒ½ç›‘æ§æµ‹è¯•"""

    print("å®Œæ•´æ€§èƒ½ç›‘æ§æ¨¡å—æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    test_model = nn.Sequential(
        nn.Linear(128, 256),
        nn.ReLU(),
        nn.LayerNorm(256),
        nn.Dropout(0.1),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 4)
    )

    # åˆ›å»ºé…ç½®
    config = PerformanceConfig(
        test_runs=50,
        warmup_runs=5,
        batch_sizes=[1, 2, 4, 8],
        enable_memory_profiling=True,
        report_format=['json', 'html']
    )

    # åˆ›å»ºæ€§èƒ½æµ‹è¯•å™¨
    tester = ComprehensivePerformanceTester(config)

    # åˆ›å»ºè¾“å…¥ç”Ÿæˆå™¨
    input_gen = create_test_input_generator()

    # è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    report_path = tester.run_full_benchmark(
        model=test_model,
        input_generator=input_gen,
        device=device,
        run_batch_test=True,
        run_precision_test=torch.cuda.is_available(),
        run_complexity_analysis=True,
        run_memory_analysis=True
    )

    print(f"\næµ‹è¯•å®Œæˆï¼è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹: {report_path}")


if __name__ == "__main__":
    comprehensive_test()