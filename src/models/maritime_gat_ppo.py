#!/usr/bin/env python3
"""
æµ·äº‹GAT-PPOæ™ºèƒ½ä½“ï¼šå®Œæ•´çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
å®Œæ•´çš„PPOå®ç°ï¼ŒåŒ…å«GAEã€å®Œæ•´æŸå¤±å‡½æ•°ã€æ‰¹é‡å¤„ç†ç­‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
from typing import Dict, List, Tuple, Optional, Union
from collections import namedtuple, deque
import logging
from dataclasses import dataclass
import pickle
import os
import sys
from pathlib import Path


# è§£å†³ç›¸å¯¹å¯¼å…¥é—®é¢˜
def setup_imports():
    """è®¾ç½®å¯¼å…¥è·¯å¾„"""
    current_dir = Path(__file__).parent
    project_root = current_dir.parent.parent  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•

    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

    # æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
    src_dir = project_root / "src"
    if str(src_dir) not in sys.path:
        sys.path.insert(0, str(src_dir))


setup_imports()

# ç°åœ¨å¯ä»¥è¿›è¡Œå¯¼å…¥
try:
    # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆå½“ä½œä¸ºåŒ…ä½¿ç”¨æ—¶ï¼‰
    from .gat_wrapper import MaritimeGATEncoder, MaritimeStateBuilder
    from .fairness_reward import ComprehensiveFairnessRewardCalculator, FairnessConfig
except ImportError:
    # å›é€€åˆ°ç»å¯¹å¯¼å…¥ï¼ˆå½“ç›´æ¥è¿è¡Œæ—¶ï¼‰
    try:
        from src.models.gat_wrapper import MaritimeGATEncoder, MaritimeStateBuilder
        from src.models.fairness_reward import ComprehensiveFairnessRewardCalculator, FairnessConfig
    except ImportError:
        # æœ€åçš„å›é€€é€‰é¡¹
        from models.gat_wrapper import MaritimeGATEncoder, MaritimeStateBuilder
        from models.fairness_reward import ComprehensiveFairnessRewardCalculator, FairnessConfig


@dataclass
class PPOConfig:
    """PPOé…ç½®å‚æ•°"""
    # ç½‘ç»œæ¶æ„
    node_feature_dim: int = 5
    gat_hidden_dim: int = 64
    gat_output_dim: int = 32
    ppo_hidden_dim: int = 128
    action_dim: int = 4

    # PPOè¶…å‚æ•°
    learning_rate: float = 3e-4
    gamma: float = 0.99
    gae_lambda: float = 0.95
    clip_ratio: float = 0.2
    entropy_coef: float = 0.01
    value_coef: float = 0.5
    max_grad_norm: float = 0.5

    # è®­ç»ƒå‚æ•°
    ppo_epochs: int = 4
    batch_size: int = 64
    mini_batch_size: int = 16
    buffer_size: int = 2048
    update_frequency: int = 2048

    # ç½‘ç»œæ›´æ–°
    target_kl: float = 0.01
    adaptive_lr: bool = True
    lr_decay: float = 0.99

    # æ¢ç´¢å‚æ•°
    initial_noise: float = 0.1
    noise_decay: float = 0.995
    min_noise: float = 0.01


# ç»éªŒæ•°æ®ç»“æ„
Experience = namedtuple('Experience', [
    'state', 'action', 'reward', 'next_state', 'done',
    'log_prob', 'value', 'advantage', 'return_'
])


class AdvancedPPOMemory:
    """
    é«˜çº§PPOç»éªŒç¼“å†²åŒº
    æ”¯æŒGAEè®¡ç®—ã€æ‰¹é‡é‡‡æ ·ã€ä¼˜å…ˆçº§å›æ”¾ç­‰
    """

    def __init__(self,
                 max_size: int = 2048,
                 gamma: float = 0.99,
                 gae_lambda: float = 0.95):
        self.max_size = max_size
        self.gamma = gamma
        self.gae_lambda = gae_lambda

        # å­˜å‚¨ç¼“å†²åŒº
        self.states = []
        self.actions = []
        self.rewards = []
        self.next_states = []
        self.dones = []
        self.log_probs = []
        self.values = []

        # è®¡ç®—åçš„å€¼
        self.advantages = []
        self.returns = []

        # è½¨è¿¹åˆ†å‰²æ ‡è®°
        self.trajectory_starts = []

        self.ptr = 0
        self.trajectory_start = 0

    def store(self,
              state: Dict[str, Dict],
              action: int,
              reward: float,
              next_state: Dict[str, Dict],
              done: bool,
              log_prob: float,
              value: float):
        """å­˜å‚¨å•æ­¥ç»éªŒ"""

        if len(self.states) < self.max_size:
            self.states.append(state)
            self.actions.append(action)
            self.rewards.append(reward)
            self.next_states.append(next_state)
            self.dones.append(done)
            self.log_probs.append(log_prob)
            self.values.append(value)
        else:
            self.states[self.ptr] = state
            self.actions[self.ptr] = action
            self.rewards[self.ptr] = reward
            self.next_states[self.ptr] = next_state
            self.dones[self.ptr] = done
            self.log_probs[self.ptr] = log_prob
            self.values[self.ptr] = value

        self.ptr = (self.ptr + 1) % self.max_size

        # å¦‚æœepisodeç»“æŸï¼Œæ ‡è®°è½¨è¿¹åˆ†å‰²ç‚¹
        if done:
            self.trajectory_starts.append(self.trajectory_start)
            self.trajectory_start = len(self.states) if len(self.states) < self.max_size else self.ptr

    def finish_trajectory(self, next_value: float = 0.0):
        """
        å®Œæˆè½¨è¿¹å¹¶è®¡ç®—GAEä¼˜åŠ¿å’Œå›æŠ¥
        """
        if len(self.rewards) == 0:
            return

        # è®¡ç®—GAEä¼˜åŠ¿
        self._compute_gae_advantages(next_value)

        # è®¡ç®—æŠ˜æ‰£å›æŠ¥
        self._compute_discounted_returns()

    def _compute_gae_advantages(self, next_value: float):
        """è®¡ç®—GAEï¼ˆGeneralized Advantage Estimationï¼‰ä¼˜åŠ¿"""

        rewards = np.array(self.rewards)
        # å®‰å…¨åœ°è½¬æ¢tensoråˆ°numpy
        values = np.array([v.detach().cpu().numpy() if torch.is_tensor(v) else v for v in self.values])
        dones = np.array(self.dones)

        # æ·»åŠ ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼
        values_with_next = np.append(values, next_value)

        # è®¡ç®—æ—¶é—´å·®åˆ†è¯¯å·®
        deltas = rewards + self.gamma * values_with_next[1:] * (1 - dones) - values_with_next[:-1]

        # è®¡ç®—GAEä¼˜åŠ¿
        advantages = np.zeros_like(rewards)
        advantage = 0

        for t in reversed(range(len(rewards))):
            advantage = deltas[t] + self.gamma * self.gae_lambda * advantage * (1 - dones[t])
            advantages[t] = advantage

        self.advantages = advantages.tolist()

    def _compute_discounted_returns(self):
        """è®¡ç®—æŠ˜æ‰£å›æŠ¥"""
        returns = np.zeros_like(self.rewards)
        returns[-1] = self.rewards[-1] if not torch.is_tensor(self.rewards[-1]) else self.rewards[-1].detach().cpu().numpy()

        for t in reversed(range(len(self.rewards) - 1)):
            returns[t] = self.rewards[t] + self.gamma * returns[t + 1] * (1 - self.dones[t])

        self.returns = returns.tolist()

    def get_all_data(self) -> Tuple[List, List, List, List, List, List, List, List, List]:
        """è·å–æ‰€æœ‰æ•°æ®"""
        return (self.states, self.actions, self.rewards, self.next_states,
                self.dones, self.log_probs, self.values, self.advantages, self.returns)

    def sample_batch(self, batch_size: int) -> Tuple[List, ...]:
        """éšæœºé‡‡æ ·æ‰¹é‡æ•°æ®"""
        indices = random.sample(range(len(self.states)), min(batch_size, len(self.states)))

        batch_states = [self.states[i] for i in indices]
        batch_actions = [self.actions[i] for i in indices]
        batch_rewards = [self.rewards[i] for i in indices]
        batch_next_states = [self.next_states[i] for i in indices]
        batch_dones = [self.dones[i] for i in indices]
        batch_log_probs = [self.log_probs[i] for i in indices]
        batch_values = [self.values[i] for i in indices]
        batch_advantages = [self.advantages[i] for i in indices]
        batch_returns = [self.returns[i] for i in indices]

        return (batch_states, batch_actions, batch_rewards, batch_next_states,
                batch_dones, batch_log_probs, batch_values, batch_advantages, batch_returns)

    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.next_states.clear()
        self.dones.clear()
        self.log_probs.clear()
        self.values.clear()
        self.advantages.clear()
        self.returns.clear()
        self.trajectory_starts.clear()
        self.ptr = 0
        self.trajectory_start = 0

    def __len__(self):
        return len(self.states)


class MaritimeGATPPOAgent(nn.Module):
    """
    å®Œæ•´çš„æµ·äº‹GAT-PPOæ™ºèƒ½ä½“
    é›†æˆäº†å›¾æ³¨æ„åŠ›ç½‘ç»œçš„å®Œæ•´PPOå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
    """

    def __init__(self,
                 node_id: int,
                 num_nodes: int = 4,
                 config: PPOConfig = None):
        super().__init__()

        self.config = config or PPOConfig()
        self.node_id = node_id
        self.num_nodes = num_nodes

        # ç½‘ç»œç»„ä»¶
        self._build_networks()

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.parameters(), lr=self.config.learning_rate)
        self.lr_scheduler = optim.lr_scheduler.ExponentialLR(
            self.optimizer, gamma=self.config.lr_decay
        ) if self.config.adaptive_lr else None

        # ç»éªŒç¼“å†²åŒº
        self.memory = AdvancedPPOMemory(
            max_size=self.config.buffer_size,
            gamma=self.config.gamma,
            gae_lambda=self.config.gae_lambda
        )

        # å…¬å¹³æ€§å¥–åŠ±è®¡ç®—å™¨
        fairness_config = FairnessConfig(
            efficiency_weight=0.6,
            fairness_weight=0.4,
            adaptive_weights=True
        )
        self.fairness_calculator = ComprehensiveFairnessRewardCalculator(fairness_config)

        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'episode': 0,
            'total_steps': 0,
            'policy_loss': [],
            'value_loss': [],
            'entropy_loss': [],
            'kl_divergence': [],
            'explained_variance': [],
            'learning_rate': []
        }

        # æ¢ç´¢å™ªå£°
        self.exploration_noise = self.config.initial_noise

        # æ—¥å¿—
        self.logger = logging.getLogger(f"Agent_{node_id}")

    def _build_networks(self):
        """æ„å»ºç¥ç»ç½‘ç»œ"""

        # GATå›¾ç¼–ç å™¨
        self.gat_encoder = MaritimeGATEncoder(
            node_feature_dim=self.config.node_feature_dim,
            hidden_dim=self.config.gat_hidden_dim,
            output_dim=self.config.gat_output_dim
        )

        # çŠ¶æ€æ„å»ºå™¨
        self.state_builder = MaritimeStateBuilder(self.num_nodes)

        # æœ¬åœ°çŠ¶æ€ç¼–ç å™¨
        self.local_encoder = nn.Sequential(
            nn.Linear(self.config.node_feature_dim, self.config.gat_output_dim),
            nn.ReLU(),
            nn.LayerNorm(self.config.gat_output_dim),
            nn.Linear(self.config.gat_output_dim, self.config.gat_output_dim)
        )

        # çŠ¶æ€èåˆå±‚
        fusion_input_dim = self.config.gat_output_dim * 3  # æœ¬åœ° + èŠ‚ç‚¹GAT + å›¾GAT
        self.state_fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, self.config.ppo_hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(self.config.ppo_hidden_dim),
            nn.Dropout(0.1),
            nn.Linear(self.config.ppo_hidden_dim, self.config.ppo_hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(self.config.ppo_hidden_dim)
        )

        # PPO Actorï¼ˆç­–ç•¥ç½‘ç»œï¼‰
        self.actor = nn.Sequential(
            nn.Linear(self.config.ppo_hidden_dim, self.config.ppo_hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(self.config.ppo_hidden_dim),
            nn.Dropout(0.1),
            nn.Linear(self.config.ppo_hidden_dim, self.config.ppo_hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(self.config.ppo_hidden_dim // 2, self.config.action_dim)
        )

        # PPO Criticï¼ˆä»·å€¼ç½‘ç»œï¼‰
        self.critic = nn.Sequential(
            nn.Linear(self.config.ppo_hidden_dim, self.config.ppo_hidden_dim),
            nn.ReLU(),
            nn.LayerNorm(self.config.ppo_hidden_dim),
            nn.Dropout(0.1),
            nn.Linear(self.config.ppo_hidden_dim, self.config.ppo_hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(self.config.ppo_hidden_dim // 2, 1)
        )

        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()

    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=0.01)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)

    def _encode_state(self, maritime_observations: Dict[str, Dict]) -> torch.Tensor:
        """ç¼–ç çŠ¶æ€"""
        # 1. æ„å»ºå›¾çŠ¶æ€
        node_features, adjacency_matrix = self.state_builder.build_state(maritime_observations)

        # 2. GATç¼–ç 
        node_embeddings, graph_embedding = self.gat_encoder(node_features, adjacency_matrix)

        # 3. æœ¬åœ°çŠ¶æ€ç¼–ç 
        local_state = node_features[self.node_id]
        local_embedding = self.local_encoder(local_state)

        # 4. å½“å‰èŠ‚ç‚¹çš„GATåµŒå…¥
        current_node_embedding = node_embeddings[self.node_id]

        # 5. çŠ¶æ€èåˆ
        fused_state = torch.cat([
            local_embedding,
            current_node_embedding,
            graph_embedding
        ])

        return self.state_fusion(fused_state)

    def forward(self, maritime_observations: Dict[str, Dict]) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­

        Args:
            maritime_observations: æµ·äº‹è§‚æµ‹æ•°æ®
        Returns:
            action_logits: åŠ¨ä½œlogits
            state_value: çŠ¶æ€ä»·å€¼
        """
        encoded_state = self._encode_state(maritime_observations)

        action_logits = self.actor(encoded_state)
        state_value = self.critic(encoded_state)

        return action_logits, state_value

    def get_action_and_value(self,
                             maritime_observations: Dict[str, Dict],
                             action: Optional[int] = None,
                             deterministic: bool = False) -> Tuple[int, float, torch.Tensor, torch.Tensor]:
        """
        è·å–åŠ¨ä½œå’Œä»·å€¼

        Returns:
            action: é€‰æ‹©çš„åŠ¨ä½œ
            log_prob: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            state_value: çŠ¶æ€ä»·å€¼
            entropy: ç­–ç•¥ç†µ
        """
        action_logits, state_value = self.forward(maritime_observations)

        # æ·»åŠ æ¢ç´¢å™ªå£°
        if not deterministic and self.training:
            noise = torch.randn_like(action_logits) * self.exploration_noise
            action_logits = action_logits + noise

        # è®¡ç®—åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
        action_probs = F.softmax(action_logits, dim=-1)
        action_dist = torch.distributions.Categorical(action_probs)

        # é€‰æ‹©åŠ¨ä½œ
        if action is None:
            if deterministic:
                action = torch.argmax(action_probs).item()
            else:
                action = action_dist.sample().item()

        # è®¡ç®—å¯¹æ•°æ¦‚ç‡å’Œç†µ
        log_prob = action_dist.log_prob(torch.tensor(action))
        entropy = action_dist.entropy()

        return action, log_prob.item(), state_value.squeeze(), entropy

    def calculate_comprehensive_reward(self,
                                       maritime_observations: Dict[str, Dict],
                                       action: int,
                                       next_observations: Dict[str, Dict],
                                       previous_observations: Optional[Dict[str, Dict]] = None) -> float:
        """
        è®¡ç®—ç»¼åˆå¥–åŠ±ï¼ˆåŒ…å«å…¬å¹³æ€§ï¼‰
        """
        # æå–ç»Ÿè®¡ä¿¡æ¯
        current_stats = self._extract_comprehensive_statistics(maritime_observations)
        next_stats = self._extract_comprehensive_statistics(next_observations)

        action_results = {
            'total_throughput': next_stats['total_throughput'],
            'avg_waiting_time': next_stats['avg_waiting_time'],
            'avg_queue_length': next_stats['avg_queue_length']
        }

        # ä½¿ç”¨ç»¼åˆå…¬å¹³æ€§å¥–åŠ±è®¡ç®—å™¨
        reward_breakdown = self.fairness_calculator.calculate_comprehensive_reward(
            node_states=next_observations,
            action_results=action_results,
            previous_states=previous_observations
        )

        return reward_breakdown['total_reward']

    def _extract_comprehensive_statistics(self, observations: Dict[str, Dict]) -> Dict[str, float]:
        """æå–ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        total_throughput = sum(obs.get('throughput', 0) for obs in observations.values())
        waiting_times = [obs.get('waiting_time', 0) for obs in observations.values()]
        queue_lengths = [obs.get('waiting_ships', 0) for obs in observations.values()]

        return {
            'total_throughput': total_throughput,
            'avg_waiting_time': np.mean(waiting_times) if waiting_times else 0,
            'avg_queue_length': np.mean(queue_lengths) if queue_lengths else 0,
            'max_waiting_time': max(waiting_times) if waiting_times else 0,
            'min_throughput': min(obs.get('throughput', 0) for obs in observations.values())
        }

    def store_transition(self,
                         observations: Dict[str, Dict],
                         action: int,
                         reward: float,
                         next_observations: Dict[str, Dict],
                         done: bool,
                         log_prob: float,
                         value: float):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.store(observations, action, reward, next_observations, done, log_prob, value)
        self.training_stats['total_steps'] += 1

    def update_policy(self) -> Dict[str, float]:
        """
        å®Œæ•´çš„PPOç­–ç•¥æ›´æ–°

        Returns:
            è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        if len(self.memory) < self.config.batch_size:
            return {}

        # å®Œæˆè½¨è¿¹å¹¶è®¡ç®—GAE
        self.memory.finish_trajectory()

        # è·å–æ‰€æœ‰æ•°æ®
        (states, actions, rewards, next_states, dones,
         old_log_probs, old_values, advantages, returns) = self.memory.get_all_data()

        if len(states) < self.config.mini_batch_size:
            return {}

        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = torch.tensor(advantages, dtype=torch.float32)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        returns = torch.tensor(returns, dtype=torch.float32)
        old_log_probs = torch.tensor(old_log_probs, dtype=torch.float32)
        old_values = torch.tensor(old_values, dtype=torch.float32)
        actions = torch.tensor(actions, dtype=torch.long)

        # è®­ç»ƒç»Ÿè®¡
        policy_losses = []
        value_losses = []
        entropy_losses = []
        kl_divergences = []

        # PPOå¤šè½®æ›´æ–°
        for epoch in range(self.config.ppo_epochs):
            # éšæœºæ‰“ä¹±æ•°æ®
            indices = torch.randperm(len(states))

            # å°æ‰¹é‡æ›´æ–°
            for start_idx in range(0, len(states), self.config.mini_batch_size):
                end_idx = start_idx + self.config.mini_batch_size
                batch_indices = indices[start_idx:end_idx]

                # å°æ‰¹é‡æ•°æ®
                batch_states = [states[i] for i in batch_indices]
                batch_actions = actions[batch_indices]
                batch_advantages = advantages[batch_indices]
                batch_returns = returns[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_old_values = old_values[batch_indices]

                # å‰å‘ä¼ æ’­
                batch_action_logits, batch_values = self._forward_batch(batch_states)

                # è®¡ç®—æ–°çš„åŠ¨ä½œæ¦‚ç‡å’Œç†µ
                batch_action_probs = F.softmax(batch_action_logits, dim=-1)
                batch_action_dist = torch.distributions.Categorical(batch_action_probs)
                batch_new_log_probs = batch_action_dist.log_prob(batch_actions)
                batch_entropy = batch_action_dist.entropy()

                # è®¡ç®—æ¯”ç‡
                ratio = torch.exp(batch_new_log_probs - batch_old_log_probs)

                # PPOç­–ç•¥æŸå¤±
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.config.clip_ratio, 1 + self.config.clip_ratio) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()

                # ä»·å€¼å‡½æ•°æŸå¤±ï¼ˆClipped Value Lossï¼‰
                if self.config.value_coef > 0:
                    value_pred_clipped = batch_old_values + torch.clamp(
                        batch_values.squeeze() - batch_old_values,
                        -self.config.clip_ratio,
                        self.config.clip_ratio
                    )
                    value_loss1 = F.mse_loss(batch_values.squeeze(), batch_returns)
                    value_loss2 = F.mse_loss(value_pred_clipped, batch_returns)
                    value_loss = torch.max(value_loss1, value_loss2)
                else:
                    value_loss = F.mse_loss(batch_values.squeeze(), batch_returns)

                # ç†µæŸå¤±
                entropy_loss = -batch_entropy.mean()

                # æ€»æŸå¤±
                total_loss = (policy_loss +
                              self.config.value_coef * value_loss +
                              self.config.entropy_coef * entropy_loss)

                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                total_loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.parameters(), self.config.max_grad_norm)

                self.optimizer.step()

                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                policy_losses.append(policy_loss.item())
                value_losses.append(value_loss.item())
                entropy_losses.append(entropy_loss.item())

                # KLæ•£åº¦ï¼ˆç”¨äºæ—©åœï¼‰
                with torch.no_grad():
                    kl_div = (batch_old_log_probs - batch_new_log_probs).mean()
                    kl_divergences.append(kl_div.item())

            # æ—©åœæ£€æŸ¥
            avg_kl = np.mean(kl_divergences[-len(range(0, len(states), self.config.mini_batch_size)):])
            if avg_kl > self.config.target_kl:
                self.logger.info(f"Early stopping at epoch {epoch} due to KL divergence {avg_kl:.4f}")
                break

        # æ›´æ–°å­¦ä¹ ç‡
        if self.lr_scheduler:
            self.lr_scheduler.step()

        # æ›´æ–°æ¢ç´¢å™ªå£°
        self.exploration_noise = max(
            self.config.min_noise,
            self.exploration_noise * self.config.noise_decay
        )

        # æ¸…ç©ºç¼“å†²åŒº
        self.memory.clear()

        # è®¡ç®—è§£é‡Šæ–¹å·®
        explained_var = self._compute_explained_variance(old_values.numpy(), returns.numpy())

        # æ›´æ–°è®­ç»ƒç»Ÿè®¡
        train_stats = {
            'policy_loss': np.mean(policy_losses),
            'value_loss': np.mean(value_losses),
            'entropy_loss': np.mean(entropy_losses),
            'kl_divergence': np.mean(kl_divergences),
            'explained_variance': explained_var,
            'learning_rate': self.optimizer.param_groups[0]['lr'],
            'exploration_noise': self.exploration_noise
        }

        # è®°å½•åˆ°å†å²
        for key, value in train_stats.items():
            if key in self.training_stats:
                self.training_stats[key].append(value)

        return train_stats

    def _forward_batch(self, batch_states: List[Dict[str, Dict]]) -> Tuple[torch.Tensor, torch.Tensor]:
        """æ‰¹é‡å‰å‘ä¼ æ’­"""
        batch_action_logits = []
        batch_values = []

        for state in batch_states:
            action_logits, value = self.forward(state)
            batch_action_logits.append(action_logits)
            batch_values.append(value)

        return torch.stack(batch_action_logits), torch.stack(batch_values)

    def _compute_explained_variance(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è®¡ç®—è§£é‡Šæ–¹å·®"""
        var_y = np.var(y_true)
        return 1 - np.var(y_true - y_pred) / (var_y + 1e-8)

    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        checkpoint = {
            'model_state_dict': self.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'config': self.config.__dict__,
            'training_stats': self.training_stats,
            'node_id': self.node_id,
            'exploration_noise': self.exploration_noise
        }

        if self.lr_scheduler:
            checkpoint['lr_scheduler_state_dict'] = self.lr_scheduler.state_dict()

        torch.save(checkpoint, filepath)
        self.logger.info(f"Model saved to {filepath}")

    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)

        self.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.training_stats = checkpoint.get('training_stats', self.training_stats)
        self.exploration_noise = checkpoint.get('exploration_noise', self.config.initial_noise)

        if 'lr_scheduler_state_dict' in checkpoint and self.lr_scheduler:
            self.lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])

        self.logger.info(f"Model loaded from {filepath}")

    def get_training_stats(self) -> Dict:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            'episode': self.training_stats['episode'],
            'total_steps': self.training_stats['total_steps'],
            'recent_policy_loss': self.training_stats['policy_loss'][-10:] if self.training_stats[
                'policy_loss'] else [],
            'recent_value_loss': self.training_stats['value_loss'][-10:] if self.training_stats['value_loss'] else [],
            'recent_kl_div': self.training_stats['kl_divergence'][-10:] if self.training_stats['kl_divergence'] else [],
            'current_lr': self.optimizer.param_groups[0]['lr'],
            'exploration_noise': self.exploration_noise
        }


def comprehensive_test():
    """ä¿®å¤åçš„å®Œæ•´æ™ºèƒ½ä½“æµ‹è¯•"""
    print("ğŸ¤– ä¿®å¤åçš„æµ·äº‹GAT-PPOæ™ºèƒ½ä½“æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºé…ç½® - ğŸ”§ ä¼˜åŒ–batch_sizeä½¿è®­ç»ƒèƒ½å¤Ÿè§¦å‘
    config = PPOConfig(
        learning_rate=3e-4,
        gamma=0.99,
        gae_lambda=0.95,
        clip_ratio=0.2,
        ppo_epochs=4,
        batch_size=20,      # ä»64é™ä½åˆ°20
        mini_batch_size=10  # ä»16é™ä½åˆ°10
    )

    # åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆNodeAçš„æ™ºèƒ½ä½“ï¼‰
    agent = MaritimeGATPPOAgent(node_id=0, config=config)
    agent.train()

    print(f"ğŸš€ æ™ºèƒ½ä½“é…ç½®:")
    print(f"  èŠ‚ç‚¹ID: {agent.node_id}")
    print(f"  åŠ¨ä½œç»´åº¦: {config.action_dim}")
    print(f"  å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"  æ‰¹é‡å¤§å°: {config.batch_size} (å·²ä¼˜åŒ–)")
    print(f"  å°æ‰¹é‡å¤§å°: {config.mini_batch_size} (å·²ä¼˜åŒ–)")

    # æ¨¡æ‹Ÿè®­ç»ƒepisode
    print(f"\næ¨¡æ‹Ÿè®­ç»ƒepisode:")

    for episode in range(3):
        print(f"\nEpisode {episode + 1}:")
        
        # ğŸ”§ ä¿®å¤ï¼šæ›´æ–°episodeè®¡æ•°å™¨
        agent.training_stats['episode'] = episode + 1

        # æ¨¡æ‹Ÿobservation
        observations = {
            'NodeA': {'waiting_ships': 5, 'throughput': 3, 'waiting_time': 12, 'signal_phase': 1,
                      'weather_condition': 0.8},
            'NodeB': {'waiting_ships': 8, 'throughput': 2, 'waiting_time': 18, 'signal_phase': 0,
                      'weather_condition': 0.8},
            'NodeC': {'waiting_ships': 3, 'throughput': 1, 'waiting_time': 8, 'signal_phase': 1,
                      'weather_condition': 0.8},
            'NodeD': {'waiting_ships': 6, 'throughput': 4, 'waiting_time': 15, 'signal_phase': 0,
                      'weather_condition': 0.8},
        }

        episode_rewards = []

        # æ¨¡æ‹Ÿepisodeæ­¥éª¤
        for step in range(10):
            # è·å–åŠ¨ä½œ
            action, log_prob, value, entropy = agent.get_action_and_value(observations)

            # æ¨¡æ‹Ÿç¯å¢ƒæ­¥éª¤
            next_observations = observations.copy()
            # ç®€å•æ¨¡æ‹Ÿï¼šåŠ¨ä½œå½±å“ç­‰å¾…æ—¶é—´
            if action == 0:  # çŸ­ä¿¡å·
                next_observations['NodeA']['waiting_time'] = max(0, observations['NodeA']['waiting_time'] - 1)
            elif action == 1:  # ä¸­ä¿¡å·
                next_observations['NodeA']['waiting_time'] = max(0, observations['NodeA']['waiting_time'] - 2)
            elif action == 2:  # é•¿ä¿¡å·
                next_observations['NodeA']['waiting_time'] = max(0, observations['NodeA']['waiting_time'] - 3)
            else:  # ç‰¹é•¿ä¿¡å·
                next_observations['NodeA']['waiting_time'] = max(0, observations['NodeA']['waiting_time'] - 4)

            # è®¡ç®—å¥–åŠ±
            reward = agent.calculate_comprehensive_reward(observations, action, next_observations)
            episode_rewards.append(reward)

            # å­˜å‚¨ç»éªŒ
            done = (step == 9)  # æœ€åä¸€æ­¥
            agent.store_transition(observations, action, reward, next_observations, done, log_prob, value.item())

            # æ›´æ–°observations
            observations = next_observations

            if step % 5 == 0:
                print(f"  æ­¥éª¤ {step}: åŠ¨ä½œ={action}, å¥–åŠ±={reward:.2f}, ä»·å€¼={value.item():.2f}")

        print(f"  Episodeæ€»å¥–åŠ±: {sum(episode_rewards):.2f}")
        print(f"  ç¼“å†²åŒºå¤§å°: {len(agent.memory)}")

        # ğŸ”§ ä¿®å¤ï¼šæ¯ä¸ªepisodeåå°è¯•PPOæ›´æ–°ï¼ˆé™ä½é—¨æ§›ï¼‰
        if len(agent.memory) >= config.mini_batch_size:  # ä»batch_size(20)æ”¹ä¸ºmini_batch_size(10)
            print(f"  ğŸ”„ å¼€å§‹PPOæ›´æ–°...")
            train_stats = agent.update_policy()
            if train_stats:
                print(f"  âœ… è®­ç»ƒç»Ÿè®¡:")
                for key, value in train_stats.items():
                    print(f"    {key}: {value:.6f}")
            else:
                print(f"  âš ï¸ æ›´æ–°è¿”å›ç©ºç»Ÿè®¡")
        else:
            print(f"  â¸ï¸ ç»éªŒä¸è¶³ï¼Œè·³è¿‡æ›´æ–° (éœ€è¦{config.mini_batch_size}æ­¥ï¼Œå½“å‰{len(agent.memory)}æ­¥)")

    # è·å–è®­ç»ƒç»Ÿè®¡
    print(f"\nğŸ“Š æœ€ç»ˆè®­ç»ƒç»Ÿè®¡:")
    stats = agent.get_training_stats()
    for key, value in stats.items():
        if key == 'episode':
            print(f"  âœ… {key}: {value}")
        elif 'loss' in key and isinstance(value, list) and len(value) > 0:
            print(f"  âœ… {key}: {value} (æœ‰è®­ç»ƒæ•°æ®)")
        else:
            print(f"  {key}: {value}")

    # æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
    print(f"\nğŸ’¾ æ¨¡å‹ä¿å­˜/åŠ è½½æµ‹è¯•:")
    save_path = "test_maritime_gat_ppo_agent.pth"
    agent.save_model(save_path)

    # åˆ›å»ºæ–°æ™ºèƒ½ä½“å¹¶åŠ è½½
    new_agent = MaritimeGATPPOAgent(node_id=0, config=config)
    new_agent.load_model(save_path)

    print(f"  æ¨¡å‹ä¿å­˜å’ŒåŠ è½½æˆåŠŸ")
    print(f"  åŠ è½½çš„æ€»æ­¥æ•°: {new_agent.training_stats['total_steps']}")

    # æ¸…ç†æ–‡ä»¶
    if os.path.exists(save_path):
        os.remove(save_path)

    print(f"\nå®Œæ•´æµ·äº‹GAT-PPOæ™ºèƒ½ä½“æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    comprehensive_test()