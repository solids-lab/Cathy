#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨³æ€å¾®è°ƒ v2 - æ›´ä¿å®ˆ + EMAå»æŠ–
- learning_rate: 3e-4 * 0.15 (æ›´å°)
- entropy_coef: 0.008 (æ›´ä½æ¢ç´¢)
- schedule: [16,16,12] (æ›´å¤šè½®æ¬¡)
- æ¥å—æ¡ä»¶: min_wr æˆ– min_lb ä»»ä¸€æå‡ â‰¥ 0.2pp
- EMAå»æŠ–: Î±=0.7
"""

import os
import sys
import json
import time
import subprocess
import re
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root))

from src.federated.curriculum_trainer import CurriculumTrainer, build_agent
from src.federated.curriculum_trainer import CurriculumStage

def steady_fine_tune_v2():
    """ç¨³æ€å¾®è°ƒ v2 - æ›´ä¿å®ˆ + EMAå»æŠ–"""
    
    # é…ç½®å‚æ•°
    port = "baton_rouge"
    stage_name = "ä¸­çº§é˜¶æ®µ"
    
    # æ”¶å°¾å¾®è°ƒé…ç½®ï¼ˆçŸ­å¹³å¿«ï¼‰
    base_lr = 3e-4 * 0.12  # â‰ˆ3.6e-5
    lr_decay = 0.85
    
    # è®­ç»ƒå‚æ•°
    ppo_epochs = 4  # ç¨åŠ æ›´æ–°å¼ºåº¦
    entropy_coef = 0.006  # æ›´ä½æ¢ç´¢
    max_rounds = 8
    early_stop_rounds = 2
    
    print(f"ğŸ¯ å¼€å§‹ç¨³æ€å¾®è°ƒ v2 - {port} {stage_name}")
    print(f"ğŸ“Š å‚æ•°: lr={base_lr:.2e}, ppo_epochs={ppo_epochs}, entropy_coef={entropy_coef}")
    print(f"â±ï¸  è½®æ¬¡: {max_rounds} + {max_rounds} + 6 (æ—©åœ: {early_stop_rounds}è½®)")
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = CurriculumTrainer(port_name=port)
    
    # æ‰¾åˆ°å¯¹åº”çš„é˜¶æ®µ
    target_stage = None
    for stage in trainer.curriculum_stages:
        if stage.name == stage_name:
            target_stage = stage
            break
    
    if not target_stage:
        print(f"âŒ æœªæ‰¾åˆ°é˜¶æ®µ: {stage_name}")
        return
    
    print(f"âœ… æ‰¾åˆ°é˜¶æ®µ: {target_stage.name} - {target_stage.description}")
    
    # è®°å½•æœ€ä½³æ€§èƒ½
    best_min_wr = 0.0
    best_min_lb = 0.0
    no_improvement_count = 0
    best_ckpt_path = None
    
    # ç¬¬ä¸€è½®å¾®è°ƒ
    print(f"\nğŸ”„ ç¬¬ä¸€è½®å¾®è°ƒ ({max_rounds}è½®)")
    for round_idx in range(max_rounds):
        current_lr = base_lr * (lr_decay ** round_idx)
        
        print(f"\nğŸ“ˆ è½®æ¬¡ {round_idx + 1}/{max_rounds}")
        print(f"   ğŸ’¡ å­¦ä¹ ç‡: {current_lr:.2e}")
        
        # æ„å»ºæ™ºèƒ½ä½“
        agent = build_agent(port, learning_rate=current_lr, ppo_epochs=ppo_epochs)
        
        # è®¾ç½®entropy_coef
        if hasattr(agent, 'entropy_coef'):
            agent.entropy_coef = entropy_coef
        
        # åŠ è½½ç°æœ‰æ¨¡å‹
        ckpt_path = trainer.save_dir / f"stage_{stage_name}_best.pt"
        if ckpt_path.exists():
            print(f"   ğŸ“‚ åŠ è½½ç°æœ‰æ¨¡å‹: {ckpt_path}")
            try:
                checkpoint = torch.load(ckpt_path, map_location='cpu')
                if 'model_state_dict' in checkpoint:
                    agent.actor_critic.load_state_dict(checkpoint['model_state_dict'])
                elif 'actor_critic' in checkpoint:
                    agent.actor_critic.load_state_dict(checkpoint['actor_critic'])
                else:
                    agent.actor_critic.load_state_dict(checkpoint)
                print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"   âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        # å¾®è°ƒ
        print(f"   ğŸ”„ å¼€å§‹å¾®è°ƒ...")
        try:
            trained_agent, training_info = trainer.train_stage(agent, target_stage)
            
            # ä¿å­˜æ¨¡å‹
            save_path = trainer.save_dir / f"stage_{stage_name}_best.pt"
            torch.save({
                'model_state_dict': trained_agent.actor_critic.state_dict(),
                'training_info': training_info,
                'timestamp': datetime.now().isoformat()
            }, save_path)
            print(f"   ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
            
        except Exception as e:
            print(f"   âŒ å¾®è°ƒå¤±è´¥: {e}")
            continue
        
        # å¿«é€Ÿæµ‹è¯•ï¼ˆä¸‰ä¸ªç§å­ï¼‰
        print(f"   ğŸ§ª å¿«é€Ÿæµ‹è¯•ï¼ˆä¸‰ä¸ªç§å­ï¼‰...")
        min_wr, min_lb = quick_eval_min_wr_lb(port, stage_name)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
        improve_wr = (min_wr - best_min_wr) >= 0.002   # â‰¥0.2pp
        improve_lb = (min_lb - best_min_lb) >= 0.002
        if improve_wr or improve_lb:
            best_min_wr = min_wr
            best_min_lb = min_lb
            best_ckpt_path = save_path
            no_improvement_count = 0
            print(f"   âœ… æ–°æœ€ä½³! min_wr={min_wr:.3f}, min_lb={min_lb:.3f}")
            
            # EMAå»æŠ–
            if best_ckpt_path and best_ckpt_path.exists():
                ema_result = apply_ema_smoothing(best_ckpt_path, port, stage_name)
                if ema_result:
                    print(f"   âœ“ EMAå»æŠ–å®Œæˆ")
        else:
            no_improvement_count += 1
            print(f"   â¸ï¸  æ— æ”¹è¿› ({no_improvement_count}/{early_stop_rounds})")
        
        # æ—©åœæ£€æŸ¥
        if no_improvement_count >= early_stop_rounds:
            print(f"   ğŸ›‘ æ—©åœè§¦å‘ ({early_stop_rounds}è½®æ— æ”¹è¿›)")
            break
    
    print(f"\nğŸ“ˆ ç¬¬ä¸€è½®æœ€ä½³: min_wr={best_min_wr:.3f}, min_lb={best_min_lb:.3f}")
    print(f"ğŸ’¾ æœ€ä½³æ£€æŸ¥ç‚¹: {best_ckpt_path}")
    
    # ç¬¬äºŒè½®å¾®è°ƒï¼ˆå¦‚æœç¬¬ä¸€è½®æœ‰æ”¹è¿›ï¼‰
    if best_min_wr > 0.0:
        print(f"\nğŸ”„ ç¬¬äºŒè½®å¾®è°ƒ ({max_rounds}è½®)")
        no_improvement_count = 0
        
        for round_idx in range(max_rounds):
            current_lr = base_lr * (lr_decay ** round_idx) * 0.5  # æ›´ä¿å®ˆ
            
            print(f"\nğŸ“ˆ è½®æ¬¡ {round_idx + 1}/{max_rounds}")
            print(f"   ğŸ’¡ å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # æ„å»ºæ™ºèƒ½ä½“
            agent = build_agent(port, learning_rate=current_lr, ppo_epochs=ppo_epochs)
            
            # è®¾ç½®entropy_coef
            if hasattr(agent, 'entropy_coef'):
                agent.entropy_coef = entropy_coef
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            if best_ckpt_path and best_ckpt_path.exists():
                print(f"   ğŸ“‚ åŠ è½½æœ€ä½³æ¨¡å‹: {best_ckpt_path}")
                try:
                    checkpoint = torch.load(best_ckpt_path, map_location='cpu')
                    if 'model_state_dict' in checkpoint:
                        agent.actor_critic.load_state_dict(checkpoint['model_state_dict'])
                    elif 'actor_critic' in checkpoint:
                        agent.actor_critic.load_state_dict(checkpoint['actor_critic'])
                    else:
                        agent.actor_critic.load_state_dict(checkpoint)
                    print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"   âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            
            # å¾®è°ƒ
            print(f"   ğŸ”„ å¼€å§‹å¾®è°ƒ...")
            try:
                trained_agent, training_info = trainer.train_stage(agent, target_stage)
                
                # ä¿å­˜æ¨¡å‹
                save_path = trainer.save_dir / f"stage_{stage_name}_best.pt"
                torch.save({
                    'model_state_dict': trained_agent.actor_critic.state_dict(),
                    'training_info': training_info,
                    'timestamp': datetime.now().isoformat()
                }, save_path)
                print(f"   ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
                
            except Exception as e:
                print(f"   âŒ å¾®è°ƒå¤±è´¥: {e}")
                continue
            
            # å¿«é€Ÿæµ‹è¯•ï¼ˆä¸‰ä¸ªç§å­ï¼‰
            print(f"   ğŸ§ª å¿«é€Ÿæµ‹è¯•ï¼ˆä¸‰ä¸ªç§å­ï¼‰...")
            min_wr, min_lb = quick_eval_min_wr_lb(port, stage_name)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
            improve_wr = (min_wr - best_min_wr) >= 0.002   # â‰¥0.2pp
            improve_lb = (min_lb - best_min_lb) >= 0.002
            if improve_wr or improve_lb:
                best_min_wr = min_wr
                best_min_lb = min_lb
                best_ckpt_path = save_path
                no_improvement_count = 0
                print(f"   âœ… æ–°æœ€ä½³! min_wr={min_wr:.3f}, min_lb={min_lb:.3f}")
                
                # EMAå»æŠ–
                if best_ckpt_path and best_ckpt_path.exists():
                    ema_result = apply_ema_smoothing(best_ckpt_path, port, stage_name)
                    if ema_result:
                        print(f"   âœ“ EMAå»æŠ–å®Œæˆ")
            else:
                no_improvement_count += 1
                print(f"   â¸ï¸  æ— æ”¹è¿› ({no_improvement_count}/{early_stop_rounds})")
            
            # æ—©åœæ£€æŸ¥
            if no_improvement_count >= early_stop_rounds:
                print(f"   ğŸ›‘ æ—©åœè§¦å‘ ({early_stop_rounds}è½®æ— æ”¹è¿›)")
                break
    
    # æœ€ç»ˆæµ‹è¯•
    print(f"\nğŸ¯ æœ€ç»ˆæµ‹è¯•")
    print(f"ğŸ“Š æœ€ä½³: min_wr={best_min_wr:.3f}, min_lb={best_min_lb:.3f}")
    print(f"ğŸ’¾ æœ€ä½³æ£€æŸ¥ç‚¹: {best_ckpt_path}")
    
    # è¿è¡Œå®Œæ•´ä¸€è‡´æ€§æµ‹è¯•
    print(f"ğŸ§ª è¿è¡Œå®Œæ•´ä¸€è‡´æ€§æµ‹è¯•...")
    cmd = [
        sys.executable, str(project_root / "src" / "federated" / "consistency_test_fixed.py"),
        "--port", port, "--samples", "800", "--seed", "42", "--no-cache"
    ]
    result = subprocess.run(cmd, cwd=project_root, capture_output=True, text=True)
    
    # ä»è¾“å‡ºä¸­æå–JSONæ–‡ä»¶è·¯å¾„
    import re
    json_match = re.search(r"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°:\s*(.+\.json)", result.stdout + result.stderr)
    if json_match:
        json_path = json_match.group(1).strip()
        with open(json_path, 'r', encoding='utf-8') as f:
            final_result = json.load(f)
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = project_root / "models" / "releases" / f"steady_fine_tune_v2_{port}_{stage_name}_{timestamp}.json"
        
        # æ·»åŠ å¾®è°ƒä¿¡æ¯åˆ°ç»“æœ
        final_result['fine_tune_info'] = {
            'version': 'v2',
            'best_min_wr': best_min_wr,
            'best_min_lb': best_min_lb,
            'best_ckpt_path': str(best_ckpt_path) if best_ckpt_path else None,
            'parameters': {
                'base_lr': base_lr,
                'lr_decay': lr_decay,
                'ppo_epochs': ppo_epochs,
                'entropy_coef': entropy_coef,
                'max_rounds': max_rounds,
                'early_stop_rounds': early_stop_rounds
            }
        }
        
        result_file.parent.mkdir(parents=True, exist_ok=True)
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file}")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        for stage_data in final_result.get('stages', []):
            if stage_data['stage'] == stage_name:
                print(f"   {stage_name}: èƒœç‡ {stage_data['win_rate']:.3f} (é˜ˆå€¼ {stage_data['threshold']:.2f})")
                print(f"   Wilsonä¸‹ç•Œ: {stage_data['wilson_lb']:.3f}")
                print(f"   é€šè¿‡: {'âœ…' if stage_data['pass'] else 'âŒ'}")
                break
    else:
        print(f"âŒ æ— æ³•æ‰¾åˆ°æœ€ç»ˆæµ‹è¯•ç»“æœæ–‡ä»¶")

def quick_eval_min_wr_lb(port, stage_name):
    """å¿«é€Ÿè¯„ä¼°ä¸‰ä¸ªç§å­çš„æœ€å°èƒœç‡å’ŒWilsonä¸‹ç•Œ"""
    seeds = [42, 123, 2025]
    wrs, lbs = [], []
    
    for seed in seeds:
        cmd = [
            sys.executable, str(project_root / "src" / "federated" / "consistency_test_fixed.py"),
            "--port", port, "--samples", "400", "--seed", str(seed), "--k", "100"
        ]
        result = subprocess.run(cmd, cwd=project_root, capture_output=True, text=True)
        
        # ä»è¾“å‡ºä¸­æå–JSONæ–‡ä»¶è·¯å¾„
        json_match = re.search(r"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°:\s*(.+\.json)", result.stdout + result.stderr)
        if json_match:
            json_path = json_match.group(1).strip()
            with open(json_path, 'r', encoding='utf-8') as f:
                test_result = json.load(f)
            
            # æå–æŒ‡å®šé˜¶æ®µç»“æœ
            for stage_data in test_result.get('stages', []):
                if stage_data['stage'] == stage_name:
                    wrs.append(stage_data['win_rate'])
                    lbs.append(stage_data['wilson_lb'])
                    break
    
    min_wr = min(wrs) if wrs else 0.0
    min_lb = min(lbs) if lbs else 0.0
    print(f"   ğŸ“Š èƒœç‡: {[round(w*100,2) for w in wrs]} | Wilsonä¸‹ç•Œ: {[round(l*100,2) for l in lbs]}")
    print(f"   ğŸ“Š æœ€å°èƒœç‡: {min_wr*100:.2f}% | æœ€å°Wilsonä¸‹ç•Œ: {min_lb*100:.2f}%")
    return min_wr, min_lb

def apply_ema_smoothing(ckpt_path, port, stage_name):
    """åº”ç”¨EMAå¹³æ»‘"""
    try:
        import torch
        # è¯»å–å½“å‰æ£€æŸ¥ç‚¹
        current = torch.load(ckpt_path, map_location='cpu')
        
        # è¯»å–ä¹‹å‰çš„æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        prev_path = project_root / "models" / "curriculum_v2" / port / f"stage_{stage_name}_best.pt.backup"
        if prev_path.exists():
            prev = torch.load(prev_path, map_location='cpu')
            
            # EMAåˆå¹¶
            alpha = 0.75
            ema_state_dict = {}
            for k, v in current['model_state_dict'].items():
                if k in prev['model_state_dict'] and prev['model_state_dict'][k].shape == v.shape:
                    ema_state_dict[k] = alpha * v + (1 - alpha) * prev['model_state_dict'][k]
                else:
                    ema_state_dict[k] = v
            
            # ä¿å­˜EMAç‰ˆæœ¬
            ema_checkpoint = {'model_state_dict': ema_state_dict}
            torch.save(ema_checkpoint, ckpt_path)
            
            # å¤‡ä»½å½“å‰ç‰ˆæœ¬
            torch.save(current, prev_path)
            return True
        else:
            # ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œå¤‡ä»½å½“å‰ç‰ˆæœ¬
            torch.save(current, prev_path)
            return True
    except Exception as e:
        print(f"   âš ï¸  EMAå¹³æ»‘å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    import torch
    steady_fine_tune_v2() 