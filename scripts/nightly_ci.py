#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os, sys, re, json, time, argparse, subprocess, logging
from pathlib import Path
from datetime import datetime
from math import sqrt
from glob import glob

logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(message)s")
log = logging.getLogger(__name__)

REPO_ROOT = Path("/Users/kaffy/Documents/GAT-FedPPO")   # ç»å¯¹è·¯å¾„
RUN_DIR    = REPO_ROOT                             
LOG_DIR    = REPO_ROOT / "logs" / "nightly"
REL_DIR    = REPO_ROOT / "models" / "releases"

DEFAULT_PORTS = ["gulfport"]  # å¤œæµ‹å…ˆç›¯ gulfportï¼›æƒ³è·‘å…¨ç«¯å°±ä¼  --ports all
DEFAULT_SEEDS = [42, 123, 2025]

# é˜ˆå€¼ï¼ˆä¸å½“å‰ CurriculumTrainer ä¿æŒä¸€è‡´ï¼›å¯ç”¨ --thr-offset å¾®è°ƒï¼‰
THRESHOLDS = {
    "baton_rouge": {"åŸºç¡€é˜¶æ®µ": 0.41, "ä¸­çº§é˜¶æ®µ": 0.45, "é«˜çº§é˜¶æ®µ": 0.39},
    "new_orleans": {"åŸºç¡€é˜¶æ®µ": 0.35, "åˆçº§é˜¶æ®µ": 0.40, "ä¸­çº§é˜¶æ®µ": 0.50, "é«˜çº§é˜¶æ®µ": 0.40, "ä¸“å®¶é˜¶æ®µ": 0.30},
    "south_louisiana": {"åŸºç¡€é˜¶æ®µ": 0.41, "ä¸­çº§é˜¶æ®µ": 0.45, "é«˜çº§é˜¶æ®µ": 0.39},
    "gulfport": {"æ ‡å‡†é˜¶æ®µ": 0.49, "å®Œæ•´é˜¶æ®µ": 0.38},
}

def wilson_lb(k, n, z=1.96):
    if n == 0: return 0.0
    p = k / n
    denom = 1 + z*z/n
    centre = p + z*z/(2*n)
    margin = z*sqrt((p*(1-p) + z*z/(4*n))/n)
    lb = (centre - margin)/denom
    return max(0.0, min(1.0, lb))

def run_consistency_once(port: str, samples: int, seed: int, timeout: int = 1800) -> Path | None:
    """è°ƒç”¨ consistency_test_fixed.py å¹¶è¿”å›å®ƒä¿å­˜çš„ JSON è·¯å¾„"""
    cmd = [
        sys.executable, "consistency_test_fixed.py",
        "--port", port, "--samples", str(samples), "--seed", str(seed)
    ]
    log.info("â–¶ï¸ è¿è¡Œ: %s", " ".join(cmd))
    try:
        # åœ¨src/federatedç›®å½•ä¸‹è¿è¡Œ
        proc = subprocess.run(cmd, cwd=RUN_DIR / "src" / "federated", capture_output=True, text=True, timeout=timeout)
    except subprocess.TimeoutExpired:
        log.error("â±ï¸ è¶…æ—¶: %s", " ".join(cmd))
        return None

    stdout = proc.stdout
    # æŠŠæœ¬æ¬¡ stdout é™„å¸¦ä¿å­˜ä¸€ä¸‹
    (LOG_DIR / f"stdout_{port}_{seed}_{int(time.time())}.log").write_text(stdout)

    # å…ˆå°è¯•ä»stdoutä¸­æå–è·¯å¾„
    m = re.search(r"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°:\s*(.+\.json)", stdout)
    if m:
        json_path = Path(m.group(1)).resolve()
        if json_path.exists():
            log.info("ğŸ“„ ç»“æœ: %s", json_path)
            return json_path
    
    # å¦‚æœstdoutä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾å½“å¤©æœ€æ–°çš„JSONæ–‡ä»¶
    today_dir = REL_DIR / datetime.now().strftime("%Y-%m-%d")
    if today_dir.exists():
        pattern = f"consistency_{port}_*.json"
        json_files = sorted(today_dir.glob(pattern), key=lambda x: x.stat().st_mtime, reverse=True)
        if json_files:
            log.info("ğŸ“„ å…œåº•æ‰¾åˆ°ç»“æœ: %s", json_files[0])
            return json_files[0]
    
    log.error("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç»“æœJSONæ–‡ä»¶")
    return None

def collect_latest_jsons(port: str) -> list[Path]:
    """å…œåº•ï¼šå½“å¤© releases ç›®å½•ä¸‹è¯¥æ¸¯å£çš„ JSONï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰"""
    today_dir = REL_DIR / datetime.now().strftime("%Y-%m-%d")
    files = sorted(today_dir.glob(f"consistency_{port}_*.json"))
    return files

def load_json(p: Path) -> dict:
    try:
        return json.loads(p.read_text())
    except Exception:
        return {}

def decide_alert(port: str, stages: list[dict], samples: int, thr_offset: float, lb_slack: float):
    """åŸºäº Wilson ä¸‹ç•Œåšå‘Šè­¦åˆ¤å®š"""
    alerts = []
    thr_cfg = THRESHOLDS.get(port, {})
    for st in stages:
        name = st.get("stage", "")
        wr = float(st.get("win_rate", 0.0))
        thr = float(st.get("threshold", thr_cfg.get(name, 0.0))) + thr_offset
        n = samples
        k = int(round(wr * n))
        lb = wilson_lb(k, n)
        ok = (lb >= max(0.0, thr - lb_slack))
        if not ok:
            alerts.append({
                "stage": name,
                "win_rate": wr,
                "thr_config": thr,
                "wilson_lb": lb
            })
    return alerts

def main():
    ap = argparse.ArgumentParser(description="Nightly CI for consistency test")
    ap.add_argument("--ports", default="gulfport", help="é€—å·åˆ†éš”çš„æ¸¯å£åæˆ– all")
    ap.add_argument("--samples", type=int, default=800, help="æ¯ç§å­æ ·æœ¬æ•°")
    ap.add_argument("--seeds", default="42,123,2025", help="é€—å·åˆ†éš”çš„ç§å­")
    ap.add_argument("--timeout", type=int, default=1800)
    ap.add_argument("--thr-offset", type=float, default=0.0, help="ç»Ÿä¸€é˜ˆå€¼åç§»ï¼ˆå¦‚ -0.02 æ”¾å®½2ppï¼‰")
    ap.add_argument("--lb-slack", type=float, default=0.03, help="Wilson ä¸‹ç•Œå®‰å…¨è¾¹ç•Œï¼ˆé»˜è®¤ 3ppï¼‰")
    ap.add_argument("--only-run", action="store_true", help="åªæ‰§è¡Œæµ‹è¯•ï¼Œä¸åšå‘Šè­¦åˆ¤å®š")
    args = ap.parse_args()

    LOG_DIR.mkdir(parents=True, exist_ok=True)

    ports = [p.strip() for p in (DEFAULT_PORTS if args.ports.lower() == "gulfport"
              else (THRESHOLDS.keys() if args.ports.lower()=="all" else args.ports.split(",")))]
    seeds = [int(s) for s in args.seeds.split(",") if s.strip()]

    run_stamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    run_id = int(time.time())

    run_summary = {
        "run_time": run_stamp,
        "ports": {},
        "samples": args.samples,
        "seeds": seeds,
        "thr_offset": args.thr_offset,
        "lb_slack": args.lb_slack,
    }

    exit_code = 0
    for port in ports:
        per_seed = []
        for sd in seeds:
            jp = run_consistency_once(port, args.samples, sd, args.timeout)
            if not jp:
                per_seed.append({"seed": sd, "status": "error"})
                exit_code = max(exit_code, 1)
                continue
            data = load_json(jp)
            per_seed.append({"seed": sd, "json": str(jp), "data": data})

        # èšåˆå‘Šè­¦
        worst_alerts = []
        if not args.only_run:
            # å¯¹æ¯ä¸ªç§å­å„è‡ªåˆ¤å®šï¼Œå†å–"æœ€å"çš„å‘Šè­¦é›†åˆ
            for row in per_seed:
                if "data" not in row: 
                    continue
                stages = row["data"].get("stages", [])
                alerts = decide_alert(port, stages, args.samples, args.thr_offset, args.lb_slack)
                if len(alerts) > len(worst_alerts):
                    worst_alerts = alerts

        port_status = {
            "seeds": per_seed,
            "alerts": worst_alerts,
            "ok": len(worst_alerts) == 0
        }
        run_summary["ports"][port] = port_status
        if not port_status["ok"]:
            exit_code = max(exit_code, 2)

    # å†™çŠ¶æ€ä¸å†å²
    (LOG_DIR / "monitoring_status.json").write_text(json.dumps(run_summary, indent=2, ensure_ascii=False))
    hist = LOG_DIR / "history.csv"
    if not hist.exists():
        hist.write_text("ts,port,ok,alerts,seeds,samples,thr_offset,lb_slack\n")
    for port, st in run_summary["ports"].items():
        hist.open("a").write(
            f'{run_stamp},{port},{int(st["ok"])},{len(st["alerts"])},{"/".join(map(str,seeds))},{args.samples},{args.thr_offset},{args.lb_slack}\n'
        )

    # æ§åˆ¶å°æç¤º
    log.info("\n===== å¤œæµ‹æ€»ç»“ @ %s =====", run_stamp)
    for port, st in run_summary["ports"].items():
        mark = "âœ…" if st["ok"] else "âŒ"
        log.info("  %s %s (%d alert)", port, mark, len(st["alerts"]))
        for a in st["alerts"]:
            log.info("    - %s: wr=%.1f%%, LB=%.1f%% < thr=%.1f%%",
                     a["stage"], a["win_rate"]*100, a["wilson_lb"]*100, a["thr_config"]*100)

    sys.exit(exit_code)

if __name__ == "__main__":
    main()