#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os, sys, re, json, time, argparse, subprocess, logging
from pathlib import Path
from datetime import datetime
from math import sqrt
from glob import glob
import yaml

logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(message)s")
log = logging.getLogger(__name__)

REPO_ROOT = Path(__file__).resolve().parents[1]  # åŠ¨æ€è·å–ä»“åº“æ ¹ç›®å½•
CONSISTENCY_SCRIPT = REPO_ROOT / "src" / "federated" / "consistency_test_fixed.py"
LOG_DIR    = REPO_ROOT / "logs" / "nightly"
REL_DIR    = REPO_ROOT / "models" / "releases"

DEFAULT_PORTS = ["gulfport"]  # å¤œæµ‹å…ˆç›¯ gulfportï¼›æƒ³è·‘å…¨ç«¯å°±ä¼  --ports all
DEFAULT_SEEDS = [42, 123, 2025]

# é˜ˆå€¼ï¼ˆä¸å½“å‰ CurriculumTrainer ä¿æŒä¸€è‡´ï¼›å¯ç”¨ --thr-offset å¾®è°ƒï¼‰
THRESHOLDS = {
    "baton_rouge": {"åŸºç¡€é˜¶æ®µ": 0.41, "ä¸­çº§é˜¶æ®µ": 0.45, "é«˜çº§é˜¶æ®µ": 0.39},
    "new_orleans": {"åŸºç¡€é˜¶æ®µ": 0.35, "åˆçº§é˜¶æ®µ": 0.40, "ä¸­çº§é˜¶æ®µ": 0.50, "é«˜çº§é˜¶æ®µ": 0.40, "ä¸“å®¶é˜¶æ®µ": 0.30},
    "south_louisiana": {"åŸºç¡€é˜¶æ®µ": 0.41, "ä¸­çº§é˜¶æ®µ": 0.45, "é«˜çº§é˜¶æ®µ": 0.39},
    "gulfport": {"æ ‡å‡†é˜¶æ®µ": 0.49, "å®Œæ•´é˜¶æ®µ": 0.37},
}

def load_sample_size_config():
    """åŠ è½½æ ·æœ¬é‡é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).parent / "sample_size_config.yaml"
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            log.warning(f"åŠ è½½æ ·æœ¬é‡é…ç½®å¤±è´¥: {e}")
    return {}

def get_optimal_sample_size(port: str, stage: str, default_samples: int = 400) -> int:
    """åŸºäºæ¸¯å£å’Œé˜¶æ®µè¿”å›æœ€ä¼˜æ ·æœ¬é‡"""
    config = load_sample_size_config()
    port_config = config.get('ports', {}).get(port, {})
    
    # é˜¶æ®µåç§°æ˜ å°„
    stage_mapping = {
        'gulfport': {'æ ‡å‡†é˜¶æ®µ': 'standard_stage', 'å®Œæ•´é˜¶æ®µ': 'complete_stage'},
        'new_orleans': {'é«˜çº§é˜¶æ®µ': 'advanced_stage'},
        'south_louisiana': {'é«˜çº§é˜¶æ®µ': 'advanced_stage'},
        'baton_rouge': {'ä¸­çº§é˜¶æ®µ': 'intermediate_stage', 'é«˜çº§é˜¶æ®µ': 'advanced_stage'}
    }
    
    mapped_stage = stage_mapping.get(port, {}).get(stage, stage)
    return port_config.get(mapped_stage, default_samples)

def wilson_lb(k, n, z=1.96):
    if n == 0: return 0.0
    p = k / n
    denom = 1 + z*z/n
    centre = p + z*z/(2*n)
    margin = z*sqrt((p*(1-p) + z*z/(4*n))/n)
    lb = (centre - margin)/denom
    return max(0.0, min(1.0, lb))

def run_consistency_once(port: str, samples: int, seed: int, timeout: int = 1800, no_cache: bool = False) -> Path | None:
    """è°ƒç”¨ consistency_test_fixed.py å¹¶è¿”å›å®ƒä¿å­˜çš„ JSON è·¯å¾„"""
    cmd = [
        sys.executable, str(CONSISTENCY_SCRIPT),
        "--port", port, "--samples", str(samples), "--seed", str(seed)
    ]
    if no_cache:
        cmd.append("--no-cache")
    log.info("â–¶ï¸ è¿è¡Œ: %s", " ".join(cmd))
    try:
        # æ³¨æ„ cwd=REPO_ROOTï¼Œé¿å…æ‰¾ä¸åˆ°è„šæœ¬/æ¨¡å‹
        proc = subprocess.run(cmd, cwd=REPO_ROOT, capture_output=True, text=True, timeout=timeout)
    except subprocess.TimeoutExpired:
        log.error("â±ï¸ è¶…æ—¶: %s", " ".join(cmd))
        return None
    
    # æ£€æŸ¥è¿”å›ç 
    if proc.returncode != 0:
        log.error("âŒ å­è¿›ç¨‹è¿”å›ç : %d", proc.returncode)
        log.error("stderr: %s", proc.stderr)
        return None

    stdout = proc.stdout
    stderr = proc.stderr
    # æŠŠæœ¬æ¬¡ stdout å’Œ stderr é™„å¸¦ä¿å­˜ä¸€ä¸‹
    (LOG_DIR / f"stdout_{port}_{seed}_{int(time.time())}.log").write_text(stdout)
    (LOG_DIR / f"stderr_{port}_{seed}_{int(time.time())}.log").write_text(stderr)
    
    # åˆå¹¶stdoutå’Œstderræ¥æŸ¥æ‰¾è¾“å‡º
    combined_output = stdout + stderr

    # å…ˆå°è¯•ä»combined_outputä¸­æå–è·¯å¾„
    m = re.search(r"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°:\s*(.+\.json)", combined_output)
    if m:
        json_path_str = m.group(1).strip()
        # å¤„ç†ç›¸å¯¹è·¯å¾„ ../../models/releases/...
        if json_path_str.startswith("../../"):
            json_path = REPO_ROOT / json_path_str[6:]  # å»æ‰ "../../"
        else:
            json_path = Path(json_path_str)
        if json_path.exists():
            log.info("ğŸ“„ ç»“æœ: %s", json_path)
            return json_path
    
    # å¦‚æœstdoutä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾å½“å¤©æœ€æ–°çš„JSONæ–‡ä»¶ï¼ˆä»…åœ¨ä¸ç¦ç”¨ç¼“å­˜æ—¶ï¼‰
    if not no_cache:
        today_dir = REL_DIR / datetime.now().strftime("%Y-%m-%d")
        if today_dir.exists():
            pattern = f"consistency_{port}_*.json"
            json_files = sorted(today_dir.glob(pattern), key=lambda x: x.stat().st_mtime, reverse=True)
            if json_files:
                log.info("ğŸ“„ å…œåº•æ‰¾åˆ°ç»“æœ: %s", json_files[0])
                return json_files[0]
    
    log.error("âŒ æœªæ‰¾åˆ°æµ‹è¯•ç»“æœJSONæ–‡ä»¶")
    return None

def collect_latest_jsons(port: str) -> list[Path]:
    """å…œåº•ï¼šå½“å¤© releases ç›®å½•ä¸‹è¯¥æ¸¯å£çš„ JSONï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰"""
    today_dir = REL_DIR / datetime.now().strftime("%Y-%m-%d")
    files = sorted(today_dir.glob(f"consistency_{port}_*.json"))
    return files

def load_json(p: Path) -> dict:
    try:
        return json.loads(p.read_text())
    except Exception:
        return {}

def _read_result(path: str):
    """è¯»å–ç»“æœæ–‡ä»¶å¹¶å…¼å®¹æ—§æ ¼å¼"""
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    # å…¼å®¹/å½’ä¸€
    for s in data.get("stages", []):
        s["pass"] = bool(s.get("pass", s.get("ok", False)))
    return data

def _summarize_port(result_file: str):
    """åŸºäºJSONçš„passå­—æ®µæ±‡æ€»æ¸¯å£çŠ¶æ€"""
    data = _read_result(result_file)
    alerts = [s for s in data.get("stages", []) if not s["pass"]]
    return len(alerts), alerts

def main():
    ap = argparse.ArgumentParser(description="Nightly CI for consistency test")
    ap.add_argument("--ports", default="gulfport", help="é€—å·åˆ†éš”çš„æ¸¯å£åæˆ– all")
    ap.add_argument("--samples", type=int, default=400, help="æ¯ç§å­æ ·æœ¬æ•°ï¼ˆåŸºç¡€å€¼ï¼Œä¼šæ ¹æ®æ¸¯å£å’Œé˜¶æ®µè‡ªåŠ¨è°ƒæ•´ï¼‰")
    ap.add_argument("--seeds", default="42,123,2025", help="é€—å·åˆ†éš”çš„ç§å­")
    ap.add_argument("--timeout", type=int, default=1800)
    ap.add_argument("--thr-offset", type=float, default=0.0, help="ç»Ÿä¸€é˜ˆå€¼åç§»ï¼ˆå¦‚ -0.02 æ”¾å®½2ppï¼‰")
    ap.add_argument("--lb-slack", type=float, default=0.03, help="Wilson ä¸‹ç•Œå®‰å…¨è¾¹ç•Œï¼ˆé»˜è®¤ 3ppï¼‰")
    ap.add_argument("--only-run", action="store_true", help="åªæ‰§è¡Œæµ‹è¯•ï¼Œä¸åšå‘Šè­¦åˆ¤å®š")
    ap.add_argument("--no-cache", action="store_true", help="ç¦ç”¨ç¼“å­˜ï¼Œå¼ºåˆ¶é‡ç®—")
    args = ap.parse_args()

    LOG_DIR.mkdir(parents=True, exist_ok=True)

    ports = [p.strip() for p in (DEFAULT_PORTS if args.ports.lower() == "gulfport"
              else (THRESHOLDS.keys() if args.ports.lower()=="all" else args.ports.split(",")))]
    seeds = [int(s) for s in args.seeds.split(",") if s.strip()]

    run_stamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    run_id = int(time.time())

    run_summary = {
        "run_time": run_stamp,
        "ports": {},
        "samples": args.samples,
        "seeds": seeds,
        "thr_offset": args.thr_offset,
        "lb_slack": args.lb_slack,
    }

    exit_code = 0
    for port in ports:
        per_seed = []
        for sd in seeds:
            # æ ¹æ®æ¸¯å£å’Œé˜¶æ®µåŠ¨æ€è°ƒæ•´æ ·æœ¬é‡
            optimal_samples = get_optimal_sample_size(port, "æ ‡å‡†é˜¶æ®µ", args.samples)  # é»˜è®¤ä½¿ç”¨æ ‡å‡†é˜¶æ®µ
            log.info(f"æ¸¯å£ {port} ç§å­ {sd} ä½¿ç”¨æ ·æœ¬é‡: {optimal_samples}")
            jp = run_consistency_once(port, optimal_samples, sd, args.timeout, args.no_cache)
            if not jp:
                per_seed.append({"seed": sd, "status": "error"})
                exit_code = max(exit_code, 1)
                continue
            data = load_json(jp)
            # æ·»åŠ from_cacheæ ‡è®°
            if "from_cache" not in data:
                data["from_cache"] = False  # é»˜è®¤æ ‡è®°ä¸ºæ–°é²œç»“æœ
            per_seed.append({"seed": sd, "json": str(jp), "data": data})

        # èšåˆå‘Šè­¦ - ä½¿ç”¨JSONçš„passå­—æ®µ
        worst_alerts = []
        if not args.only_run:
            # å¯¹æ¯ä¸ªç§å­å„è‡ªåˆ¤å®šï¼Œå†å–"æœ€å"çš„å‘Šè­¦é›†åˆ
            for row in per_seed:
                if "data" not in row: 
                    continue
                alerts_count, alerts = _summarize_port(row["json"])
                if len(alerts) > len(worst_alerts):
                    worst_alerts = alerts

        port_status = {
            "seeds": per_seed,
            "alerts": worst_alerts,
            "ok": len(worst_alerts) == 0
        }
        run_summary["ports"][port] = port_status
        if not port_status["ok"]:
            exit_code = max(exit_code, 2)

    # å†™çŠ¶æ€ä¸å†å²
    (LOG_DIR / "monitoring_status.json").write_text(json.dumps(run_summary, indent=2, ensure_ascii=False))
    hist = LOG_DIR / "history.csv"
    if not hist.exists():
        hist.write_text("ts,port,ok,alerts,seeds,samples,thr_offset,lb_slack\n")
    for port, st in run_summary["ports"].items():
        hist.open("a").write(
            f'{run_stamp},{port},{int(st["ok"])},{len(st["alerts"])},{"/".join(map(str,seeds))},{args.samples},{args.thr_offset},{args.lb_slack}\n'
        )

    # æ§åˆ¶å°æç¤º
    log.info("\n===== å¤œæµ‹æ€»ç»“ @ %s =====", run_stamp)
    for port, st in run_summary["ports"].items():
        mark = "âœ…" if st["ok"] else "âŒ"
        log.info("  %s %s (%d alert)", port, mark, len(st["alerts"]))
        for a in st["alerts"]:
            wr = a.get("win_rate", 0.0)*100
            thr = a.get("threshold", 0.0)*100
            lb = a.get("wilson_lb", 0.0)*100
            log.info("    - %s: wr=%.1f%%, LB=%.1f%% < thr=%.1f%%",
                     a["stage"], wr, lb, thr)

    sys.exit(exit_code)

if __name__ == "__main__":
    main()